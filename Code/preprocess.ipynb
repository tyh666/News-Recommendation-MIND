{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from utils.utils import newsample, getId2idx, tokenize, getVocab, my_collate, Partition_Sampler\n",
    "from data.configs.demo import config\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import BertTokenizer,BertModel,BertTokenizerFast\n",
    "from utils.MIND import MIND\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MIND(Dataset):\n",
    "    \"\"\" Map Style Dataset for MIND, use bert tokenizer\n",
    "\n",
    "    Args:\n",
    "        config(dict): pre-defined dictionary of hyper parameters\n",
    "        news_file(str): path of news_file\n",
    "        behaviors_file(str): path of behaviors_file\n",
    "        shuffle(bool): whether to shuffle the order of impressions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, news_file, behaviors_file, shuffle_pos=False):\n",
    "        # initiate the whole iterator\n",
    "        self.npratio = config.npratio\n",
    "        self.shuffle_pos = shuffle_pos\n",
    "        self.signal_length = config.signal_length\n",
    "        self.his_size = config.his_size\n",
    "        self.impr_size = config.impr_size\n",
    "        self.k = config.k\n",
    "        self.ascend_history = config.ascend_history\n",
    "        self.reducer = config.reducer\n",
    "\n",
    "        pat = re.search(\"MIND/(.*_(.*)/)news\", news_file)\n",
    "        self.mode = pat.group(2)\n",
    "\n",
    "        self.cache_directory = \"/\".join([\"data/cache\", config.embedding, pat.group(1)])\n",
    "        self.news_path = self.cache_directory + \"news.pkl\"\n",
    "        self.behav_path = self.cache_directory + \"{}/{}\".format(self.impr_size, re.search(\"(\\w*\\.)tsv\", behaviors_file).group(1) + \"pkl\")\n",
    "\n",
    "        # only preprocess on the master node, the worker can directly load the cache\n",
    "        if config.rank in [-1, 0]:\n",
    "            if os.path.exists(self.behav_path):\n",
    "                logger.info(\"loading cached user behavior from {}\".format(self.behav_path))\n",
    "                with open(self.behav_path, \"rb\") as f:\n",
    "                    behaviors = pickle.load(f)\n",
    "                    for k,v in behaviors.items():\n",
    "                        setattr(self, k, v)\n",
    "\n",
    "            else:\n",
    "                logger.info(\"encoding user behaviors of {}...\".format(behaviors_file))\n",
    "                os.makedirs(self.cache_directory + str(self.impr_size), exist_ok=True)\n",
    "                self.behaviors_file = behaviors_file\n",
    "                self.nid2index = getId2idx(\"data/dictionaries/nid2idx_{}_{}.json\".format(config.scale, self.mode))\n",
    "                self.uid2index = getId2idx(\"data/dictionaries/uid2idx_{}.json\".format(config.scale))\n",
    "\n",
    "                self.init_behaviors()\n",
    "\n",
    "            if os.path.exists(self.news_path):\n",
    "                logger.info(\"loading cached news tokenization from {}\".format(self.news_path))\n",
    "                with open(self.news_path, \"rb\") as f:\n",
    "                    news = pickle.load(f)\n",
    "                    for k,v in news.items():\n",
    "                        setattr(self, k, v)\n",
    "            else:\n",
    "                from transformers import BertTokenizerFast\n",
    "                logger.info(\"encoding news of {}...\".format(news_file))\n",
    "                self.news_file = news_file\n",
    "                self.max_news_length = 512\n",
    "                # there are only two types of vocabulary\n",
    "                self.tokenizer = BertTokenizerFast.from_pretrained(config.bert, cache=config.path + \"bert_cache/\")\n",
    "                self.nid2index = getId2idx(\"data/dictionaries/nid2idx_{}_{}.json\".format(config.scale, self.mode))\n",
    "                self.init_news()\n",
    "\n",
    "        if config.world_size > 1:\n",
    "            dist.barrier()\n",
    "            if config.rank != 0:\n",
    "                logger.info(\"child process NO.{} loading cached user behavior from {}\".format(config.rank, self.behav_path))\n",
    "                with open(self.behav_path, \"rb\") as f:\n",
    "                    behaviors = pickle.load(f)\n",
    "                    for k,v in behaviors.items():\n",
    "                        setattr(self, k, v)\n",
    "                logger.info(\"child process NO.{} loading cached news tokenization from {}\".format(config.rank, self.news_path))\n",
    "                with open(self.news_path, \"rb\") as f:\n",
    "                    news = pickle.load(f)\n",
    "                    for k,v in news.items():\n",
    "                        setattr(self, k, v)\n",
    "\n",
    "        self.reduction_path = self.cache_directory + self.reducer + \".pkl\"\n",
    "        if config.reducer == \"bm25\":\n",
    "            from .utils import BM25\n",
    "            reducer = BM25(self.signal_length)\n",
    "        elif config.reducer == \"bow\":\n",
    "            from .utils import BagOfWords\n",
    "            reducer = BagOfWords(self.signal_length, self.k)\n",
    "        elif config.reducer == \"matching\":\n",
    "            from utils.utils import DeDuplicate\n",
    "            reducer = DeDuplicate(self.signal_length, self.k, ~config.no_dedup)\n",
    "        else:\n",
    "            reducer=None\n",
    "        logger.info(\"reducing news of {}...\".format(news_file))\n",
    "        self.init_reduction(reducer)\n",
    "\n",
    "\n",
    "    def init_news(self):\n",
    "        \"\"\"\n",
    "            init news information given news file, such as news_title_array.\n",
    "\n",
    "        Args:\n",
    "            bm25: whether to sort the terms by bm25 score\n",
    "        \"\"\"\n",
    "\n",
    "        # VERY IMPORTANT!!!\n",
    "        # The nid2idx dictionary must follow the original order of news in news.tsv\n",
    "\n",
    "        documents = [\"\"]\n",
    "        subwords = [[]]\n",
    "        with open(self.news_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "            for idx in rd:\n",
    "                nid, vert, subvert, title, ab, url, _, _ = idx.strip(\"\\n\").split(\"\\t\")\n",
    "                document = \" \".join([\"[CLS]\", title, vert, subvert, ab])\n",
    "                tokens = self.tokenizer.tokenize(document)[:512]\n",
    "\n",
    "                # index for 1 entry\n",
    "                subword = []\n",
    "\n",
    "                i = -1\n",
    "                j = -1\n",
    "                for token in tokens:\n",
    "                    if token.startswith('##'):\n",
    "                        j += 1\n",
    "                        subword.append([i,j])\n",
    "\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        j += 1\n",
    "                        subword.append([i,j])\n",
    "\n",
    "                documents.append(document)\n",
    "                subwords.append(subword)\n",
    "\n",
    "        encoded_dict = self.tokenizer(documents, add_special_tokens=False, padding=True, truncation=True, max_length=self.max_news_length, return_tensors=\"np\")\n",
    "        self.encoded_news = encoded_dict.input_ids\n",
    "        self.attn_mask = encoded_dict.attention_mask\n",
    "\n",
    "        max_token_length = self.encoded_news.shape[1]\n",
    "        for i,subword in enumerate(subwords):\n",
    "            subwords[i].extend([[0,0]] * (max_token_length - len(subword)))\n",
    "        self.subwords = torch.as_tensor(subwords)\n",
    "\n",
    "        with open(self.news_path, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"encoded_news\": self.encoded_news,\n",
    "                    \"subwords\": self.subwords,\n",
    "                    \"attn_mask\": self.attn_mask\n",
    "                },\n",
    "                f\n",
    "            )\n",
    "\n",
    "    def init_reduction(self, reducer):\n",
    "        \"\"\"\n",
    "            init reduced news\n",
    "        \"\"\"\n",
    "        if not reducer:\n",
    "            return\n",
    "\n",
    "        reduced_news_mask = reducer(self.encoded_news, self.attn_mask)\n",
    "        self.reduced_news = reduced_news_mask[0]\n",
    "\n",
    "        if self.reducer == \"bow\":\n",
    "            self.attn_mask = reduced_news_mask[1]\n",
    "            self.attn_mask_reduced = reduced_news_mask[2]\n",
    "\n",
    "        else:\n",
    "            self.attn_mask_reduced = reduced_news_mask[1]\n",
    "\n",
    "    def init_behaviors(self):\n",
    "        \"\"\"\n",
    "            init behavior logs given behaviors file.\n",
    "        \"\"\"\n",
    "        # list of list of history news index\n",
    "        histories = []\n",
    "        # list of user index\n",
    "        uindexes = []\n",
    "        # list of impression indexes\n",
    "        # self.impr_indexes = []\n",
    "\n",
    "        impr_index = 0\n",
    "\n",
    "        # only store positive behavior\n",
    "        if self.mode == \"train\":\n",
    "            # list of lists, each list represents a\n",
    "            imprs = []\n",
    "            negatives = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "                    # store negative samples of each impression\n",
    "                    negative = []\n",
    "\n",
    "                    for news, label in zip(impr_news, labels):\n",
    "                        if label == 1:\n",
    "                            imprs.append((impr_index, news))\n",
    "                        else:\n",
    "                            negative.append(news)\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    negatives.append(negative)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.negatives = negatives\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"negatives\": self.negatives,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        # store every behavior\n",
    "        elif self.mode == \"dev\":\n",
    "            # list of every cdd news index along with its impression index and label\n",
    "            imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for i in range(0, len(impr_news), self.impr_size):\n",
    "                        imprs.append((impr_index, impr_news[i:i+self.impr_size], labels[i:i+self.impr_size]))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        # store every behavior\n",
    "        elif self.mode == \"test\":\n",
    "            # list of every cdd news index along with its impression index and label\n",
    "            imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i] for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for i in range(0, len(impr_news), self.impr_size):\n",
    "                        imprs.append((impr_index, impr_news[i:i+self.impr_size]))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        with open(self.behav_path, \"wb\") as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            return length of the whole dataset\n",
    "        \"\"\"\n",
    "        return len(self.imprs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\" return data\n",
    "        Args:\n",
    "            index: the index for stored impression\n",
    "\n",
    "        Returns:\n",
    "            back_dic: dictionary of data slice\n",
    "        \"\"\"\n",
    "\n",
    "        impr = self.imprs[index] # (impression_index, news_index)\n",
    "        impr_index = impr[0]\n",
    "        impr_news = impr[1]\n",
    "\n",
    "\n",
    "        user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "        # each time called to return positive one sample and its negative samples\n",
    "        if self.mode == \"train\":\n",
    "            # user\"s unhis news in the same impression\n",
    "            negs = self.negatives[impr_index]\n",
    "            neg_list, neg_pad = newsample(negs, self.npratio)\n",
    "\n",
    "            cdd_ids = [impr_news] + neg_list\n",
    "            cdd_size = self.npratio + 1\n",
    "\n",
    "            label = np.asarray([1] + [0] * self.npratio)\n",
    "\n",
    "            if self.shuffle_pos:\n",
    "                s = np.arange(0, len(label), 1)\n",
    "                np.random.shuffle(s)\n",
    "                cdd_ids = np.asarray(cdd_ids)[s]\n",
    "                label = np.asarray(label)[s]\n",
    "\n",
    "            label = np.arange(0, len(cdd_ids), 1)[label == 1][0]\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "\n",
    "            cdd_mask = torch.ones((cdd_size, 1))\n",
    "            cdd_mask[-neg_pad:] = 0\n",
    "\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids][:, :self.signal_length]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids][:, :self.signal_length]\n",
    "            his_encoded_index = self.encoded_news[his_ids][:, :self.signal_length]\n",
    "            his_attn_mask = self.attn_mask[his_ids][:, :self.signal_length]\n",
    "\n",
    "            cdd_subword_index_all = self.subwords[cdd_ids][:, :self.signal_length]\n",
    "            cdd_subword_index = cdd_subword_index_all[:, :, 0] * self.signal_length + cdd_subword_index_all[:, :, 1]\n",
    "            his_subword_index_all = self.subwords[his_ids][:, :self.signal_length]\n",
    "            his_subword_index = his_subword_index_all[:, :, 0] * self.signal_length + his_subword_index_all[:, :, 1]\n",
    "\n",
    "            cdd_dest = torch.zeros((cdd_size, self.signal_length * self.signal_length))\n",
    "            cdd_subword_prefix = cdd_dest.index_fill(dim=-1, index=cdd_subword_index, value=1) * cdd_mask\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_dest = torch.zeros((self.his_size, self.signal_length * self.signal_length))\n",
    "            his_subword_prefix = his_dest.index_fill(dim=-1, index=his_subword_index, value=1) * his_mask\n",
    "            his_subword_prefix = his_subword_prefix.view(self.his_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            back_dic = {\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                # \"cdd_mask\": np.asarray(neg_pad),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"cdd_subword_prefix\": cdd_subword_prefix,\n",
    "                \"his_subword_prefix\": his_subword_prefix,\n",
    "                \"his_mask\": his_mask,\n",
    "                \"label\": label\n",
    "            }\n",
    "\n",
    "            if self.reducer == \"bm25\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.k + 1]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.k + 1]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"matching\":\n",
    "                cdd_reduced_mask = self.attn_mask_reduced[cdd_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"cdd_reduced_mask\"] = cdd_reduced_mask\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        # each time called return one sample, and no labels\n",
    "        elif self.mode == \"dev\":\n",
    "            cdd_ids = impr_news\n",
    "            cdd_size = len(cdd_ids)\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            user_index = [self.uindexes[impr_index]]\n",
    "            label = impr[2]\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids][:, :self.signal_length]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids][:, :self.signal_length]\n",
    "            his_encoded_index = self.encoded_news[his_ids][:, :self.signal_length]\n",
    "            his_attn_mask = self.attn_mask[his_ids][:, :self.signal_length]\n",
    "\n",
    "            cdd_subword_index_all = self.subwords[cdd_ids][:, :self.signal_length]\n",
    "            cdd_subword_index = cdd_subword_index_all[:, :, 0] * self.signal_length + cdd_subword_index_all[:, :, 1]\n",
    "            his_subword_index_all = self.subwords[his_ids][:, :self.signal_length]\n",
    "            his_subword_index = his_subword_index_all[:, :, 0] * self.signal_length + his_subword_index_all[:, :, 1]\n",
    "\n",
    "            cdd_dest = torch.zeros((cdd_size, self.signal_length * self.signal_length))\n",
    "            cdd_subword_prefix = cdd_dest.index_fill(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_dest = torch.zeros((self.his_size, self.signal_length * self.signal_length))\n",
    "            his_subword_prefix = his_dest.index_fill(dim=-1, index=his_subword_index, value=1) * his_mask\n",
    "            his_subword_prefix = his_subword_prefix.view(self.his_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            back_dic = {\n",
    "                \"impr_index\": impr_index + 1,\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"cdd_subword_prefix\": cdd_subword_prefix,\n",
    "                \"his_subword_prefix\": his_subword_prefix,\n",
    "                \"his_mask\": his_mask,\n",
    "                \"label\": np.asarray(label)\n",
    "            }\n",
    "\n",
    "            if self.reducer == \"bm25\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.k + 1]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.k + 1]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"matching\":\n",
    "                cdd_reduced_mask = self.attn_mask_reduced[cdd_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"cdd_reduced_mask\"] = cdd_reduced_mask\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        elif self.mode == \"test\":\n",
    "            cdd_ids = impr_news\n",
    "            cdd_size = len(cdd_ids)\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids][:, :self.signal_length]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids][:, :self.signal_length]\n",
    "            his_encoded_index = self.encoded_news[his_ids][:, :self.signal_length]\n",
    "            his_attn_mask = self.attn_mask[his_ids][:, :self.signal_length]\n",
    "\n",
    "            cdd_subword_index_all = self.subwords[cdd_ids][:, :self.signal_length]\n",
    "            cdd_subword_index = cdd_subword_index_all[:, :, 0] * self.signal_length + cdd_subword_index_all[:, :, 1]\n",
    "            his_subword_index_all = self.subwords[his_ids][:, :self.signal_length]\n",
    "            his_subword_index = his_subword_index_all[:, :, 0] * self.signal_length + his_subword_index_all[:, :, 1]\n",
    "\n",
    "            cdd_dest = torch.zeros((cdd_size, self.signal_length * self.signal_length))\n",
    "            cdd_subword_prefix = cdd_dest.index_fill(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_dest = torch.zeros((self.his_size, self.signal_length * self.signal_length))\n",
    "            his_subword_prefix = his_dest.index_fill(dim=-1, index=his_subword_index, value=1) * his_mask\n",
    "            his_subword_prefix = his_subword_prefix.view(self.his_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            back_dic = {\n",
    "                \"impr_index\": impr_index + 1,\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"cdd_subword_prefix\": cdd_subword_prefix,\n",
    "                \"his_subword_prefix\": his_subword_prefix,\n",
    "                \"his_mask\": his_mask,\n",
    "            }\n",
    "\n",
    "            if self.reducer == \"bm25\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.k + 1]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.k + 1]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"matching\":\n",
    "                cdd_reduced_mask = self.attn_mask_reduced[cdd_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"cdd_reduced_mask\"] = cdd_reduced_mask\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                his_reduced_index = self.reduced_news[his_ids][:, :self.signal_length]\n",
    "                his_reduced_mask = self.attn_mask_reduced[his_ids][:, :self.signal_length]\n",
    "                back_dic[\"his_reduced_index\"] = his_reduced_index\n",
    "                back_dic[\"his_reduced_mask\"] = his_reduced_mask\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Mode {} not defined\".format(self.mode))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# config.reducer = 'bm25'\n",
    "# config.reducer = 'bow'\n",
    "# config.reducer = 'matching'\n",
    "\n",
    "# config.signal_length = 10\n",
    "# config.scale = 'large'\n",
    "path = config.path + 'MIND/MINDdemo_train/'\n",
    "a = MIND(config, path + 'news.tsv', path + 'behaviors.tsv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-29 15:32:04,898] INFO (utils.MIND) loading cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-08-29 15:32:04,914] INFO (utils.MIND) loading cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-08-29 15:32:05,753] INFO (utils.MIND) reducing news of ../../../Data/MIND/MINDdemo_train/news.tsv...\n",
      "[2021-08-29 15:32:05,755] INFO (utils.utils) unmasking at least k...\n",
      "[2021-08-29 15:32:06,019] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loader1 = DataLoader(a, batch_size=1, pin_memory=False, num_workers=0, drop_last=False, shuffle=False)\n",
    "records1 = list(loader1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "records1[0]['his_attn_mask']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sub = a.subwords[1:3][:,10:20]\n",
    "index = sub[:,:,0] * 10 + sub[:,:,1]\n",
    "\n",
    "dest = torch.zeros((2, 10 * 10))\n",
    "index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[110, 121, 132, 143, 154, 165, 166, 167, 178, 189],\n",
       "        [ 90, 101, 112, 123, 134, 145, 156, 167, 178, 189]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "dest[torch.arange(2), ]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dic = pickle.load(open('/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_dev/10/behaviors.pkl', 'rb'))\n",
    "dic['imprs']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "t = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "res = t([\"[CLS] Newark Liberty Airport's Terminal One a $2.7 billion 'transformative' project\",\"shit bro\"], add_special_tokens=False, padding=True, max_length=20)\n",
    "res"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 12948, 7044, 3199, 1005, 1055, 5536, 2028, 1037, 1002, 1016, 1012, 1021, 4551, 1005, 10938, 8082, 1005, 2622], [4485, 22953, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "re.sub('##','',' '.join(tokens))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[CLS] newark liberty airport ' s terminal one a $ 2 . 7 billion ' transform ative ' project ?\""
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "words = []\n",
    "for tok in tokens:\n",
    "    if tok.startswith('##'):\n",
    "        words[-1] += tok[2:]\n",
    "    else:\n",
    "        words.append(tok)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "' '.join(np.array(['I','you','she'],dtype=object))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I you she'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "res.token_to_word()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "token_to_word() missing 1 required positional argument: 'batch_or_token_index'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_297795/660135163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: token_to_word() missing 1 required positional argument: 'batch_or_token_index'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}