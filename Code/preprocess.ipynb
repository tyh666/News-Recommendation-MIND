{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from utils.utils import newsample, getId2idx, tokenize, getVocab, my_collate, Partition_Sampler, convert_tokens_to_words\n",
    "from data.configs.demo import config\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import BertTokenizer,BertModel,BertTokenizerFast\n",
    "from utils.MIND import MIND\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "t = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class MIND(Dataset):\n",
    "    \"\"\" Map Style Dataset for MIND, use bert tokenizer\n",
    "\n",
    "    Args:\n",
    "        config(dict): pre-defined dictionary of hyper parameters\n",
    "        news_file(str): path of news_file\n",
    "        behaviors_file(str): path of behaviors_file\n",
    "        shuffle(bool): whether to shuffle the order of impressions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, news_file, behaviors_file, shuffle_pos=False):\n",
    "        reducer_map = {\n",
    "            'matching': '',\n",
    "            'bm25': '_bm25',\n",
    "            'bow': '',\n",
    "        }\n",
    "        # initiate the whole iterator\n",
    "        self.npratio = config.npratio\n",
    "        self.shuffle_pos = shuffle_pos\n",
    "        self.signal_length = config.signal_length\n",
    "        self.his_size = config.his_size\n",
    "        self.impr_size = config.impr_size\n",
    "        self.k = config.k\n",
    "        self.ascend_history = config.ascend_history\n",
    "        self.reducer = config.reducer\n",
    "        self.granularity = config.granularity\n",
    "\n",
    "        pat = re.search(\"MIND/(.*_(.*)/)news\", news_file)\n",
    "        self.mode = pat.group(2)\n",
    "\n",
    "        self.cache_directory = \"/\".join([\"data/cache\", config.embedding, pat.group(1)])\n",
    "        self.news_path = self.cache_directory + \"news\" + reducer_map[self.reducer] + \".pkl\"\n",
    "        self.behav_path = self.cache_directory + \"{}/{}\".format(self.impr_size, re.search(\"(\\w*\\.)tsv\", behaviors_file).group(1) + \".pkl\")\n",
    "\n",
    "        # only preprocess on the master node, the worker can directly load the cache\n",
    "        if config.rank in [-1, 0]:\n",
    "            if not os.path.exists(self.behav_path):\n",
    "                logger.info(\"encoding user behaviors of {}...\".format(behaviors_file))\n",
    "                os.makedirs(self.cache_directory + str(self.impr_size), exist_ok=True)\n",
    "                self.behaviors_file = behaviors_file\n",
    "                try:\n",
    "                    # VERY IMPORTANT!!!\n",
    "                    # The nid2idx dictionary must follow the original order of news in news.tsv\n",
    "                    self.nid2index = getId2idx(\"data/dictionaries/nid2idx_{}_{}.json\".format(config.scale, self.mode))\n",
    "                except FileNotFoundError:\n",
    "                    config.construct_nid2idx()\n",
    "                    self.nid2index = getId2idx(\"data/dictionaries/nid2idx_{}_{}.json\".format(config.scale, self.mode))\n",
    "                try:\n",
    "                    self.uid2index = getId2idx(\"data/dictionaries/uid2idx_{}.json\".format(config.scale))\n",
    "                except FileNotFoundError:\n",
    "                    config.construct_uid2idx()\n",
    "                    self.uid2index = getId2idx(\"data/dictionaries/uid2idx_{}.json\".format(config.scale))\n",
    "\n",
    "                self.init_behaviors()\n",
    "\n",
    "\n",
    "            if not os.path.exists(self.news_path):\n",
    "                from transformers import BertTokenizerFast\n",
    "                logger.info(\"encoding news of {}...\".format(news_file))\n",
    "                self.news_file = news_file\n",
    "                self.max_news_length = 512\n",
    "                # there are only two types of vocabulary\n",
    "                self.tokenizer = BertTokenizerFast.from_pretrained(config.bert, cache=config.path + \"bert_cache/\")\n",
    "                self.nid2index = getId2idx(\"data/dictionaries/nid2idx_{}_{}.json\".format(config.scale, self.mode))\n",
    "\n",
    "                if config.reducer == \"matching\":\n",
    "                    from utils.utils import DoNothing\n",
    "                    reducer = DoNothing()\n",
    "                elif config.reducer == \"bm25\":\n",
    "                    from utils.utils import BM25\n",
    "                    reducer = BM25()\n",
    "                elif config.reducer == \"bow\":\n",
    "                    from utils.utils import DoNothing\n",
    "                    reducer = DoNothing()\n",
    "\n",
    "                self.init_news(reducer)\n",
    "\n",
    "        # synchronize all processes\n",
    "        if config.world_size > 1:\n",
    "            dist.barrier()\n",
    "\n",
    "        logger.info(\"process NO.{} loading cached user behavior from {}\".format(config.rank, self.behav_path))\n",
    "        with open(self.behav_path, \"rb\") as f:\n",
    "            behaviors = pickle.load(f)\n",
    "            for k,v in behaviors.items():\n",
    "                setattr(self, k, v)\n",
    "    \n",
    "        logger.info(\"process NO.{} loading cached news tokenization from {}\".format(config.rank, self.news_path))\n",
    "        with open(self.news_path, \"rb\") as f:\n",
    "            news = pickle.load(f)\n",
    "            self.encoded_news = news['encoded_news']\n",
    "            self.attn_mask = news['attn_mask']\n",
    "            if self.granularity in ['avg','sum']:\n",
    "                self.subwords = news['subwords_all'][:, :self.signal_length]\n",
    "            elif self.granularity == 'first':\n",
    "                self.subwords = news['subwords_first'][:, :self.signal_length]\n",
    "            else:\n",
    "                self.subwords = None\n",
    "        \n",
    "        if self.reducer == 'bm25':\n",
    "            try:\n",
    "                with open(self.cache_directory + \"news_matching.pkl\", \"rb\") as f:\n",
    "                    news = pickle.load(f)\n",
    "                    self.encoded_news_original = news['encoded_news']\n",
    "                    self.attn_mask_original = news['attn_mask']\n",
    "                    if self.granularity in ['avg','sum']:\n",
    "                        self.subwords_original = news['subwords_all'][:, :self.signal_length]\n",
    "                    elif self.granularity == 'first':\n",
    "                        self.subwords_original = news['subwords_first'][:, :self.signal_length]\n",
    "                    else:\n",
    "                        self.subwords_original = None\n",
    "            except FileNotFoundError:\n",
    "                raise FileNotFoundError(\"You should always encode with matching reducer before the first time of bm25 reducer\")\n",
    "\n",
    "\n",
    "        if config.reducer == \"matching\":\n",
    "            if not config.no_dedup:\n",
    "                from utils.utils import DeDuplicate\n",
    "                refiner = DeDuplicate(self.signal_length)\n",
    "        elif config.reducer == \"bm25\":\n",
    "            from utils.utils import Truncate\n",
    "            refiner = Truncate(self.k + 1)\n",
    "        elif config.reducer == \"bow\":\n",
    "            from utils.utils import CountFreq\n",
    "            refiner = CountFreq(self.signal_length)\n",
    "        else:\n",
    "            refiner = None\n",
    "\n",
    "        logger.info(\"reducing news of {}...\".format(news_file))\n",
    "        self.init_refinement(refiner)\n",
    "\n",
    "\n",
    "    def init_news(self, reducer):\n",
    "        \"\"\"\n",
    "            1. encode news text to tokens\n",
    "            2. rerank words in the news text according to reduction methods\n",
    "            2. get subword indices\n",
    "\n",
    "            No assignment to self\n",
    "        \"\"\"\n",
    "        articles = [\"\"]\n",
    "        subwords_all = [[]]\n",
    "        subwords_first = [[]]\n",
    "        with open(self.news_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "            for idx in rd:\n",
    "                nid, vert, subvert, title, ab, url, _, _ = idx.strip(\"\\n\").split(\"\\t\")\n",
    "                article = \" \".join([\"[CLS]\", title, ab, vert, subvert])\n",
    "                tokens = self.tokenizer.tokenize(article)[:512]\n",
    "                # unify subwords\n",
    "                words = convert_tokens_to_words(tokens)\n",
    "                articles.append(' '.join(words))\n",
    "        \n",
    "        # rank words according to reduction rules\n",
    "        articles = reducer(articles)\n",
    "\n",
    "        article_toks = []\n",
    "        attention_masks = []\n",
    "        for article in articles:\n",
    "            tokens = self.tokenizer.tokenize(article)\n",
    "            \n",
    "            # maintain subword entry\n",
    "            subword_all = []\n",
    "            # mask subword entry\n",
    "            subword_first = []\n",
    "\n",
    "            i = -1\n",
    "            j = -1\n",
    "            for token in tokens:\n",
    "                if token.startswith('##'):\n",
    "                    j += 1\n",
    "                    # subword.append([0,0])\n",
    "                    subword_all.append([i,j])\n",
    "                    subword_first.append([0,0])\n",
    "\n",
    "                else:\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "                    subword_all.append([i,j])\n",
    "                    subword_first.append([i,j])\n",
    "\n",
    "            pad_length = self.max_news_length - len(tokens)\n",
    "\n",
    "            article_toks.append(self.tokenizer.convert_tokens_to_ids(tokens[:self.max_news_length]) + [0] * pad_length)\n",
    "            attention_masks.append([1] * min(len(tokens), self.max_news_length) + [0] * pad_length)\n",
    "            subwords_all.append(subword_all)\n",
    "            subwords_first.append(subword_first)\n",
    "\n",
    "        # encode news\n",
    "        encoded_news = np.asarray(article_toks)\n",
    "        attn_mask = np.asarray(attention_masks)\n",
    "\n",
    "        for i,subword in enumerate(subwords_all):\n",
    "            pad_length = self.max_news_length - len(subword)\n",
    "\n",
    "            subwords_all[i].extend([[0,0]] * pad_length)\n",
    "            subwords_first[i].extend([[0,0]] * pad_length)\n",
    "\n",
    "        subwords_all = np.asarray(subwords_all)\n",
    "        subwords_first = np.asarray(subwords_first)\n",
    "\n",
    "        with open(self.news_path, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"encoded_news\": encoded_news,\n",
    "                    \"subwords_first\": subwords_first,\n",
    "                    \"subwords_all\": subwords_all,\n",
    "                    \"attn_mask\": attn_mask\n",
    "                },\n",
    "                f\n",
    "            )\n",
    "\n",
    "\n",
    "    def init_behaviors(self):\n",
    "        \"\"\"\n",
    "            init behavior logs given behaviors file.\n",
    "        \"\"\"\n",
    "        # list of list of history news index\n",
    "        histories = []\n",
    "        # list of user index\n",
    "        uindexes = []\n",
    "        # list of impression indexes\n",
    "        # self.impr_indexes = []\n",
    "\n",
    "        impr_index = 0\n",
    "\n",
    "        # only store positive behavior\n",
    "        if self.mode == \"train\":\n",
    "            # list of lists, each list represents a\n",
    "            imprs = []\n",
    "            negatives = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "                    # store negative samples of each impression\n",
    "                    negative = []\n",
    "\n",
    "                    for news, label in zip(impr_news, labels):\n",
    "                        if label == 1:\n",
    "                            imprs.append((impr_index, news))\n",
    "                        else:\n",
    "                            negative.append(news)\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    negatives.append(negative)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.negatives = negatives\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"negatives\": self.negatives,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        # store every behavior\n",
    "        elif self.mode == \"dev\":\n",
    "            # list of every cdd news index along with its impression index and label\n",
    "            imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i.split(\"-\")[0]] for i in impr.split()]\n",
    "                    labels = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for i in range(0, len(impr_news), self.impr_size):\n",
    "                        imprs.append((impr_index, impr_news[i:i+self.impr_size], labels[i:i+self.impr_size]))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        # store every behavior\n",
    "        elif self.mode == \"test\":\n",
    "            # list of every cdd news index along with its impression index and label\n",
    "            imprs = []\n",
    "\n",
    "            with open(self.behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n",
    "                for idx in rd:\n",
    "                    _, uid, time, history, impr = idx.strip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "                    history = [self.nid2index[i] for i in history.split()]\n",
    "\n",
    "                    impr_news = [self.nid2index[i] for i in impr.split()]\n",
    "                    # user will always in uid2index\n",
    "                    uindex = self.uid2index[uid]\n",
    "\n",
    "                    # store every impression\n",
    "                    for i in range(0, len(impr_news), self.impr_size):\n",
    "                        imprs.append((impr_index, impr_news[i:i+self.impr_size]))\n",
    "\n",
    "                    # 1 impression correspond to 1 of each of the following properties\n",
    "                    histories.append(history)\n",
    "                    uindexes.append(uindex)\n",
    "\n",
    "                    impr_index += 1\n",
    "\n",
    "            self.imprs = imprs\n",
    "            self.histories = histories\n",
    "            self.uindexes = uindexes\n",
    "\n",
    "            save_dict = {\n",
    "                \"imprs\": self.imprs,\n",
    "                \"histories\": self.histories,\n",
    "                \"uindexes\": self.uindexes\n",
    "            }\n",
    "\n",
    "        with open(self.behav_path, \"wb\") as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "    \n",
    "\n",
    "    def init_refinement(self, refiner):\n",
    "        \"\"\"\n",
    "            token level refinement, determined by reducer\n",
    "            \n",
    "            matching -> deduplicate\n",
    "            bm25 -> truncate\n",
    "            bow -> count\n",
    "        \"\"\"\n",
    "        if not refiner:\n",
    "            return\n",
    "\n",
    "        refined_news, refined_mask = refiner(self.encoded_news, self.attn_mask)\n",
    "        if self.reducer == 'matching':\n",
    "            self.encoded_news = refined_news\n",
    "            self.attn_mask_dedup = refined_mask\n",
    "            # truncate the attention mask\n",
    "            self.attn_mask = self.attn_mask[:, :self.signal_length]\n",
    "        \n",
    "        elif self.reducer == 'bm25':\n",
    "            self.encoded_news = refined_news\n",
    "            self.attn_mask = refined_mask\n",
    "            # truncate the original text tokens\n",
    "            self.encoded_news_original = self.encoded_news_original[:, :self.signal_length]\n",
    "            self.attn_mask_original = self.attn_mask_original[:, :self.signal_length]\n",
    "\n",
    "        elif self.reducer == 'bow':\n",
    "            self.encoded_news = refined_news\n",
    "            self.attn_mask = refined_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            return length of the whole dataset\n",
    "        \"\"\"\n",
    "        return len(self.imprs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\" return data\n",
    "        Args:\n",
    "            index: the index for stored impression\n",
    "\n",
    "        Returns:\n",
    "            back_dic: dictionary of data slice\n",
    "        \"\"\"\n",
    "\n",
    "        impr = self.imprs[index] # (impression_index, news_index)\n",
    "        impr_index = impr[0]\n",
    "        impr_news = impr[1]\n",
    "\n",
    "\n",
    "        user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "        # each time called to return positive one sample and its negative samples\n",
    "        if self.mode == \"train\":\n",
    "            # user\"s unhis news in the same impression\n",
    "            negs = self.negatives[impr_index]\n",
    "            neg_list, neg_num = newsample(negs, self.npratio)\n",
    "\n",
    "            cdd_ids = [impr_news] + neg_list\n",
    "            cdd_size = self.npratio + 1\n",
    "\n",
    "            label = np.asarray([1] + [0] * self.npratio)\n",
    "\n",
    "            if self.shuffle_pos:\n",
    "                s = np.arange(0, len(label), 1)\n",
    "                np.random.shuffle(s)\n",
    "                cdd_ids = np.asarray(cdd_ids)[s]\n",
    "                label = np.asarray(label)[s]\n",
    "\n",
    "            label = np.arange(0, len(cdd_ids), 1)[label == 1][0]\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "\n",
    "            cdd_mask = torch.zeros((cdd_size, 1))\n",
    "            cdd_mask[:neg_num + 1] = 1\n",
    "\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids]\n",
    "            his_encoded_index = self.encoded_news[his_ids]\n",
    "            his_attn_mask = self.attn_mask[his_ids]\n",
    "\n",
    "            back_dic = {\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"cdd_mask\": cdd_mask,\n",
    "                \"his_mask\": his_mask,\n",
    "                \"label\": label\n",
    "            }\n",
    "\n",
    "            if self.subwords is not None:\n",
    "                cdd_subword_index = self.subwords[cdd_ids]\n",
    "                his_subword_index = self.subwords[his_ids]\n",
    "                back_dic[\"cdd_subword_index\"] = cdd_subword_index\n",
    "                back_dic[\"his_subword_index\"] = his_subword_index\n",
    "\n",
    "            if self.reducer == \"matching\":\n",
    "                his_attn_mask_dedup = self.attn_mask_dedup[his_ids]\n",
    "                back_dic[\"his_refined_mask\"] = his_attn_mask_dedup\n",
    "\n",
    "            elif self.reducer == \"bm25\":\n",
    "                back_dic[\"cdd_encoded_index\"] = self.encoded_news_original[cdd_ids]\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        # each time called return one sample, and no labels\n",
    "        elif self.mode == \"dev\":\n",
    "            cdd_ids = impr_news\n",
    "            cdd_size = len(cdd_ids)\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            user_index = [self.uindexes[impr_index]]\n",
    "            label = impr[2]\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids]\n",
    "            his_encoded_index = self.encoded_news[his_ids]\n",
    "            his_attn_mask = self.attn_mask[his_ids]\n",
    "\n",
    "            back_dic = {\n",
    "                \"impr_index\": impr_index + 1,\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"his_mask\": his_mask,\n",
    "                \"label\": np.asarray(label)\n",
    "            }\n",
    "\n",
    "            if self.subwords is not None:\n",
    "                cdd_subword_index = self.subwords[cdd_ids]\n",
    "                his_subword_index = self.subwords[his_ids]\n",
    "                back_dic[\"cdd_subword_index\"] = cdd_subword_index\n",
    "                back_dic[\"his_subword_index\"] = his_subword_index\n",
    "\n",
    "            if self.reducer == \"matching\":\n",
    "                his_attn_mask_dedup = self.attn_mask_dedup[his_ids]\n",
    "                back_dic[\"his_refined_mask\"] = his_attn_mask_dedup\n",
    "\n",
    "            elif self.reducer == \"bm25\":\n",
    "                back_dic[\"cdd_encoded_index\"] = self.encoded_news_original[cdd_ids]\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        elif self.mode == \"test\":\n",
    "            cdd_ids = impr_news\n",
    "            cdd_size = len(cdd_ids)\n",
    "\n",
    "            his_ids = self.histories[impr_index][:self.his_size]\n",
    "            # true means the corresponding history news is padded\n",
    "            his_mask = torch.zeros((self.his_size, 1), dtype=bool)\n",
    "            his_mask[:len(his_ids)] = 1\n",
    "\n",
    "            if self.ascend_history:\n",
    "                his_ids = his_ids + [0] * (self.his_size - len(his_ids))\n",
    "            else:\n",
    "                his_ids = his_ids[::-1] + [0] * (self.his_size - len(his_ids))\n",
    "\n",
    "            user_index = [self.uindexes[impr_index]]\n",
    "\n",
    "            cdd_encoded_index = self.encoded_news[cdd_ids]\n",
    "            cdd_attn_mask = self.attn_mask[cdd_ids]\n",
    "            his_encoded_index = self.encoded_news[his_ids]\n",
    "            his_attn_mask = self.attn_mask[his_ids]\n",
    "\n",
    "            back_dic = {\n",
    "                \"impr_index\": impr_index + 1,\n",
    "                \"user_index\": np.asarray(user_index),\n",
    "                \"cdd_id\": np.asarray(cdd_ids),\n",
    "                \"his_id\": np.asarray(his_ids),\n",
    "                \"cdd_encoded_index\": cdd_encoded_index,\n",
    "                \"his_encoded_index\": his_encoded_index,\n",
    "                \"cdd_attn_mask\": cdd_attn_mask,\n",
    "                \"his_attn_mask\": his_attn_mask,\n",
    "                \"his_mask\": his_mask,\n",
    "            }\n",
    "\n",
    "            if self.subwords is not None:\n",
    "                cdd_subword_index = self.subwords[cdd_ids]\n",
    "                his_subword_index = self.subwords[his_ids]\n",
    "                back_dic[\"cdd_subword_index\"] = cdd_subword_index\n",
    "                back_dic[\"his_subword_index\"] = his_subword_index\n",
    "\n",
    "            if self.reducer == \"matching\":\n",
    "                his_attn_mask_dedup = self.attn_mask_dedup[his_ids]\n",
    "                back_dic[\"his_refined_mask\"] = his_attn_mask_dedup\n",
    "\n",
    "            elif self.reducer == \"bm25\":\n",
    "                back_dic[\"cdd_encoded_index\"] = self.encoded_news_original[cdd_ids]\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            elif self.reducer == \"bow\":\n",
    "                # placeholder\n",
    "                back_dic[\"his_refined_mask\"] = None\n",
    "\n",
    "            return back_dic\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Mode {} not defined\".format(self.mode))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# config.reducer = 'bm25'\n",
    "config.reducer = 'bow'\n",
    "# config.reducer = 'matching'\n",
    "\n",
    "# config.signal_length = 10\n",
    "# config.scale = 'large'\n",
    "# config.impr_size = 100\n",
    "# config.mode = 'test'\n",
    "\n",
    "path = config.path + 'MIND/MINDdemo_train/'\n",
    "a = MIND(config, path + 'news.tsv', path + 'behaviors.tsv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-31 19:54:51,838] INFO (__main__) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-08-31 19:54:51,856] INFO (__main__) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-08-31 19:54:52,609] INFO (__main__) reducing news of ../../../Data/MIND/MINDdemo_train/news.tsv...\n",
      "[2021-08-31 19:54:52,610] INFO (utils.utils) reducing to Bag-of-Words...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "a[1]['his_encoded_index'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50, 100, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "r = np.array([[[1,1],[2,2],[2,3]],[[1,1],[1,2],[1,3]]])\n",
    "r[r[:,:,0] == 1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "t.convert_ids_to_tokens(a.encoded_news[3][:110])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'trump',\n",
       " \"'\",\n",
       " 's',\n",
       " 'aid',\n",
       " 'freeze',\n",
       " 'in',\n",
       " 'the',\n",
       " 'trenches',\n",
       " 'of',\n",
       " 'ukraine',\n",
       " \"'\",\n",
       " 's',\n",
       " 'war',\n",
       " 'lt',\n",
       " '.',\n",
       " 'ivan',\n",
       " 'mo',\n",
       " '##lch',\n",
       " '##ane',\n",
       " '##ts',\n",
       " 'peeked',\n",
       " 'over',\n",
       " 'a',\n",
       " 'parapet',\n",
       " 'of',\n",
       " 'sand',\n",
       " 'bags',\n",
       " 'at',\n",
       " 'the',\n",
       " 'front',\n",
       " 'line',\n",
       " 'of',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'ukraine',\n",
       " '.',\n",
       " 'next',\n",
       " 'to',\n",
       " 'him',\n",
       " 'was',\n",
       " 'an',\n",
       " 'empty',\n",
       " 'helmet',\n",
       " 'propped',\n",
       " 'up',\n",
       " 'to',\n",
       " 'trick',\n",
       " 'sniper',\n",
       " '##s',\n",
       " ',',\n",
       " 'already',\n",
       " 'per',\n",
       " '##for',\n",
       " '##ated',\n",
       " 'with',\n",
       " 'multiple',\n",
       " 'holes',\n",
       " '.',\n",
       " 'news',\n",
       " 'news',\n",
       " '##world',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a.subwords[3][:110]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 1,  1],\n",
       "       [ 2,  2],\n",
       "       [ 3,  3],\n",
       "       [ 4,  4],\n",
       "       [ 5,  5],\n",
       "       [ 6,  6],\n",
       "       [ 7,  7],\n",
       "       [ 8,  8],\n",
       "       [ 9,  9],\n",
       "       [10, 10],\n",
       "       [11, 11],\n",
       "       [12, 12],\n",
       "       [13, 13],\n",
       "       [14, 14],\n",
       "       [15, 15],\n",
       "       [16, 16],\n",
       "       [17, 17],\n",
       "       [18, 18],\n",
       "       [19, 19],\n",
       "       [20, 20],\n",
       "       [20, 21],\n",
       "       [20, 22],\n",
       "       [20, 23],\n",
       "       [21, 24],\n",
       "       [22, 25],\n",
       "       [23, 26],\n",
       "       [24, 27],\n",
       "       [25, 28],\n",
       "       [26, 29],\n",
       "       [27, 30],\n",
       "       [28, 31],\n",
       "       [29, 32],\n",
       "       [30, 33],\n",
       "       [31, 34],\n",
       "       [32, 35],\n",
       "       [33, 36],\n",
       "       [34, 37],\n",
       "       [35, 38],\n",
       "       [36, 39],\n",
       "       [37, 40],\n",
       "       [38, 41],\n",
       "       [39, 42],\n",
       "       [40, 43],\n",
       "       [41, 44],\n",
       "       [42, 45],\n",
       "       [43, 46],\n",
       "       [44, 47],\n",
       "       [45, 48],\n",
       "       [46, 49],\n",
       "       [47, 50],\n",
       "       [48, 51],\n",
       "       [49, 52],\n",
       "       [49, 53],\n",
       "       [50, 54],\n",
       "       [51, 55],\n",
       "       [52, 56],\n",
       "       [52, 57],\n",
       "       [52, 58],\n",
       "       [53, 59],\n",
       "       [54, 60],\n",
       "       [55, 61],\n",
       "       [56, 62],\n",
       "       [57, 63],\n",
       "       [58, 64],\n",
       "       [58, 65],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0],\n",
       "       [ 0,  0]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loader1 = DataLoader(a, batch_size=1, pin_memory=False, num_workers=0, drop_last=False, shuffle=False, sampler=Partition_Sampler(a,2,0))\n",
    "records1 = list(loader1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "records1[0]['his_attn_mask']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dic = pickle.load(open('/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news.pkl', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sb = dic['subwords_first']\n",
    "sb"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([list([]),\n",
       "       list([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9], [10, 10], [11, 11], [12, 12], [13, 13], [14, 14], [15, 15], [16, 16], [0, 0], [17, 18], [18, 19], [19, 20], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [29, 30], [30, 31], [31, 32], [32, 33], [0, 0], [0, 0]]),\n",
       "       list([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9], [10, 10], [11, 11], [12, 12], [13, 13], [14, 14], [15, 15], [16, 16], [17, 17], [18, 18], [19, 19], [0, 0], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [0, 0], [0, 0]]),\n",
       "       ...,\n",
       "       list([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9], [10, 10], [11, 11], [12, 12], [13, 13], [14, 14], [15, 15], [16, 16], [17, 17], [18, 18], [19, 19], [0, 0], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [29, 30], [30, 31], [31, 32], [32, 33], [33, 34], [34, 35], [35, 36], [36, 37], [37, 38], [38, 39], [39, 40], [40, 41], [41, 42], [42, 43], [43, 44], [44, 45], [45, 46], [46, 47], [47, 48], [48, 49], [49, 50], [50, 51], [51, 52], [52, 53], [53, 54], [54, 55], [55, 56], [56, 57], [57, 58], [58, 59], [59, 60], [60, 61], [61, 62], [62, 63], [63, 64], [64, 65], [65, 66], [66, 67], [67, 68], [68, 69], [69, 70], [70, 71], [0, 0], [71, 73], [72, 74], [73, 75], [74, 76], [75, 77], [76, 78], [77, 79], [78, 80], [79, 81], [80, 82], [81, 83], [82, 84], [83, 85], [84, 86], [85, 87], [86, 88], [87, 89], [88, 90], [89, 91], [90, 92], [91, 93], [92, 94], [93, 95], [94, 96], [95, 97], [96, 98], [97, 99], [98, 100], [99, 101], [100, 102], [101, 103], [102, 104], [103, 105], [104, 106], [105, 107], [106, 108], [107, 109], [108, 110], [109, 111], [110, 112], [111, 113], [112, 114], [113, 115], [114, 116], [115, 117], [116, 118], [117, 119], [118, 120]]),\n",
       "       list([[0, 0], [1, 1], [2, 2], [3, 3], [0, 0], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [12, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 18], [18, 19], [0, 0], [19, 21], [20, 22], [21, 23], [22, 24], [23, 25], [24, 26], [0, 0]]),\n",
       "       list([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [0, 0], [8, 9], [0, 0], [0, 0]])],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "np.unique(a[:,0]),a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [2, 3],\n",
       "        [3, 4],\n",
       "        [4, 5],\n",
       "        [5, 6],\n",
       "        [6, 7],\n",
       "        [7, 8],\n",
       "        [8, 9]]))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "t.tokenize(\"I don't give a fuck\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i', 'don', \"'\", 't', 'give', 'a', 'fuck']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}