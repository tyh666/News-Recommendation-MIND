{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "# from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.BaseModel import BaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "\n",
    "t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "# config.bert = 'longformer'\n",
    "# config.seed = None\n",
    "config.mode = \"inspect\"\n",
    "config.recall_type = \"s\"\n",
    "config.scale = \"large\"\n",
    "config.case = True\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "# X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-11-08 08:05:30,871] INFO (utils.Manager) Hyper Parameters are \n",
      "{\n",
      "    \"scale\": \"large\",\n",
      "    \"mode\": \"inspect\",\n",
      "    \"batch_size\": 5,\n",
      "    \"batch_size_news\": 100,\n",
      "    \"batch_size_history\": 100,\n",
      "    \"k\": 3,\n",
      "    \"threshold\": -Infinity,\n",
      "    \"abs_length\": 40,\n",
      "    \"signal_length\": 100,\n",
      "    \"news\": null,\n",
      "    \"his_size\": 50,\n",
      "    \"cdd_size\": 5,\n",
      "    \"impr_size\": 10,\n",
      "    \"dropout_p\": 0.2,\n",
      "    \"lr\": 0.0001,\n",
      "    \"bert_lr\": 3e-05,\n",
      "    \"embedding\": \"bert\",\n",
      "    \"encoderN\": \"cnn\",\n",
      "    \"encoderU\": \"lstm\",\n",
      "    \"selector\": \"sfi\",\n",
      "    \"reducer\": \"matching\",\n",
      "    \"ranker\": \"onepass\",\n",
      "    \"pooler\": \"attn\",\n",
      "    \"bert_dim\": 768,\n",
      "    \"embedding_dim\": 768,\n",
      "    \"hidden_dim\": 384,\n",
      "    \"base_rank\": 0,\n",
      "    \"world_size\": 0,\n",
      "    \"seed\": 42,\n",
      "    \"granularity\": \"token\",\n",
      "    \"debias\": false,\n",
      "    \"full_attn\": true,\n",
      "    \"descend_history\": false,\n",
      "    \"shuffle_pos\": false,\n",
      "    \"save_pos\": false,\n",
      "    \"sep_his\": false,\n",
      "    \"diversify\": false,\n",
      "    \"no_dedup\": false,\n",
      "    \"segment_embed\": false,\n",
      "    \"no_rm_punc\": false,\n",
      "    \"fast\": false,\n",
      "    \"scheduler\": \"linear\",\n",
      "    \"warmup\": 100,\n",
      "    \"shuffle\": false,\n",
      "    \"bert\": \"bert\",\n",
      "    \"tb\": false,\n",
      "    \"recall_type\": \"s\",\n",
      "    \"case\": true\n",
      "}\n",
      "[2021-11-08 08:05:30,872] INFO (utils.Manager) preparing dataset...\n",
      "[2021-11-08 08:05:30,872] INFO (utils.MIND) encoding user behaviors of data/case/behaviors.tsv...\n",
      "2it [00:00, 10894.30it/s]\n",
      "[2021-11-08 08:05:31,380] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDlarge_dev/news.pkl\n",
      "[2021-11-08 08:05:31,972] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Matching_Reducer(nn.Module):\n",
    "    \"\"\"\n",
    "    select top k terms from each historical news with max cosine similarity\n",
    "\n",
    "    1. keep the first K terms unmasked\n",
    "    2. add order embedding to terms from different historical news\n",
    "    3. insert [SEP] token to separate terms from different news if called\n",
    "    \"\"\"\n",
    "    def __init__(self, manager):\n",
    "        super().__init__()\n",
    "        self.name = \"matching\"\n",
    "        self.k = manager.k\n",
    "        self.his_size = manager.his_size\n",
    "        self.embedding_dim = manager.embedding_dim\n",
    "\n",
    "        self.diversify = manager.diversify\n",
    "        self.sep_his = manager.sep_his\n",
    "        # if aggregator is enabled, do not flatten the personalized terms\n",
    "        self.flatten = (manager.aggregator is None)\n",
    "\n",
    "        manager.term_num = manager.k * manager.his_size\n",
    "\n",
    "        # strip [CLS]\n",
    "        keep_k_modifier = torch.zeros(1, manager.signal_length - 1)\n",
    "        keep_k_modifier[:, :self.k] = 1\n",
    "        self.register_buffer('keep_k_modifier', keep_k_modifier, persistent=False)\n",
    "\n",
    "        if self.diversify:\n",
    "            self.newsUserAlign = nn.Linear(manager.hidden_dim * 2, manager.hidden_dim)\n",
    "            nn.init.xavier_normal_(self.newsUserAlign.weight)\n",
    "\n",
    "        if manager.threshold != -float('inf'):\n",
    "            threshold = torch.tensor([manager.threshold])\n",
    "            self.register_buffer('threshold', threshold)\n",
    "\n",
    "        if self.sep_his:\n",
    "            manager.term_num += (self.his_size - 1)\n",
    "            self.sep_embedding = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
    "            self.register_buffer('extra_sep_mask', torch.ones(1, 1, 1), persistent=False)\n",
    "\n",
    "        if manager.segment_embed:\n",
    "            self.segment_embedding = nn.Parameter(torch.randn(manager.his_size, 1, manager.embedding_dim))\n",
    "            nn.init.xavier_normal_(self.segment_embedding)\n",
    "\n",
    "\n",
    "    def forward(self, news_selection_embedding, news_embedding, user_repr, news_repr, his_attn_mask, his_refined_mask):\n",
    "        \"\"\"\n",
    "        Extract words from news text according to the overall user interest\n",
    "\n",
    "        Args:\n",
    "            news_selection_embedding: encoded word-level embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_embedding: word-level news embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_repr: news-level representation, [batch_size, his_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "            his_refined_mask: dedupicated attention mask, [batch_size, his_size, signal_length]\n",
    "        Returns:\n",
    "            ps_terms: weighted embedding for personalized terms, [batch_size, term_num, embedding_dim]\n",
    "            ps_term_mask: attention mask of output terms, [batch_size, term_num]\n",
    "            kid: the index of personalized terms\n",
    "        \"\"\"\n",
    "        batch_size = news_embedding.size(0)\n",
    "\n",
    "        if self.diversify:\n",
    "            news_user_repr = torch.cat([user_repr.expand(news_repr.size()), news_repr], dim=-1)\n",
    "            selection_query = self.newsUserAlign(news_user_repr).unsqueeze(-1)\n",
    "        else:\n",
    "            selection_query = user_repr.unsqueeze(-1)\n",
    "\n",
    "        news_selection_embedding = news_selection_embedding[:, :, 1:]\n",
    "\n",
    "        news_embedding_text = news_embedding[:, :, 1:]\n",
    "        his_attn_mask = his_attn_mask[:, :, 1:]\n",
    "\n",
    "        # [bs, hs, sl - 1]\n",
    "        scores = F.normalize(news_selection_embedding, dim=-1).matmul(F.normalize(selection_query, dim=-2)).squeeze(-1)\n",
    "        # scores = news_selection_embedding.matmul(selection_query).squeeze(-1)/math.sqrt(selection_query.size(-1))\n",
    "        pad_pos = ~((his_refined_mask[:, :, 1:] + self.keep_k_modifier).bool())\n",
    "\n",
    "        # mask the padded term\n",
    "        scores = scores.masked_fill(pad_pos, -float('inf'))\n",
    "\n",
    "        score_k, score_kid = scores.topk(dim=-1, k=self.k)\n",
    "\n",
    "        ps_terms = news_embedding_text.gather(dim=-2,index=score_kid.unsqueeze(-1).expand(*score_kid.size(), news_embedding_text.size(-1)))\n",
    "        # [bs, hs, k]\n",
    "        ps_term_mask = his_attn_mask.gather(dim=-1, index=score_kid)\n",
    "\n",
    "        if hasattr(self, 'threshold'):\n",
    "            mask_pos = score_k < self.threshold\n",
    "            # ps_terms = personalized_terms * (nn.functional.softmax(score_k.masked_fill(score_k < self.threshold, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_terms = ps_terms * (F.softmax(score_k.masked_fill(mask_pos, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_term_mask = ps_term_mask * (~mask_pos)\n",
    "        else:\n",
    "            ps_terms = ps_terms * (F.softmax(score_k, dim=-1).unsqueeze(-1))\n",
    "\n",
    "        if hasattr(self, 'segment_embedding'):\n",
    "            ps_terms += self.segment_embedding\n",
    "\n",
    "        # flatten the selected terms into one dimension\n",
    "        if self.flatten:\n",
    "            # separate historical news only practical when squeeze=True\n",
    "            if self.sep_his:\n",
    "                # [bs, hs, ed]\n",
    "                sep_embedding = self.sep_embedding.expand(batch_size, self.his_size, 1, self.embedding_dim)\n",
    "                # add extra [SEP] token to separate terms from different history news, slice to -1 to strip off the last [SEP]\n",
    "                ps_terms = torch.cat([ps_terms, sep_embedding], dim=-2).view(batch_size, -1, self.embedding_dim)[:, :-1]\n",
    "                ps_term_mask = torch.cat([ps_term_mask, self.extra_sep_mask.expand(batch_size, self.his_size, 1)], dim=-1).view(batch_size, -1)[:, :-1]\n",
    "\n",
    "            else:\n",
    "                # [bs, 1, ed]\n",
    "                ps_terms = ps_terms.reshape(batch_size, -1, self.embedding_dim)\n",
    "                ps_term_mask = ps_term_mask.reshape(batch_size, -1)\n",
    "\n",
    "        return ps_terms, ps_term_mask, score_kid, scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Two tower baseline\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TESRec(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, embedding, encoderN, encoderU, reducer, aggregator=None):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        # only these reducers need selection encoding\n",
    "        if manager.reducer in manager.get_need_encode_reducers():\n",
    "            self.encoderN = encoderN\n",
    "            self.encoderU = encoderU\n",
    "        self.reducer = reducer\n",
    "        self.aggregator = aggregator\n",
    "        self.bert = BERT_Encoder(manager)\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,self.bert.hidden_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        if aggregator is not None:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.aggregator, manager.granularity, str(manager.k)])\n",
    "        else:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.granularity, str(manager.k)])\n",
    "\n",
    "        self.name = manager.name\n",
    "\n",
    "\n",
    "    def encode_news(self, x):\n",
    "        \"\"\"\n",
    "        encode candidate news\n",
    "        \"\"\"\n",
    "        # encode news with MIND_news\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_dest = self.cdd_dest[:batch_size]\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, 0] * self.signal_length + cdd_subword_index[:, :, 1]\n",
    "\n",
    "            cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        _, cdd_news_repr, _ = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "\n",
    "        return cdd_news_repr\n",
    "\n",
    "\n",
    "    def encode_user(self, x):\n",
    "        \"\"\"\n",
    "        encoder user\n",
    "        \"\"\"\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['his_encoded_index'].size(0)\n",
    "            his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            his_subword_prefix = None\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        if hasattr(self, 'encoderN'):\n",
    "            his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "                his_news_embedding, his_attn_mask\n",
    "            )\n",
    "        else:\n",
    "            his_news_encoded_embedding = None\n",
    "            his_news_repr = None\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr_ext = self.encoderU(his_news_repr, his_mask=x['his_mask'].to(self.device), user_index=x['user_id'].to(self.device))\n",
    "        else:\n",
    "            user_repr_ext = None\n",
    "\n",
    "        ps_terms, ps_term_mask, kid, scores = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr_ext, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        _, user_repr, _ = self.bert(ps_terms, ps_term_mask, ps_term_input=True)\n",
    "\n",
    "        if self.aggregator is not None:\n",
    "            user_repr = self.aggregator(user_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return user_repr, kid, his_news_encoded_embedding, user_repr_ext, scores\n",
    "\n",
    "\n",
    "    def compute_score(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        cdd_repr = self.encode_news(x)\n",
    "        user_repr, kid = self.encode_user(x)\n",
    "\n",
    "        score = self.compute_score(cdd_repr, user_repr)\n",
    "\n",
    "        if self.training:\n",
    "            logits = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            logits = torch.sigmoid(score)\n",
    "\n",
    "        return logits, kid\n",
    "\n",
    "\n",
    "    def predict_fast(self, x):\n",
    "        # [bs, cs, hd]\n",
    "        cdd_repr = self.news_reprs(x['cdd_id'].to(self.device))\n",
    "        user_repr, _ = self.encode_user(x)\n",
    "        scores = self.compute_score(cdd_repr, user_repr)\n",
    "        logits = torch.sigmoid(scores)\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "# model = XFormer(manager).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "manager.load(model, 230000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-11-08 08:24:01,966] INFO (utils.Manager) loading model from data/model_params/tesrec__bert__cnn__lstm__matching__token__3/large_step230000.model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "a,b,c,d,e = model.encode_user(x1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "np.asarray(t.convert_ids_to_tokens(x1['his_encoded_index'][0,0])[1:])[[86,2,93]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['##gb', 'fix', 'he'], dtype='<U11')"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model.encoderN.cnn.weight"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.0491, -0.0237,  0.0978],\n",
       "         [ 0.0875,  0.0161, -0.0656],\n",
       "         [ 0.0285, -0.0399,  0.0502],\n",
       "         ...,\n",
       "         [-0.0014,  0.0307, -0.0174],\n",
       "         [ 0.0100, -0.0201,  0.0153],\n",
       "         [ 0.0079,  0.0301, -0.0928]],\n",
       "\n",
       "        [[-0.0331, -0.0284,  0.0490],\n",
       "         [-0.0505, -0.0271, -0.0014],\n",
       "         [ 0.0446,  0.0101,  0.0496],\n",
       "         ...,\n",
       "         [ 0.0815, -0.0096,  0.0206],\n",
       "         [ 0.0013,  0.0414,  0.0997],\n",
       "         [-0.0093,  0.0101,  0.0060]],\n",
       "\n",
       "        [[ 0.0264, -0.0080, -0.0189],\n",
       "         [ 0.0627,  0.0110, -0.0748],\n",
       "         [ 0.0237,  0.0505,  0.0461],\n",
       "         ...,\n",
       "         [-0.0432, -0.0009,  0.0765],\n",
       "         [-0.0431, -0.0080, -0.0108],\n",
       "         [ 0.0030, -0.0535,  0.0183]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0090, -0.0659, -0.0015],\n",
       "         [ 0.0181,  0.0028, -0.0396],\n",
       "         [ 0.0556,  0.0066, -0.0121],\n",
       "         ...,\n",
       "         [ 0.0344,  0.0430,  0.0057],\n",
       "         [ 0.0281,  0.0134, -0.0368],\n",
       "         [-0.0003, -0.0285,  0.0063]],\n",
       "\n",
       "        [[-0.0286, -0.0096,  0.0326],\n",
       "         [-0.0006, -0.0512, -0.0110],\n",
       "         [-0.0017, -0.0063, -0.0343],\n",
       "         ...,\n",
       "         [ 0.0165, -0.0096,  0.0090],\n",
       "         [ 0.0365, -0.0222,  0.0153],\n",
       "         [ 0.0028,  0.0270, -0.0192]],\n",
       "\n",
       "        [[-0.0420,  0.0234, -0.0254],\n",
       "         [ 0.0135, -0.0259,  0.0274],\n",
       "         [ 0.0342,  0.1137,  0.0521],\n",
       "         ...,\n",
       "         [-0.0738, -0.0649, -0.0214],\n",
       "         [-0.0270, -0.0690,  0.0290],\n",
       "         [ 0.0582,  0.0489, -0.0083]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "d[0,0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-4.4400e-03,  6.3619e-04, -2.2063e-01,  1.8354e-04,  3.8074e-04,\n",
       "        -8.7050e-04,  3.6028e-04,  3.1499e-04,  9.9705e-05,  5.7517e-04,\n",
       "        -2.1719e-02,  5.3033e-05,  5.7555e-05, -4.0111e-04, -1.7791e-03,\n",
       "         5.1019e-04, -2.2478e-04,  3.9305e-04, -3.9745e-01, -7.8493e-03,\n",
       "         2.3317e-04, -2.2985e-03,  8.6744e-01,  3.8022e-04, -7.0838e-01,\n",
       "         1.2778e-04, -5.1304e-01,  1.8707e-04, -1.3358e-04,  4.0864e-05,\n",
       "         2.8037e-03, -1.3580e-03, -9.4156e-04,  4.6346e-01, -1.2117e-03,\n",
       "        -5.8578e-02, -9.8103e-04, -6.7563e-04,  2.5512e-03, -6.6214e-01,\n",
       "         1.5321e-05, -1.7117e-01, -1.9915e-01, -1.5334e-04, -5.9781e-03,\n",
       "        -6.9705e-05, -1.6098e-02, -2.7902e-03, -2.5514e-01,  4.5431e-05,\n",
       "         1.7989e-03, -1.2389e-04, -4.8516e-04, -8.4373e-04, -2.3518e-04,\n",
       "        -2.6455e-03,  5.0970e-04,  3.8207e-02, -1.5263e-03, -2.8502e-05,\n",
       "         2.3914e-04,  9.7725e-05,  3.3706e-03,  1.7348e-04, -1.8785e-04,\n",
       "        -1.6702e-03, -2.8307e-04,  7.0267e-04, -7.6970e-05, -7.7917e-04,\n",
       "        -1.6547e-03,  2.5399e-05, -4.4893e-04, -6.3439e-01, -7.2008e-04,\n",
       "        -1.1508e-04,  2.8335e-04, -1.8958e-03, -2.4555e-03,  1.5955e-03,\n",
       "        -5.8115e-04, -6.3274e-01, -1.4459e-03, -5.6833e-02,  1.5761e-03,\n",
       "         1.6483e-03, -8.3075e-04, -2.0943e-02,  3.1525e-04,  1.3023e-04,\n",
       "         4.4024e-03, -1.8407e-04,  1.0656e-04,  5.2911e-04,  5.5386e-04,\n",
       "        -6.0786e-04, -4.8390e-03,  6.1511e-04, -7.9869e-04,  1.2165e-03,\n",
       "         9.8275e-04,  2.8133e-03, -6.2498e-04,  1.3707e-04, -3.2468e-05,\n",
       "         6.5952e-04, -1.9942e-04, -1.7507e-04,  5.7203e-04,  4.3327e-04,\n",
       "         4.3545e-04, -2.7784e-04,  1.4452e-03,  2.4218e-04,  6.7071e-02,\n",
       "         6.7515e-03,  3.5552e-04,  2.6192e-04, -1.3769e-04,  2.4517e-02,\n",
       "        -1.5484e-04, -5.0484e-04, -1.4623e-04,  5.4647e-04,  1.9608e-03,\n",
       "         2.1880e-04,  7.7003e-01,  1.3457e-05, -6.7518e-04,  7.3888e-02,\n",
       "         2.1534e-04, -2.6180e-03, -2.4303e-03, -2.6824e-05, -4.9391e-04,\n",
       "        -4.1104e-04,  2.1098e-04,  1.3511e-04,  1.3037e-02,  3.0002e-03,\n",
       "        -9.3141e-05, -2.9039e-03, -1.5339e-01,  7.3407e-04, -7.0989e-01,\n",
       "        -2.7571e-04, -2.0662e-04, -7.6671e-04,  8.8066e-01, -1.1705e-01,\n",
       "        -4.9542e-03,  2.3418e-03, -3.0060e-03,  1.1296e-04, -1.1261e-03,\n",
       "         2.9278e-03,  3.3018e-04,  7.0217e-03,  8.1364e-04, -2.1828e-04,\n",
       "        -1.1840e-01,  4.6189e-05,  2.5365e-03,  5.5020e-04, -1.1364e-03,\n",
       "         5.0586e-03, -2.4299e-04,  9.4704e-03,  9.5552e-03,  2.5252e-05,\n",
       "         3.3854e-04,  7.5021e-03, -3.8910e-04, -3.8515e-05, -2.0539e-04,\n",
       "         9.1059e-01, -1.5426e-04, -2.5973e-03,  1.3598e-04,  3.2450e-04,\n",
       "         2.5303e-04,  5.6334e-04,  1.6303e-03,  2.1722e-02, -5.5242e-01,\n",
       "        -2.0964e-01, -7.6234e-01,  1.8869e-03,  9.5699e-05,  5.8852e-04,\n",
       "        -5.8385e-01,  6.6949e-04, -8.4111e-04,  6.2119e-01,  3.1926e-04,\n",
       "         3.0953e-04, -6.0369e-02,  1.2068e-03, -4.3730e-04, -2.2518e-04,\n",
       "        -8.5266e-02, -9.2881e-04, -2.0927e-02, -2.2214e-02, -1.3027e-02,\n",
       "         5.3591e-04,  4.2126e-04, -1.7573e-04,  1.3932e-03,  1.7330e-03,\n",
       "        -1.1426e-03, -2.2454e-03,  7.5874e-03,  3.6003e-03, -8.3593e-05,\n",
       "         2.2298e-04, -1.5954e-03, -6.4935e-02,  8.2956e-02, -1.1698e-04,\n",
       "         1.5051e-03, -4.8065e-04, -6.5472e-05, -2.7451e-03, -4.1120e-04,\n",
       "         2.1320e-02,  9.1119e-04,  6.4310e-05, -3.5626e-03, -1.1308e-03,\n",
       "         4.1152e-04,  6.4644e-01, -1.3152e-03, -4.6483e-03,  2.8366e-04,\n",
       "         2.5057e-04, -3.0823e-04, -1.6317e-03,  4.7555e-04, -2.1050e-03,\n",
       "        -8.1176e-01, -3.1614e-03, -4.5745e-04, -4.5458e-03,  2.1589e-03,\n",
       "         4.0263e-03, -5.5090e-04, -1.0861e-03,  6.7844e-03,  2.4362e-03,\n",
       "         1.2294e-03,  9.9786e-05, -1.9310e-01,  2.2840e-03,  4.8814e-04,\n",
       "        -5.8353e-02,  1.1208e-02, -1.5084e-03,  1.1442e-03, -4.3338e-02,\n",
       "        -3.3847e-05, -1.3993e-03,  1.4263e-03,  2.8246e-04, -1.6120e-03,\n",
       "         1.8260e-04, -1.5363e-04,  2.4276e-03, -1.2813e-02, -1.1920e-03,\n",
       "         8.1835e-03, -1.1354e-01, -4.1977e-01, -8.3584e-04, -5.6663e-03,\n",
       "        -4.5488e-02, -1.2489e-02,  9.2356e-03,  3.3481e-04,  5.6417e-04,\n",
       "        -8.0777e-04,  1.2174e-03, -1.7377e-01, -3.0877e-04, -6.3723e-04,\n",
       "         1.4250e-03, -1.3187e-02,  1.4420e-03, -3.5201e-04,  4.2517e-05,\n",
       "        -7.1644e-05,  3.1864e-03,  1.8712e-03,  1.6910e-03, -2.0986e-04,\n",
       "        -1.0980e-01, -7.9094e-03, -5.3472e-04, -1.0313e-02, -8.6877e-04,\n",
       "        -3.8753e-02, -1.3509e-03, -1.1627e-04, -1.3114e-03,  4.4170e-04,\n",
       "        -1.1591e-03,  1.6692e-03, -7.8442e-05,  2.1432e-04, -6.5713e-05,\n",
       "         4.2526e-04, -1.8176e-02,  2.5925e-04,  7.7909e-01, -1.1541e-04,\n",
       "        -5.1764e-04,  7.4828e-04,  1.2389e-04, -2.2297e-03, -6.4735e-04,\n",
       "         6.7624e-01, -2.9583e-03, -6.9727e-04, -3.3267e-03,  8.7540e-04,\n",
       "        -7.1336e-04, -1.3815e-04,  2.7626e-04, -6.5963e-01,  6.6096e-06,\n",
       "         6.9572e-02, -2.8870e-04, -5.8485e-04,  8.0474e-01, -7.3325e-01,\n",
       "        -4.4873e-04,  6.4166e-04,  9.9335e-04, -1.2614e-05, -1.8860e-04,\n",
       "         6.7919e-01,  7.9348e-04, -4.2942e-04,  3.5900e-03,  2.1730e-04,\n",
       "         4.4338e-04,  4.4741e-05,  1.4628e-03, -5.7153e-01, -2.3641e-02,\n",
       "         6.3088e-04, -6.1008e-04, -1.8779e-03, -2.8277e-03,  5.4971e-05,\n",
       "         9.9280e-05, -2.3945e-03,  8.4995e-01,  5.0611e-04,  6.3437e-04,\n",
       "         1.4343e-03, -1.5075e-04,  3.6132e-03,  3.8110e-04, -1.0237e-03,\n",
       "         2.9692e-02, -1.0167e-03,  1.3919e-03, -4.2328e-03, -5.4713e-03,\n",
       "        -3.8531e-04,  9.6919e-01,  5.3418e-04,  5.3675e-04, -2.6670e-03,\n",
       "        -4.0771e-03,  3.5289e-02, -9.0098e-04, -1.6817e-03, -1.1840e-02,\n",
       "         1.3338e-03,  1.5788e-04,  3.1939e-04, -7.7705e-03], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "e[0,3], e.shape, e.topk(dim=-1, k=3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([ 0.2419, -0.5074, -0.6057, -0.5703, -0.5971, -0.0613, -0.5894,  0.5420,\n",
       "         -0.3725, -0.6369, -0.6338, -0.5033, -0.4872, -0.6208, -0.5951,    -inf,\n",
       "            -inf, -0.6143, -0.5980, -0.6238, -0.6118, -0.6045, -0.6209, -0.5754,\n",
       "         -0.5565, -0.5947,    -inf,    -inf, -0.4800, -0.4611, -0.6024, -0.6179,\n",
       "         -0.5408,    -inf, -0.5862, -0.3453,    -inf, -0.3828,    -inf, -0.4651,\n",
       "            -inf,    -inf, -0.3774, -0.5738, -0.6290,    -inf, -0.4551,    -inf,\n",
       "         -0.4164,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf], device='cuda:0', grad_fn=<SelectBackward>),\n",
       " torch.Size([1, 50, 99]),\n",
       " torch.return_types.topk(\n",
       " values=tensor([[[ 0.5549,  0.4496,  0.2574],\n",
       "          [ 0.5549,  0.4496,  0.2574],\n",
       "          [ 0.5549,  0.4496,  0.2574],\n",
       "          [ 0.5420,  0.2419, -0.0613],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320],\n",
       "          [-0.3320, -0.3320, -0.3320]]], device='cuda:0',\n",
       "        grad_fn=<TopkBackward>),\n",
       " indices=tensor([[[14,  0, 41],\n",
       "          [14,  0, 41],\n",
       "          [14,  0, 41],\n",
       "          [ 7,  0,  5],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0]]], device='cuda:0')))"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "trump = c[0,:,1:].gather(index=b[0].unsqueeze(-1).expand(50,5,384), dim=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "b[0,3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([44, 32, 21, 20, 43])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t.batch_decode(x1['his_encoded_index'][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "t(\"trump\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8398, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall = 0\n",
    "count = 0\n",
    "for x in tqdm(loaders[0], ncols=120, leave=True):\n",
    "    kid = model.encode_user(x)[1]\n",
    "    ps_terms = x['his_encoded_index'][:, :, 1:].to(manager.device).gather(index=kid, dim=-1).view(-1).tolist()\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = 32*50*2*100*768\n",
    "ext_encoding = 32*50*(2*3*384*100 + 4*100*384) + 32*8*2*384*384*50\n",
    "reduction = (2*100*768 + 100*math.log2(100))*32*50*2+32*50*3\n",
    "bert_embedding = 32*150*768*2\n",
    "bert_project = 32*150*768*2*3\n",
    "bert_attn = 32*150*12*64*64*2 + 32*150*12*12*64*2\n",
    "bert_intm = 32*150*768*768*2 + 32*150*768*2 + 32*150*768*3072*4\n",
    "bert_pool = 32*150*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total_isrec = embedding + ext_encoding + reduction + bert\n",
    "\n",
    "embedding = 32*50*2*100*768 + 32*50*100*768*2\n",
    "bert_project = 32*5000*768*2*3\n",
    "bert_attn = 32*5000*12*64*64*2 + 32*5000*12*12*64*2\n",
    "bert_intm = 32*5000*768*768*2 + 32*5000*768*2 + 32*5000*768*3072*4\n",
    "bert_pool = 32*5000*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total =  embedding + bert + 32*8*2*768*768*50\n",
    "\n",
    "total_isrec, total"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}