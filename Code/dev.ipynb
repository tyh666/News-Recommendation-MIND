{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare, convert_tokens_to_words\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention\n",
    "\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "m = AutoModel.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')\n",
    "# t = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "attn_mask = torch.tensor([[1.,1,1,1,0],[1,0,0,0,0]])\n",
    "hidden_states = torch.rand((2,3,4))\n",
    "extended_attn_mask = m.encoder.get_attention_mask(attn_mask)\n",
    "rel_pos = m.encoder.get_rel_pos(hidden_states)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "rel_pos, extended_attn_mask"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 0, -1, -2],\n",
       "          [ 1,  0, -1],\n",
       "          [ 2,  1,  0]]]),\n",
       " tensor([[[[1, 0, 1],\n",
       "           [0, 0, 0],\n",
       "           [1, 0, 1]]],\n",
       " \n",
       " \n",
       "         [[[1, 0, 0],\n",
       "           [0, 0, 0],\n",
       "           [0, 0, 0]]]], dtype=torch.uint8))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "\n",
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "x1 = list(loaders[0])[0]\n",
    "x2 = list(loaders[1])[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-09-06 14:01:59,640] INFO (utils.utils) Hyper Parameters are \n",
      "scale:demo\n",
      "batch_size:5\n",
      "k:5\n",
      "threshold:-inf\n",
      "signal_length:100\n",
      "his_size:50\n",
      "impr_size:10\n",
      "lr:0.0001\n",
      "bert_lr:3e-05\n",
      "hidden_dim:384\n",
      "world_size:0\n",
      "step:0\n",
      "ascend_history:False\n",
      "no_dedup:False\n",
      "diversify:False\n",
      "granularity:avg\n",
      "no_sep_his:False\n",
      "no_order_embed:False\n",
      "bert:bert-base-uncased\n",
      "[2021-09-06 14:01:59,641] INFO (utils.utils) preparing dataset...\n",
      "[2021-09-06 14:01:59,646] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-09-06 14:01:59,660] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-09-06 14:02:00,511] INFO (utils.utils) deduplicating...\n",
      "[2021-09-06 14:02:01,876] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-09-06 14:02:01,878] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_dev/news.pkl\n",
      "[2021-09-06 14:02:02,566] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "class TTMS(nn.Module):\n",
    "    def __init__(self, config, embedding, encoderN, encoderU, reducer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = config.scale\n",
    "        self.cdd_size = config.cdd_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.his_size = config.his_size\n",
    "        self.signal_length = config.signal_length\n",
    "        self.device = config.device\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderN = encoderN\n",
    "        self.encoderU = encoderU\n",
    "\n",
    "        self.reducer = reducer\n",
    "        self.bert = BERT_Encoder(config)\n",
    "\n",
    "        self.granularity = config.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, config.impr_size, config.signal_length * config.signal_length)), persistent=False)\n",
    "            if config.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (config.k + 1) * (config.k + 1))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, config.signal_length * config.signal_length)), persistent=False)\n",
    "\n",
    "        self.userProject = nn.Sequential(\n",
    "            nn.Linear(self.bert.hidden_dim, self.bert.hidden_dim),\n",
    "            nn.LayerNorm(self.bert.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.register_buffer('extra_cls_mask', torch.ones(1,1), persistent=False)\n",
    "\n",
    "        self.name = '__'.join(['ttms', self.encoderN.name, self.encoderU.name, config.reducer, self.granularity])\n",
    "        config.name = self.name\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        # print(user_repr.mean(), cdd_news_repr.mean(), user_repr.max(), cdd_news_repr.max(), user_repr.sum(), cdd_news_repr.sum())\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)\n",
    "        return score\n",
    "\n",
    "    def _forward(self,x):\n",
    "        t1 = time.time()\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                if batch_size != self.batch_size:\n",
    "                    cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                    his_dest = self.his_dest[:batch_size]\n",
    "                else:\n",
    "                    cdd_dest = self.cdd_dest[:, :cdd_size]\n",
    "                    his_dest = self.his_dest\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1))\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].long().to(self.device)\n",
    "        cdd_news_embedding = self.embedding(cdd_news, cdd_subword_prefix)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].long().to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "\n",
    "        t6 = time.time()\n",
    "        _, cdd_news_repr = self.bert(\n",
    "            cdd_news_embedding, cdd_attn_mask\n",
    "        )\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding\n",
    "        )\n",
    "\n",
    "        t7 = time.time()\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr = self.encoderU(his_news_repr)\n",
    "        else:\n",
    "            user_repr = None\n",
    "        \n",
    "        t3 = time.time()\n",
    "\n",
    "        ps_terms, ps_term_mask, kid = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        # append CLS to the entire browsing history, directly deriving user repr\n",
    "        batch_size = ps_terms.size(0)\n",
    "        ps_terms = torch.cat([his_news_embedding[:, 0, 0].unsqueeze(1), ps_terms], dim=-2)\n",
    "        ps_term_mask = torch.cat([self.extra_cls_mask.expand(batch_size, 1), ps_term_mask], dim=-1)\n",
    "\n",
    "        t4 = time.time()\n",
    "\n",
    "        _, user_cls = self.bert(ps_terms.unsqueeze(1), ps_term_mask.unsqueeze(1))\n",
    "        user_repr = self.userProject(user_cls)\n",
    "\n",
    "        t5 = time.time()\n",
    "\n",
    "        print(\"preparing time:{}, embedding time:{}, encoding time:{}, course profiling time:{}, selecting time:{}, profiling time:{}\".format(t2-t1, t6-t2, t7-t6, t3-t7, t4-t3, t5-t4))\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), kid, (user_repr, cdd_news_repr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, kid, extra = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, kid, extra, score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "# encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "a,b,c,d = model(x1)\n",
    "d.retain_grad()\n",
    "a,d"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preparing time:0.013997554779052734, embedding time:0.05136728286743164, encoding time:1.095184564590454, course profiling time:0.0003993511199951172, selecting time:0.055777549743652344, profiling time:2.38578462600708\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-2.3739e+00, -3.9156e-01, -2.5645e+00, -2.5645e+00, -2.5645e+00],\n",
       "         [-6.2833e+00, -3.4192e+00, -7.4988e+00, -2.2180e-01, -1.8093e+00],\n",
       "         [-6.7023e+00, -1.2734e+01, -1.7231e-03, -8.7279e+00, -8.0205e+00],\n",
       "         [-9.3084e+00, -7.9760e-01, -1.8439e+00, -9.6730e-01, -4.4922e+00],\n",
       "         [-2.5075e+00, -3.0121e+00, -6.7501e+00, -6.1541e-01, -1.1155e+00]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-3.3020, -1.3197, -3.4926, -3.4926, -3.4926],\n",
       "         [-1.9839,  0.8802, -3.1994,  4.0776,  2.4901],\n",
       "         [ 6.3503,  0.3183, 13.0509,  4.3247,  5.0321],\n",
       "         [-7.8380,  0.6728, -0.3735,  0.5031, -3.0218],\n",
       "         [-1.2666, -1.7712, -5.5092,  0.6255,  0.1254]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "ls = loss(a,target=x1['label'])\n",
    "ls.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}