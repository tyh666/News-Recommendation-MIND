{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention\n",
    "\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "# m = AutoModel.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')\n",
    "t = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# config.reducer = 'entity'\n",
    "config.embedding = 'deberta'\n",
    "config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "\n",
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "x1 = list(loaders[0])[0]\n",
    "x2 = list(loaders[1])[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "manager.hidden_dim = 768"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "manager.load(model,3534)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Matching_Reducer(nn.Module):\n",
    "    \"\"\"\n",
    "    basic document reducer: topk of each historical article\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = \"matching\"\n",
    "\n",
    "        self.k = config.k\n",
    "        self.diversify = config.diversify\n",
    "        self.his_size = config.his_size\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "\n",
    "        config.term_num = config.k * config.his_size\n",
    "\n",
    "        keep_k_modifier = torch.zeros(1, config.signal_length)\n",
    "        keep_k_modifier[:, :self.k+1] = 1\n",
    "        self.register_buffer('keep_k_modifier', keep_k_modifier, persistent=False)\n",
    "\n",
    "        if self.diversify:\n",
    "            self.newsUserAlign = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "            nn.init.xavier_normal_(self.newsUserAlign.weight)\n",
    "\n",
    "        if config.threshold != -float('inf'):\n",
    "            threshold = torch.tensor([config.threshold])\n",
    "            self.register_buffer('threshold', threshold)\n",
    "\n",
    "        if config.sep_his:\n",
    "            config.term_num += (self.his_size - 1)\n",
    "            self.sep_embedding = nn.Parameter(torch.randn(1, 1, 1, config.embedding_dim))\n",
    "            self.register_buffer('extra_sep_mask', torch.ones(1, 1, 1), persistent=False)\n",
    "            nn.init.xavier_normal_(self.sep_embedding)\n",
    "\n",
    "        if not config.no_order_embed:\n",
    "            self.order_embedding = nn.Parameter(torch.randn(config.his_size, 1, config.embedding_dim))\n",
    "            nn.init.xavier_normal_(self.order_embedding)\n",
    "\n",
    "\n",
    "    def forward(self, news_selection_embedding, news_embedding, user_repr, news_repr, his_attn_mask, his_refined_mask):\n",
    "        \"\"\"\n",
    "        Extract words from news text according to the overall user interest\n",
    "\n",
    "        Args:\n",
    "            news_selection_embedding: encoded word-level embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_embedding: word-level news embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_repr: news-level representation, [batch_size, his_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            ps_terms: weighted embedding for personalized terms, [batch_size, term_num, embedding_dim]\n",
    "            ps_term_mask: attention mask of output terms, [batch_size, term_num]\n",
    "            kid: the index of personalized terms\n",
    "        \"\"\"\n",
    "        batch_size = news_embedding.size(0)\n",
    "\n",
    "        # strip off [CLS]\n",
    "        news_selection_embedding = news_selection_embedding[:, :, 1:]\n",
    "        news_embedding = news_embedding[:, :, 1:]\n",
    "        if self.diversify:\n",
    "            news_user_repr = torch.cat([user_repr.expand(news_repr.size()), news_repr], dim=-1)\n",
    "            selection_query = self.newsUserAlign(news_user_repr).unsqueeze(-1)\n",
    "        else:\n",
    "            selection_query = user_repr.unsqueeze(-1)\n",
    "\n",
    "        # [bs, hs, sl - 1]\n",
    "        scores = F.normalize(news_selection_embedding, dim=-1).matmul(F.normalize(selection_query, dim=-2)).squeeze(-1)\n",
    "        # print(scores[0])\n",
    "        pad_pos = ~(((his_refined_mask + self.keep_k_modifier)[:, :, 1:]).bool())\n",
    "        # mask the padded term\n",
    "        scores = scores.masked_fill(pad_pos, -float('inf'))\n",
    "        print(scores[0,0])\n",
    "\n",
    "        score_k, score_kid = scores.topk(dim=-1, k=self.k)\n",
    "\n",
    "        ps_terms = news_embedding.gather(dim=-2,index=score_kid.unsqueeze(-1).expand(score_kid.size() + (news_embedding.size(-1),)))\n",
    "        # [bs, hs, k]\n",
    "        ps_term_mask = his_attn_mask[:, :, 1:].gather(dim=-1, index=score_kid)\n",
    "\n",
    "        if hasattr(self, 'threshold'):\n",
    "            mask_pos = score_k < self.threshold\n",
    "            # ps_terms = personalized_terms * (nn.functional.softmax(score_k.masked_fill(score_k < self.threshold, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_terms = ps_terms * (score_k.masked_fill(mask_pos, 0).unsqueeze(-1))\n",
    "            ps_term_mask = ps_term_mask * (~mask_pos)\n",
    "\n",
    "        else:\n",
    "            ps_terms = ps_terms * (F.softmax(score_k, dim=-1).unsqueeze(-1))\n",
    "            # ps_terms = ps_terms * (score_k.unsqueeze(-1))\n",
    "        if hasattr(self, 'order_embedding'):\n",
    "            ps_terms += self.order_embedding\n",
    "\n",
    "        if hasattr(self, 'sep_embedding'):\n",
    "            ps_terms = torch.cat([ps_terms, self.sep_embedding.expand(batch_size, self.his_size, 1, self.embedding_dim)], dim=-2).view(batch_size, -1, self.embedding_dim)[:, :-1]\n",
    "            ps_term_mask = torch.cat([ps_term_mask, self.extra_sep_mask.expand(batch_size, self.his_size, 1)], dim=-1).view(batch_size, -1)[:, :-1]\n",
    "        else:\n",
    "            ps_terms = ps_terms.view(batch_size, -1, self.embedding_dim)\n",
    "            ps_term_mask = ps_term_mask.view(batch_size, -1)\n",
    "\n",
    "        return ps_terms, ps_term_mask, score_kid\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "dest = torch.zeros((config.signal_length * config.signal_length))\n",
    "index = x2['his_subword_index'][0,0,:,0] * config.signal_length + x2['his_subword_index'][0,0,:,1]\n",
    "index"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([   0,  101,  202,  303,  404,  505,  606,  707,  808,  909, 1010, 1111,\n",
       "        1212, 1313, 1414, 1515, 1616, 1717, 1818, 1919, 2020, 2121, 2222, 2323,\n",
       "        2424, 2525, 2626, 2727, 2828, 2929, 3030, 3131, 3232, 3333, 3434, 3535,\n",
       "        3636, 3737, 3838, 3939, 4040, 4141, 4242, 4343, 4444, 4545, 4646, 4747,\n",
       "        4748,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "dest = F.normalize(dest.scatter(dim=-1,index=index,value=1).view(config.signal_length, config.signal_length),p=1,dim=-1)\n",
    "dest.matmul(x2['his_refined_mask'][0,0].float())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "t.convert_ids_to_tokens(x2['his_encoded_index'][0,0])[:49]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Donald',\n",
       " 'ĠTrump',\n",
       " 'ĠJr',\n",
       " '.',\n",
       " 'Ġreflects',\n",
       " 'Ġon',\n",
       " 'Ġexplosive',\n",
       " 'ĠView',\n",
       " 'Ġchat',\n",
       " ':',\n",
       " 'ĠI',\n",
       " 'Ġdont',\n",
       " 'Ġthink',\n",
       " 'Ġthey',\n",
       " 'Ġlike',\n",
       " 'Ġme',\n",
       " 'Ġmuch',\n",
       " 'Ġanymore',\n",
       " 'ĠAfter',\n",
       " 'Ġa',\n",
       " 'Ġheated',\n",
       " 'Ġappearance',\n",
       " 'Ġon',\n",
       " 'ĠThe',\n",
       " 'ĠView',\n",
       " 'ĠThursday',\n",
       " ',',\n",
       " 'ĠDonald',\n",
       " 'ĠTrump',\n",
       " 'ĠJr',\n",
       " '.',\n",
       " 'Ġtalked',\n",
       " 'Ġabout',\n",
       " 'Ġthe',\n",
       " 'Ġexperience',\n",
       " 'Ġlater',\n",
       " 'Ġthat',\n",
       " 'Ġday',\n",
       " 'Ġwith',\n",
       " 'ĠSean',\n",
       " 'ĠHannity',\n",
       " 'Ġon',\n",
       " 'ĠFox',\n",
       " 'ĠNews',\n",
       " '.',\n",
       " 'Ġtv',\n",
       " 'Ġtv',\n",
       " 'news']"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "model.eval()\n",
    "a,b = model(x2)\n",
    "a,b"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0150, grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0.5936, 0.5504, 0.5749, 0.5547, 0.5725, 0.5211, 0.5547, 0.5392, 0.5201,\n",
       "          0.5793]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[[39, 17, 46, 25, 27],\n",
       "          [ 8, 11, 43, 41, 16],\n",
       "          [ 5, 11, 46, 50, 12],\n",
       "          [14, 13, 37, 35, 31],\n",
       "          [30,  1, 19, 12, 10],\n",
       "          [ 4,  9,  3, 17, 14],\n",
       "          [16, 18,  3, 40,  9],\n",
       "          [ 6,  3, 28, 16, 10],\n",
       "          [ 1,  2, 23,  4,  6],\n",
       "          [22,  6, 20,  2,  9],\n",
       "          [ 8, 27, 19,  5,  9],\n",
       "          [16, 74, 57, 71, 31],\n",
       "          [ 6,  8, 25, 10, 17],\n",
       "          [ 9, 24, 11, 22, 14],\n",
       "          [48, 22, 20,  7, 34],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls = loss(a,target=x1['label'])\n",
    "ls.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}