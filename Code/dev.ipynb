{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.BaseModel import BaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "from models.UniLM.tokenization_tnlrv3 import TuringNLRv3Tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "config = TuringNLRv3Config.from_pretrained('/data/workspace/Peitian/Code/Document-Reduction/Code/data/bert_cache/unilm2-base-uncased-config.json')\n",
    "model = TuringNLRv3ForSequenceClassification.from_pretrained('/data/workspace/Peitian/Code/Document-Reduction/Code/data/bert_cache/unilm2-base-uncased.bin', config=config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at /data/workspace/Peitian/Code/Document-Reduction/Code/data/bert_cache/unilm2-base-uncased.bin were not used when initializing TuringNLRv3ForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TuringNLRv3ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TuringNLRv3ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TuringNLRv3ForSequenceClassification were not initialized from the model checkpoint at /data/workspace/Peitian/Code/Document-Reduction/Code/data/bert_cache/unilm2-base-uncased.bin and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "inputs = t(\"i love you\", return_tensors='pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "cls_embedding = model(**inputs)[1][0,0].clone()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "    \"\"\"\n",
    "    Adapted from Mesh Tensorflow:\n",
    "    https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "    \"\"\"\n",
    "    ret = 0\n",
    "    if bidirectional:\n",
    "        num_buckets //= 2\n",
    "        # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n",
    "        ret += (relative_position > 0).long() * num_buckets\n",
    "        n = torch.abs(relative_position)\n",
    "    else:\n",
    "        n = torch.max(-relative_position, torch.zeros_like(relative_position))\n",
    "    # now n is in the range [0, inf)\n",
    "\n",
    "    # half of the buckets are for exact increments in positions\n",
    "    max_exact = num_buckets // 2\n",
    "    is_small = n < max_exact\n",
    "\n",
    "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "    val_if_large = max_exact + (\n",
    "        torch.log(n.float() / max_exact) / math.log(max_distance /\n",
    "                                                    max_exact) * (num_buckets - max_exact)\n",
    "    ).to(torch.long)\n",
    "    val_if_large = torch.min(\n",
    "        val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "    ret += torch.where(is_small, n, val_if_large)\n",
    "    return ret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "position_ids = torch.arange(5, dtype=torch.long).unsqueeze(0).expand(1,5)\n",
    "\n",
    "rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "rel_pos = relative_position_bucket(rel_pos_mat, num_buckets=32, max_distance=128)\n",
    "rel_pos = F.one_hot(rel_pos, num_classes=32).float()\n",
    "rel_pos = model.bert.rel_pos_bias(rel_pos).permute(0, 3, 1, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "embeddings,position_ids = model.bert.embeddings(inputs.input_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "outputs = model.bert.encoder(embeddings, attention_mask=(1 - inputs.attention_mask[:, None, None, :]) * -10000., rel_pos=rel_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "outputs"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_171403/4102611780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "(outputs[0][0,0] == cls_embedding).all()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = AutoModel.from_pretrained(\"microsoft/unilm-base-cased\",cache_dir=config.path + 'bert_cache/')\n",
    "\n",
    "t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "config.seed = None\n",
    "manager = Manager(config)\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "x2 = X2[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TTMS(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, embedding, encoderN, encoderU, reducer):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderN = encoderN\n",
    "        self.encoderU = encoderU\n",
    "\n",
    "        self.reducer = reducer\n",
    "        self.bert = BERT_Encoder(manager)\n",
    "\n",
    "        self.newsUserProject = nn.Sequential(\n",
    "            nn.Linear(self.bert.hidden_dim, self.bert.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,self.bert.hidden_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = self.bert.hidden_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, manager.impr_size, manager.signal_length * manager.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 1) * (manager.k + 1))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, manager.signal_length * manager.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        manager.name = '__'.join(['ttms', manager.embedding, manager.encoderN, manager.encoderU, manager.reducer, manager.granularity])\n",
    "        self.name = manager.name\n",
    "\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(self.embedding.embedding_dim)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _forward(self,x):\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                # * cdd_mask to filter out padded cdd news\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            # FIXME historical news not need this\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        _, cdd_news_repr = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "        cdd_news_repr = self.newsUserProject(cdd_news_repr)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding, his_attn_mask\n",
    "        )\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr = self.encoderU(his_news_repr, his_mask=x['his_mask'].to(self.device), user_index=x[\"user_id\"].to(self.device))\n",
    "        else:\n",
    "            user_repr = None\n",
    "\n",
    "        ps_terms, ps_term_mask, kid = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        _, user_cls = self.bert(ps_terms, ps_term_mask)\n",
    "        user_repr = self.newsUserProject(user_cls)\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), kid, (cdd_news_repr, user_repr)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, kid,c = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, kid, c\n",
    "\n",
    "\n",
    "    def encode_news(self, x):\n",
    "        \"\"\"\n",
    "        encode news of loader_news\n",
    "        \"\"\"\n",
    "        if not self.ready_encode:\n",
    "            self._init_encoding()\n",
    "\n",
    "        # encode news with MIND_news\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_dest = self.cdd_dest[:batch_size]\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, 0] * self.signal_length + cdd_subword_index[:, :, 1]\n",
    "            cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        _, cdd_news_repr = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "        cdd_news_repr = self.newsUserProject(cdd_news_repr.squeeze(1))\n",
    "\n",
    "        return cdd_news_repr\n",
    "\n",
    "\n",
    "    def encode_user(self, x):\n",
    "        \"\"\"\n",
    "        encode user of loader_history\n",
    "        \"\"\"\n",
    "        if not self.ready_encode:\n",
    "            self._init_encoding()\n",
    "\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['his_encoded_index'].size(0)\n",
    "            his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            his_subword_prefix = None\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding, his_attn_mask\n",
    "        )\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr = self.encoderU(his_news_repr, his_mask=x['his_mask'].to(self.device), user_index=x['user_id'].to(self.device))\n",
    "        else:\n",
    "            user_repr = None\n",
    "\n",
    "        ps_terms, ps_term_mask, _ = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        _, user_cls = self.bert(ps_terms, ps_term_mask)\n",
    "        user_repr = self.newsUserProject(user_cls.squeeze(1))\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "        return user_repr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "\n",
    "manager.load(model, 589, strict=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "a,b,c = model(x2)\n",
    "d = model.encode_news(xn)\n",
    "e = model.encode_user(xu)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.sigmoid(torch.dot(e[0],d[28])/math.sqrt(768))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "manager.fast = True\n",
    "manager.mode = \"dev\"\n",
    "\n",
    "loaders = manager.prepare()\n",
    "xn = list(loaders[1])[413]\n",
    "xu = list(loaders[2])[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xn['cdd_id'][28]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3811jvsc74a57bd0decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b",
   "display_name": "Python 3.8.11 64-bit (conda)",
   "language": "python"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}