{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "# from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.BaseModel import BaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "\n",
    "t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "# config.bert = 'longformer'\n",
    "# config.seed = None\n",
    "config.mode = \"inspect\"\n",
    "config.recall_type = \"s\"\n",
    "config.scale = \"large\"\n",
    "config.case = True\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "# X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-11-08 08:05:30,871] INFO (utils.Manager) Hyper Parameters are \n",
      "{\n",
      "    \"scale\": \"large\",\n",
      "    \"mode\": \"inspect\",\n",
      "    \"batch_size\": 5,\n",
      "    \"batch_size_news\": 100,\n",
      "    \"batch_size_history\": 100,\n",
      "    \"k\": 3,\n",
      "    \"threshold\": -Infinity,\n",
      "    \"abs_length\": 40,\n",
      "    \"signal_length\": 100,\n",
      "    \"news\": null,\n",
      "    \"his_size\": 50,\n",
      "    \"cdd_size\": 5,\n",
      "    \"impr_size\": 10,\n",
      "    \"dropout_p\": 0.2,\n",
      "    \"lr\": 0.0001,\n",
      "    \"bert_lr\": 3e-05,\n",
      "    \"embedding\": \"bert\",\n",
      "    \"encoderN\": \"cnn\",\n",
      "    \"encoderU\": \"lstm\",\n",
      "    \"selector\": \"sfi\",\n",
      "    \"reducer\": \"matching\",\n",
      "    \"ranker\": \"onepass\",\n",
      "    \"pooler\": \"attn\",\n",
      "    \"bert_dim\": 768,\n",
      "    \"embedding_dim\": 768,\n",
      "    \"hidden_dim\": 384,\n",
      "    \"base_rank\": 0,\n",
      "    \"world_size\": 0,\n",
      "    \"seed\": 42,\n",
      "    \"granularity\": \"token\",\n",
      "    \"debias\": false,\n",
      "    \"full_attn\": true,\n",
      "    \"descend_history\": false,\n",
      "    \"shuffle_pos\": false,\n",
      "    \"save_pos\": false,\n",
      "    \"sep_his\": false,\n",
      "    \"diversify\": false,\n",
      "    \"no_dedup\": false,\n",
      "    \"segment_embed\": false,\n",
      "    \"no_rm_punc\": false,\n",
      "    \"fast\": false,\n",
      "    \"scheduler\": \"linear\",\n",
      "    \"warmup\": 100,\n",
      "    \"shuffle\": false,\n",
      "    \"bert\": \"bert\",\n",
      "    \"tb\": false,\n",
      "    \"recall_type\": \"s\",\n",
      "    \"case\": true\n",
      "}\n",
      "[2021-11-08 08:05:30,872] INFO (utils.Manager) preparing dataset...\n",
      "[2021-11-08 08:05:30,872] INFO (utils.MIND) encoding user behaviors of data/case/behaviors.tsv...\n",
      "2it [00:00, 10894.30it/s]\n",
      "[2021-11-08 08:05:31,380] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDlarge_dev/news.pkl\n",
      "[2021-11-08 08:05:31,972] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Matching_Reducer(nn.Module):\n",
    "    \"\"\"\n",
    "    select top k terms from each historical news with max cosine similarity\n",
    "\n",
    "    1. keep the first K terms unmasked\n",
    "    2. add order embedding to terms from different historical news\n",
    "    3. insert [SEP] token to separate terms from different news if called\n",
    "    \"\"\"\n",
    "    def __init__(self, manager):\n",
    "        super().__init__()\n",
    "        self.name = \"matching\"\n",
    "        self.k = manager.k\n",
    "        self.his_size = manager.his_size\n",
    "        self.embedding_dim = manager.embedding_dim\n",
    "\n",
    "        self.diversify = manager.diversify\n",
    "        self.sep_his = manager.sep_his\n",
    "        # if aggregator is enabled, do not flatten the personalized terms\n",
    "        self.flatten = (manager.aggregator is None)\n",
    "\n",
    "        manager.term_num = manager.k * manager.his_size\n",
    "\n",
    "        # strip [CLS]\n",
    "        keep_k_modifier = torch.zeros(1, manager.signal_length - 1)\n",
    "        keep_k_modifier[:, :self.k] = 1\n",
    "        self.register_buffer('keep_k_modifier', keep_k_modifier, persistent=False)\n",
    "\n",
    "        if self.diversify:\n",
    "            self.newsUserAlign = nn.Linear(manager.hidden_dim * 2, manager.hidden_dim)\n",
    "            nn.init.xavier_normal_(self.newsUserAlign.weight)\n",
    "\n",
    "        if manager.threshold != -float('inf'):\n",
    "            threshold = torch.tensor([manager.threshold])\n",
    "            self.register_buffer('threshold', threshold)\n",
    "\n",
    "        if self.sep_his:\n",
    "            manager.term_num += (self.his_size - 1)\n",
    "            self.sep_embedding = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
    "            self.register_buffer('extra_sep_mask', torch.ones(1, 1, 1), persistent=False)\n",
    "\n",
    "        if manager.segment_embed:\n",
    "            self.segment_embedding = nn.Parameter(torch.randn(manager.his_size, 1, manager.embedding_dim))\n",
    "            nn.init.xavier_normal_(self.segment_embedding)\n",
    "\n",
    "\n",
    "    def forward(self, news_selection_embedding, news_embedding, user_repr, news_repr, his_attn_mask, his_refined_mask):\n",
    "        \"\"\"\n",
    "        Extract words from news text according to the overall user interest\n",
    "\n",
    "        Args:\n",
    "            news_selection_embedding: encoded word-level embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_embedding: word-level news embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_repr: news-level representation, [batch_size, his_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "            his_refined_mask: dedupicated attention mask, [batch_size, his_size, signal_length]\n",
    "        Returns:\n",
    "            ps_terms: weighted embedding for personalized terms, [batch_size, term_num, embedding_dim]\n",
    "            ps_term_mask: attention mask of output terms, [batch_size, term_num]\n",
    "            kid: the index of personalized terms\n",
    "        \"\"\"\n",
    "        batch_size = news_embedding.size(0)\n",
    "\n",
    "        if self.diversify:\n",
    "            news_user_repr = torch.cat([user_repr.expand(news_repr.size()), news_repr], dim=-1)\n",
    "            selection_query = self.newsUserAlign(news_user_repr).unsqueeze(-1)\n",
    "        else:\n",
    "            selection_query = user_repr.unsqueeze(-1)\n",
    "\n",
    "        news_selection_embedding = news_selection_embedding[:, :, 1:]\n",
    "\n",
    "        news_embedding_text = news_embedding[:, :, 1:]\n",
    "        his_attn_mask = his_attn_mask[:, :, 1:]\n",
    "\n",
    "        # [bs, hs, sl - 1]\n",
    "        scores = F.normalize(news_selection_embedding, dim=-1).matmul(F.normalize(selection_query, dim=-2)).squeeze(-1)\n",
    "        # scores = news_selection_embedding.matmul(selection_query).squeeze(-1)/math.sqrt(selection_query.size(-1))\n",
    "        pad_pos = ~((his_refined_mask[:, :, 1:] + self.keep_k_modifier).bool())\n",
    "\n",
    "        # mask the padded term\n",
    "        scores = scores.masked_fill(pad_pos, -float('inf'))\n",
    "\n",
    "        score_k, score_kid = scores.topk(dim=-1, k=self.k)\n",
    "\n",
    "        ps_terms = news_embedding_text.gather(dim=-2,index=score_kid.unsqueeze(-1).expand(*score_kid.size(), news_embedding_text.size(-1)))\n",
    "        # [bs, hs, k]\n",
    "        ps_term_mask = his_attn_mask.gather(dim=-1, index=score_kid)\n",
    "\n",
    "        if hasattr(self, 'threshold'):\n",
    "            mask_pos = score_k < self.threshold\n",
    "            # ps_terms = personalized_terms * (nn.functional.softmax(score_k.masked_fill(score_k < self.threshold, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_terms = ps_terms * (F.softmax(score_k.masked_fill(mask_pos, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_term_mask = ps_term_mask * (~mask_pos)\n",
    "        else:\n",
    "            ps_terms = ps_terms * (F.softmax(score_k, dim=-1).unsqueeze(-1))\n",
    "\n",
    "        if hasattr(self, 'segment_embedding'):\n",
    "            ps_terms += self.segment_embedding\n",
    "\n",
    "        # flatten the selected terms into one dimension\n",
    "        if self.flatten:\n",
    "            # separate historical news only practical when squeeze=True\n",
    "            if self.sep_his:\n",
    "                # [bs, hs, ed]\n",
    "                sep_embedding = self.sep_embedding.expand(batch_size, self.his_size, 1, self.embedding_dim)\n",
    "                # add extra [SEP] token to separate terms from different history news, slice to -1 to strip off the last [SEP]\n",
    "                ps_terms = torch.cat([ps_terms, sep_embedding], dim=-2).view(batch_size, -1, self.embedding_dim)[:, :-1]\n",
    "                ps_term_mask = torch.cat([ps_term_mask, self.extra_sep_mask.expand(batch_size, self.his_size, 1)], dim=-1).view(batch_size, -1)[:, :-1]\n",
    "\n",
    "            else:\n",
    "                # [bs, 1, ed]\n",
    "                ps_terms = ps_terms.reshape(batch_size, -1, self.embedding_dim)\n",
    "                ps_term_mask = ps_term_mask.reshape(batch_size, -1)\n",
    "\n",
    "        return ps_terms, ps_term_mask, score_kid, scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Two tower baseline\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TESRec(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, embedding, encoderN, encoderU, reducer, aggregator=None):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        # only these reducers need selection encoding\n",
    "        if manager.reducer in manager.get_need_encode_reducers():\n",
    "            self.encoderN = encoderN\n",
    "            self.encoderU = encoderU\n",
    "        self.reducer = reducer\n",
    "        self.aggregator = aggregator\n",
    "        self.bert = BERT_Encoder(manager)\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,self.bert.hidden_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        if aggregator is not None:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.aggregator, manager.granularity, str(manager.k)])\n",
    "        else:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.granularity, str(manager.k)])\n",
    "\n",
    "        self.name = manager.name\n",
    "\n",
    "\n",
    "    def encode_news(self, x):\n",
    "        \"\"\"\n",
    "        encode candidate news\n",
    "        \"\"\"\n",
    "        # encode news with MIND_news\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_dest = self.cdd_dest[:batch_size]\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, 0] * self.signal_length + cdd_subword_index[:, :, 1]\n",
    "\n",
    "            cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        _, cdd_news_repr, _ = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "\n",
    "        return cdd_news_repr\n",
    "\n",
    "\n",
    "    def encode_user(self, x):\n",
    "        \"\"\"\n",
    "        encoder user\n",
    "        \"\"\"\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['his_encoded_index'].size(0)\n",
    "            his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            his_subword_prefix = None\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        if hasattr(self, 'encoderN'):\n",
    "            his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "                his_news_embedding, his_attn_mask\n",
    "            )\n",
    "        else:\n",
    "            his_news_encoded_embedding = None\n",
    "            his_news_repr = None\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr_ext = self.encoderU(his_news_repr, his_mask=x['his_mask'].to(self.device), user_index=x['user_id'].to(self.device))\n",
    "        else:\n",
    "            user_repr_ext = None\n",
    "\n",
    "        ps_terms, ps_term_mask, kid, scores = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr_ext, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        _, user_repr, _ = self.bert(ps_terms, ps_term_mask, ps_term_input=True)\n",
    "\n",
    "        if self.aggregator is not None:\n",
    "            user_repr = self.aggregator(user_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return user_repr, kid, his_news_encoded_embedding, user_repr_ext, scores\n",
    "\n",
    "\n",
    "    def compute_score(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        cdd_repr = self.encode_news(x)\n",
    "        user_repr, kid = self.encode_user(x)\n",
    "\n",
    "        score = self.compute_score(cdd_repr, user_repr)\n",
    "\n",
    "        if self.training:\n",
    "            logits = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            logits = torch.sigmoid(score)\n",
    "\n",
    "        return logits, kid\n",
    "\n",
    "\n",
    "    def predict_fast(self, x):\n",
    "        # [bs, cs, hd]\n",
    "        cdd_repr = self.news_reprs(x['cdd_id'].to(self.device))\n",
    "        user_repr, _ = self.encode_user(x)\n",
    "        scores = self.compute_score(cdd_repr, user_repr)\n",
    "        logits = torch.sigmoid(scores)\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "# model = XFormer(manager).to(manager.device)\n",
    "# manager.load(model, 589, strict=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "a,b,c,d,e = model.encode_user(x1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "np.asarray(t.convert_ids_to_tokens(x1['his_encoded_index'][0,0])[1:])[[86,2,93]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['##gb', 'fix', 'he'], dtype='<U11')"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "e[0,3], e.shape, e.topk(dim=-1, k=3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([-0.0250, -0.0369, -0.0322, -0.0319, -0.0002, -0.0238, -0.0041,  0.0148,\n",
       "          0.0490, -0.0601,  0.0483, -0.0070,  0.0105,  0.0228,  0.0615,    -inf,\n",
       "            -inf,  0.0478,  0.0520, -0.0799, -0.0197, -0.0340, -0.0406,  0.0227,\n",
       "          0.0712, -0.0329,    -inf,    -inf,  0.0028,  0.0594,  0.0367, -0.0295,\n",
       "          0.0426,    -inf,  0.0217, -0.0294,    -inf,  0.0229,    -inf,  0.0313,\n",
       "            -inf,    -inf,  0.0719, -0.0273, -0.0248,    -inf, -0.0080,    -inf,\n",
       "          0.0070,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "            -inf,    -inf,    -inf], device='cuda:0', grad_fn=<SelectBackward>),\n",
       " torch.Size([1, 50, 99]),\n",
       " torch.return_types.topk(\n",
       " values=tensor([[[ 0.0968,  0.0846,  0.0556],\n",
       "          [ 0.0968,  0.0846,  0.0556],\n",
       "          [ 0.0968,  0.0846,  0.0556],\n",
       "          [ 0.0719,  0.0712,  0.0615],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090],\n",
       "          [-0.0090, -0.0090, -0.0090]]], device='cuda:0',\n",
       "        grad_fn=<TopkBackward>),\n",
       " indices=tensor([[[86,  2, 93],\n",
       "          [86,  2, 93],\n",
       "          [86,  2, 93],\n",
       "          [42, 24, 14],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0],\n",
       "          [ 2,  1,  0]]], device='cuda:0')))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "trump = c[0,:,1:].gather(index=b[0].unsqueeze(-1).expand(50,5,384), dim=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "b[0,3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([44, 32, 21, 20, 43])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t.batch_decode(x1['his_encoded_index'][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "t(\"trump\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8398, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall = 0\n",
    "count = 0\n",
    "for x in tqdm(loaders[0], ncols=120, leave=True):\n",
    "    kid = model.encode_user(x)[1]\n",
    "    ps_terms = x['his_encoded_index'][:, :, 1:].to(manager.device).gather(index=kid, dim=-1).view(-1).tolist()\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = 32*50*2*100*768\n",
    "ext_encoding = 32*50*(2*3*384*100 + 4*100*384) + 32*8*2*384*384*50\n",
    "reduction = (2*100*768 + 100*math.log2(100))*32*50*2+32*50*3\n",
    "bert_embedding = 32*150*768*2\n",
    "bert_project = 32*150*768*2*3\n",
    "bert_attn = 32*150*12*64*64*2 + 32*150*12*12*64*2\n",
    "bert_intm = 32*150*768*768*2 + 32*150*768*2 + 32*150*768*3072*4\n",
    "bert_pool = 32*150*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total_isrec = embedding + ext_encoding + reduction + bert\n",
    "\n",
    "embedding = 32*50*2*100*768 + 32*50*100*768*2\n",
    "bert_project = 32*5000*768*2*3\n",
    "bert_attn = 32*5000*12*64*64*2 + 32*5000*12*12*64*2\n",
    "bert_intm = 32*5000*768*768*2 + 32*5000*768*2 + 32*5000*768*3072*4\n",
    "bert_pool = 32*5000*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total =  embedding + bert + 32*8*2*768*768*50\n",
    "\n",
    "total_isrec, total"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}