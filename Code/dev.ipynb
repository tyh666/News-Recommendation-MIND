{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "# from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.BaseModel import BaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class A:\n",
    "    a = np.array([[1,0],[2,1],[3,0]])\n",
    "A.a[:,-1] = 102*(A.a[:, -1] != 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "A.a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  1,   0],\n",
       "       [  2, 102],\n",
       "       [  3,   0]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "\n",
    "t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "# config.bert = 'longformer'\n",
    "# config.seed = None\n",
    "config.mode = \"inspect\"\n",
    "config.recall_type = \"s\"\n",
    "config.scale = \"large\"\n",
    "config.case = True\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "# X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Matching_Reducer(nn.Module):\n",
    "    \"\"\"\n",
    "    select top k terms from each historical news with max cosine similarity\n",
    "\n",
    "    1. keep the first K terms unmasked\n",
    "    2. add order embedding to terms from different historical news\n",
    "    3. insert [SEP] token to separate terms from different news if called\n",
    "    \"\"\"\n",
    "    def __init__(self, manager):\n",
    "        super().__init__()\n",
    "        self.name = \"matching\"\n",
    "        self.k = manager.k\n",
    "        self.his_size = manager.his_size\n",
    "        self.embedding_dim = manager.embedding_dim\n",
    "\n",
    "        self.diversify = manager.diversify\n",
    "        self.sep_his = manager.sep_his\n",
    "        # if aggregator is enabled, do not flatten the personalized terms\n",
    "        self.flatten = (manager.aggregator is None)\n",
    "\n",
    "        manager.term_num = manager.k * manager.his_size\n",
    "\n",
    "        # strip [CLS]\n",
    "        keep_k_modifier = torch.zeros(1, manager.signal_length - 1)\n",
    "        keep_k_modifier[:, :self.k] = 1\n",
    "        self.register_buffer('keep_k_modifier', keep_k_modifier, persistent=False)\n",
    "\n",
    "        if self.diversify:\n",
    "            self.newsUserAlign = nn.Linear(manager.hidden_dim * 2, manager.hidden_dim)\n",
    "            nn.init.xavier_normal_(self.newsUserAlign.weight)\n",
    "\n",
    "        if manager.threshold != -float('inf'):\n",
    "            threshold = torch.tensor([manager.threshold])\n",
    "            self.register_buffer('threshold', threshold)\n",
    "\n",
    "        if self.sep_his:\n",
    "            manager.term_num += (self.his_size - 1)\n",
    "            self.sep_embedding = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
    "            self.register_buffer('extra_sep_mask', torch.ones(1, 1, 1), persistent=False)\n",
    "\n",
    "        if manager.segment_embed:\n",
    "            self.segment_embedding = nn.Parameter(torch.randn(manager.his_size, 1, manager.embedding_dim))\n",
    "            nn.init.xavier_normal_(self.segment_embedding)\n",
    "\n",
    "\n",
    "    def forward(self, news_selection_embedding, news_embedding, user_repr, news_repr, his_attn_mask, his_refined_mask):\n",
    "        \"\"\"\n",
    "        Extract words from news text according to the overall user interest\n",
    "\n",
    "        Args:\n",
    "            news_selection_embedding: encoded word-level embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_embedding: word-level news embedding, [batch_size, his_size, signal_length, hidden_dim]\n",
    "            news_repr: news-level representation, [batch_size, his_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "            his_refined_mask: dedupicated attention mask, [batch_size, his_size, signal_length]\n",
    "        Returns:\n",
    "            ps_terms: weighted embedding for personalized terms, [batch_size, term_num, embedding_dim]\n",
    "            ps_term_mask: attention mask of output terms, [batch_size, term_num]\n",
    "            kid: the index of personalized terms\n",
    "        \"\"\"\n",
    "        batch_size = news_embedding.size(0)\n",
    "\n",
    "        if self.diversify:\n",
    "            news_user_repr = torch.cat([user_repr.expand(news_repr.size()), news_repr], dim=-1)\n",
    "            selection_query = self.newsUserAlign(news_user_repr).unsqueeze(-1)\n",
    "        else:\n",
    "            selection_query = user_repr.unsqueeze(-1)\n",
    "\n",
    "        news_selection_embedding = news_selection_embedding[:, :, 1:]\n",
    "\n",
    "        news_embedding_text = news_embedding[:, :, 1:]\n",
    "        his_attn_mask = his_attn_mask[:, :, 1:]\n",
    "\n",
    "        # [bs, hs, sl - 1]\n",
    "        scores = F.normalize(news_selection_embedding, dim=-1).matmul(F.normalize(selection_query, dim=-2)).squeeze(-1)\n",
    "        # scores = news_selection_embedding.matmul(selection_query).squeeze(-1)/math.sqrt(selection_query.size(-1))\n",
    "        pad_pos = ~((his_refined_mask[:, :, 1:] + self.keep_k_modifier).bool())\n",
    "\n",
    "        # mask the padded term\n",
    "        scores = scores.masked_fill(pad_pos, -float('inf'))\n",
    "\n",
    "        score_k, score_kid = scores.topk(dim=-1, k=self.k)\n",
    "\n",
    "        ps_terms = news_embedding_text.gather(dim=-2,index=score_kid.unsqueeze(-1).expand(*score_kid.size(), news_embedding_text.size(-1)))\n",
    "        # [bs, hs, k]\n",
    "        ps_term_mask = his_attn_mask.gather(dim=-1, index=score_kid)\n",
    "\n",
    "        if hasattr(self, 'threshold'):\n",
    "            mask_pos = score_k < self.threshold\n",
    "            # ps_terms = personalized_terms * (nn.functional.softmax(score_k.masked_fill(score_k < self.threshold, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_terms = ps_terms * (F.softmax(score_k.masked_fill(mask_pos, 0), dim=-1).unsqueeze(-1))\n",
    "            ps_term_mask = ps_term_mask * (~mask_pos)\n",
    "        else:\n",
    "            ps_terms = ps_terms * (F.softmax(score_k, dim=-1).unsqueeze(-1))\n",
    "\n",
    "        if hasattr(self, 'segment_embedding'):\n",
    "            ps_terms += self.segment_embedding\n",
    "\n",
    "        # flatten the selected terms into one dimension\n",
    "        if self.flatten:\n",
    "            # separate historical news only practical when squeeze=True\n",
    "            if self.sep_his:\n",
    "                # [bs, hs, ed]\n",
    "                sep_embedding = self.sep_embedding.expand(batch_size, self.his_size, 1, self.embedding_dim)\n",
    "                # add extra [SEP] token to separate terms from different history news, slice to -1 to strip off the last [SEP]\n",
    "                ps_terms = torch.cat([ps_terms, sep_embedding], dim=-2).view(batch_size, -1, self.embedding_dim)[:, :-1]\n",
    "                ps_term_mask = torch.cat([ps_term_mask, self.extra_sep_mask.expand(batch_size, self.his_size, 1)], dim=-1).view(batch_size, -1)[:, :-1]\n",
    "\n",
    "            else:\n",
    "                # [bs, 1, ed]\n",
    "                ps_terms = ps_terms.reshape(batch_size, -1, self.embedding_dim)\n",
    "                ps_term_mask = ps_term_mask.reshape(batch_size, -1)\n",
    "\n",
    "        return ps_terms, ps_term_mask, score_kid, scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Two tower baseline\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TESRec(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, embedding, encoderN, encoderU, reducer, aggregator=None):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        # only these reducers need selection encoding\n",
    "        if manager.reducer in manager.get_need_encode_reducers():\n",
    "            self.encoderN = encoderN\n",
    "            self.encoderU = encoderU\n",
    "        self.reducer = reducer\n",
    "        self.aggregator = aggregator\n",
    "        self.bert = BERT_Encoder(manager)\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,self.bert.hidden_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        if aggregator is not None:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.aggregator, manager.granularity, str(manager.k)])\n",
    "        else:\n",
    "            manager.name = '__'.join(['tesrec', manager.bert, manager.encoderN, manager.encoderU, manager.reducer, manager.granularity, str(manager.k)])\n",
    "\n",
    "        self.name = manager.name\n",
    "\n",
    "\n",
    "    def encode_news(self, x):\n",
    "        \"\"\"\n",
    "        encode candidate news\n",
    "        \"\"\"\n",
    "        # encode news with MIND_news\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_dest = self.cdd_dest[:batch_size]\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, 0] * self.signal_length + cdd_subword_index[:, :, 1]\n",
    "\n",
    "            cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        _, cdd_news_repr, _ = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "\n",
    "        return cdd_news_repr\n",
    "\n",
    "\n",
    "    def encode_user(self, x):\n",
    "        \"\"\"\n",
    "        encoder user\n",
    "        \"\"\"\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['his_encoded_index'].size(0)\n",
    "            his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            his_subword_prefix = None\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        if hasattr(self, 'encoderN'):\n",
    "            his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "                his_news_embedding, his_attn_mask\n",
    "            )\n",
    "        else:\n",
    "            his_news_encoded_embedding = None\n",
    "            his_news_repr = None\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr_ext = self.encoderU(his_news_repr, his_mask=x['his_mask'].to(self.device), user_index=x['user_id'].to(self.device))\n",
    "        else:\n",
    "            user_repr_ext = None\n",
    "\n",
    "        ps_terms, ps_term_mask, kid, scores = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr_ext, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        _, user_repr, _ = self.bert(ps_terms, ps_term_mask, ps_term_input=True)\n",
    "\n",
    "        if self.aggregator is not None:\n",
    "            user_repr = self.aggregator(user_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return user_repr, kid, his_news_encoded_embedding, user_repr_ext, scores\n",
    "\n",
    "\n",
    "    def compute_score(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        cdd_repr = self.encode_news(x)\n",
    "        user_repr, kid = self.encode_user(x)\n",
    "\n",
    "        score = self.compute_score(cdd_repr, user_repr)\n",
    "\n",
    "        if self.training:\n",
    "            logits = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            logits = torch.sigmoid(score)\n",
    "\n",
    "        return logits, kid\n",
    "\n",
    "\n",
    "    def predict_fast(self, x):\n",
    "        # [bs, cs, hd]\n",
    "        cdd_repr = self.news_reprs(x['cdd_id'].to(self.device))\n",
    "        user_repr, _ = self.encode_user(x)\n",
    "        scores = self.compute_score(cdd_repr, user_repr)\n",
    "        logits = torch.sigmoid(scores)\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "# model = XFormer(manager).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "manager.load(model, 230000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a,b,c,d,e = model.encode_user(x1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.asarray(t.convert_ids_to_tokens(x1['his_encoded_index'][0,0])[1:])[[86,2,93]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "e[0,3,35], e.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(-0.3453, device='cuda:0', grad_fn=<SelectBackward>),\n",
       " torch.Size([1, 50, 99]))"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "t.convert_ids_to_tokens(x1['his_encoded_index'][0,3,[36]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['google']"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(t.convert_ids_to_tokens(x1['his_encoded_index'][0,3]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'trump', \"'\", 's', 'trust', '##bus', '##ters', 'bring', 'microsoft', 'lessons', 'to', 'big', 'tech', 'fight', 'do', '##j', \"'\", 's', 'ma', '##kan', 'del', '##rah', '##im', 'and', 'the', 'ft', '##c', \"'\", 's', 'joe', 'simon', '##s', 'have', 'agreed', 'to', 'divide', 'google', ',', 'facebook', ',', 'amazon', ',', 'and', 'apple', 'between', 'them', '.', 'news', '##bus', '##iness', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "e[0,3], e.shape, e.topk(dim=-1, k=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall = 0\n",
    "count = 0\n",
    "for x in tqdm(loaders[0], ncols=120, leave=True):\n",
    "    kid = model.encode_user(x)[1]\n",
    "    ps_terms = x['his_encoded_index'][:, :, 1:].to(manager.device).gather(index=kid, dim=-1).view(-1).tolist()\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding = 32*50*2*100*768\n",
    "ext_encoding = 32*50*(2*3*384*100 + 4*100*384) + 32*8*2*384*384*50\n",
    "reduction = (2*100*768 + 100*math.log2(100))*32*50*2+32*50*3\n",
    "bert_embedding = 32*150*768*2\n",
    "bert_project = 32*150*768*2*3\n",
    "bert_attn = 32*150*12*64*64*2 + 32*150*12*12*64*2\n",
    "bert_intm = 32*150*768*768*2 + 32*150*768*2 + 32*150*768*3072*4\n",
    "bert_pool = 32*150*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total_isrec = embedding + ext_encoding + reduction + bert\n",
    "\n",
    "embedding = 32*50*2*100*768 + 32*50*100*768*2\n",
    "bert_project = 32*5000*768*2*3\n",
    "bert_attn = 32*5000*12*64*64*2 + 32*5000*12*12*64*2\n",
    "bert_intm = 32*5000*768*768*2 + 32*5000*768*2 + 32*5000*768*3072*4\n",
    "bert_pool = 32*5000*768*4\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "total =  embedding + bert + 32*8*2*768*768*50\n",
    "\n",
    "total_isrec, total"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}