{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare, convert_tokens_to_words\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention\n",
    "\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "m = AutoModel.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')\n",
    "t = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "inputs = t('I love you',return_tensors=\"pt\")\n",
    "m(**inputs), inputs, t.convert_ids_to_tokens(inputs.input_ids[0].tolist())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(BaseModelOutput(last_hidden_state=tensor([[[ 0.0323, -0.0305, -0.0739,  ...,  0.0099,  0.0663, -0.0718],\n",
       "          [-0.6678, -0.7011, -0.5957,  ...,  0.5535, -0.1700, -0.1757],\n",
       "          [-0.8192,  0.0860, -0.5883,  ...,  0.6707, -0.5009, -0.1986],\n",
       "          [-0.9229,  0.1552, -0.1609,  ...,  1.1191, -0.5578,  0.1927],\n",
       "          [ 0.1646,  0.0510, -0.1161,  ...,  0.0469,  0.1763,  0.0288]]],\n",
       "        grad_fn=<AddBackward0>), hidden_states=None, attentions=None),\n",
       " {'input_ids': tensor([[  1, 100, 657,  47,   2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])},\n",
       " ['[CLS]', 'I', 'Ġlove', 'Ġyou', '[SEP]'])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "attn_mask = torch.tensor([[1.,0,1,1,0],[1,0,0,0,0]])\n",
    "hidden_states = torch.rand((2,3,4))\n",
    "extended_attn_mask = m.encoder.get_attention_mask(attn_mask)\n",
    "rel_pos = m.encoder.get_rel_pos(hidden_states)\n",
    "\n",
    "rel_pos, extended_attn_mask"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 0, -1, -2],\n",
       "          [ 1,  0, -1],\n",
       "          [ 2,  1,  0]]]),\n",
       " tensor([[[[1, 1, 1, 1, 0],\n",
       "           [1, 1, 1, 1, 0],\n",
       "           [1, 1, 1, 1, 0],\n",
       "           [1, 1, 1, 1, 0],\n",
       "           [0, 0, 0, 0, 0]]],\n",
       " \n",
       " \n",
       "         [[[1, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0]]]], dtype=torch.uint8))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# config.reducer = 'entity'\n",
    "config.embedding = 'deberta'\n",
    "config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "\n",
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "x1 = list(loaders[0])[0]\n",
    "x2 = list(loaders[1])[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-09-08 15:46:47,558] INFO (utils.utils) Hyper Parameters are \n",
      "scale:demo\n",
      "batch_size:5\n",
      "k:5\n",
      "threshold:-inf\n",
      "signal_length:100\n",
      "his_size:50\n",
      "impr_size:10\n",
      "lr:0.0001\n",
      "bert_lr:3e-05\n",
      "hidden_dim:384\n",
      "world_size:0\n",
      "step:0\n",
      "ascend_history:False\n",
      "no_dedup:False\n",
      "diversify:False\n",
      "granularity:avg\n",
      "no_sep_his:False\n",
      "no_order_embed:False\n",
      "bert:microsoft/deberta-base\n",
      "[2021-09-08 15:46:47,559] INFO (utils.utils) preparing dataset...\n",
      "[2021-09-08 15:46:47,564] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/deberta/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-09-08 15:46:47,577] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/deberta/MINDdemo_train/news.pkl\n",
      "[2021-09-08 15:46:48,481] INFO (utils.utils) deduplicating...\n",
      "[2021-09-08 15:46:49,979] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/deberta/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-09-08 15:46:50,016] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/deberta/MINDdemo_dev/news.pkl\n",
      "[2021-09-08 15:46:50,727] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TTMS(nn.Module):\n",
    "    def __init__(self, config, embedding, encoderN, encoderU, reducer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = config.scale\n",
    "        self.cdd_size = config.cdd_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.his_size = config.his_size\n",
    "        self.signal_length = config.signal_length\n",
    "        self.device = config.device\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderN = encoderN\n",
    "        self.encoderU = encoderU\n",
    "\n",
    "        self.reducer = reducer\n",
    "        self.bert = BERT_Encoder(config)\n",
    "\n",
    "        self.granularity = config.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, config.impr_size, config.signal_length * config.signal_length)), persistent=False)\n",
    "            if config.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (config.k + 1) * (config.k + 1))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, config.signal_length * config.signal_length)), persistent=False)\n",
    "\n",
    "        self.userProject = nn.Sequential(\n",
    "            nn.Linear(self.bert.hidden_dim, self.bert.hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.register_buffer('extra_cls_mask', torch.ones(1,1), persistent=False)\n",
    "\n",
    "        config.name = '__'.join(['ttms', config.embedding, config.encoderN, config.encoderU, config.reducer, config.granularity])\n",
    "\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        # print(user_repr.mean(), cdd_news_repr.mean(), user_repr.max(), cdd_news_repr.max(), user_repr.sum(), cdd_news_repr.sum())\n",
    "        # score = F.normalize(cdd_news_repr, dim=-1).matmul(F.normalize(user_repr, dim=-1).transpose(-2,-1)).squeeze(-1)\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)\n",
    "        return score\n",
    "\n",
    "    def _forward(self,x):\n",
    "        if self.granularity != 'token':\n",
    "            batch_size = x['cdd_subword_index'].size(0)\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                if batch_size != self.batch_size:\n",
    "                    cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                    his_dest = self.his_dest[:batch_size]\n",
    "                else:\n",
    "                    cdd_dest = self.cdd_dest[:, :cdd_size]\n",
    "                    his_dest = self.his_dest\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = his_subword_prefix.matmul(x[\"his_refined_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "            his_refined_mask = None\n",
    "            if 'his_refined_mask' in x:\n",
    "                his_refined_mask = x[\"his_refined_mask\"].to(self.device)\n",
    "\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].long().to(self.device)\n",
    "        _, cdd_news_repr = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].long().to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news, his_subword_prefix)\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding\n",
    "        )\n",
    "        # no need to calculate this if ps_terms are fixed in advance\n",
    "        if self.reducer.name == 'matching':\n",
    "            user_repr = self.encoderU(his_news_repr)\n",
    "        else:\n",
    "            user_repr = None\n",
    "\n",
    "        ps_terms, ps_term_mask, kid = self.reducer(his_news_encoded_embedding, his_news_embedding, user_repr, his_news_repr, his_attn_mask, his_refined_mask)\n",
    "\n",
    "        # append CLS to the entire browsing history, directly deriving user repr\n",
    "        batch_size = ps_terms.size(0)\n",
    "        ps_terms = torch.cat([his_news_embedding[:, 0, 0].unsqueeze(1), ps_terms], dim=-2)\n",
    "        ps_term_mask = torch.cat([self.extra_cls_mask.expand(batch_size, 1), ps_term_mask], dim=-1)\n",
    "        _, user_cls = self.bert(ps_terms.unsqueeze(1), ps_term_mask.unsqueeze(1))\n",
    "        user_repr = self.userProject(user_cls)\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), kid\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, kid = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, kid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "# encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a,b = model(x1)\n",
    "a,b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-2.3317, -2.3317, -1.3146, -1.3146, -1.3146],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.9124, -1.9123, -1.9123, -1.9124, -0.8939],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]],\n",
       "        grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[[26, 25, 12, 16,  9],\n",
       "          [ 3, 36,  2,  8, 26],\n",
       "          [11, 38,  1, 23,  4],\n",
       "          ...,\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3]],\n",
       " \n",
       "         [[22, 21, 27, 14,  1],\n",
       "          [ 9, 30, 16, 25, 11],\n",
       "          [ 3, 20, 19, 24, 23],\n",
       "          ...,\n",
       "          [31, 24, 28, 29, 25],\n",
       "          [16, 12,  3,  7, 14],\n",
       "          [18, 33, 19, 37, 17]],\n",
       " \n",
       "         [[32, 17,  1, 19,  3],\n",
       "          [ 1, 12,  2,  7, 26],\n",
       "          [32,  4,  2,  5, 15],\n",
       "          ...,\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3]],\n",
       " \n",
       "         [[ 7, 19, 21, 20, 11],\n",
       "          [10, 26,  8, 11, 28],\n",
       "          [ 7, 18, 23, 12, 24],\n",
       "          ...,\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3]],\n",
       " \n",
       "         [[28, 21, 24,  6,  4],\n",
       "          [26,  3, 27,  7,  4],\n",
       "          [32, 21, 23,  6, 30],\n",
       "          ...,\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3],\n",
       "          [ 1,  0,  4,  2,  3]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "ls = loss(a,target=x1['label'])\n",
    "ls.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "a.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}