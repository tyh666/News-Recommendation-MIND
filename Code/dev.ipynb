{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = AutoModel.from_pretrained(\"microsoft/unilm-base-cased\",cache_dir=config.path + 'bert_cache/')\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15c6ef3cb6b64fd28313b1df13fe3395"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Unrecognized model in microsoft/unilm-base-cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_203188/87780649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'microsoft/unilm-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'bert_cache/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/workspace/Peitian/nn/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    379\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             )\n",
      "\u001b[0;32m/data/workspace/Peitian/nn/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;34m\"Should have a `model_type` key in its config.json, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in microsoft/unilm-base-cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "\n",
    "manager = Manager(config)\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "x2 = X2[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-09-27 13:52:48,037] INFO (utils.Manager) Hyper Parameters are \n",
      "{\n",
      "    \"scale\": \"demo\",\n",
      "    \"mode\": \"tune\",\n",
      "    \"batch_size\": 5,\n",
      "    \"k\": 5,\n",
      "    \"threshold\": -Infinity,\n",
      "    \"abs_length\": 40,\n",
      "    \"signal_length\": 100,\n",
      "    \"his_size\": 50,\n",
      "    \"cdd_size\": 5,\n",
      "    \"impr_size\": 10,\n",
      "    \"dropout_p\": 0.2,\n",
      "    \"lr\": 0.0001,\n",
      "    \"bert_lr\": 3e-05,\n",
      "    \"embedding\": \"bert\",\n",
      "    \"encoderN\": \"cnn\",\n",
      "    \"encoderU\": \"rnn\",\n",
      "    \"selector\": \"sfi\",\n",
      "    \"reducer\": \"matching\",\n",
      "    \"ranker\": \"onepass\",\n",
      "    \"embedding_dim\": 768,\n",
      "    \"hidden_dim\": 384,\n",
      "    \"base_rank\": 0,\n",
      "    \"world_size\": 0,\n",
      "    \"seed\": 42,\n",
      "    \"granularity\": \"avg\",\n",
      "    \"debias\": false,\n",
      "    \"full_attn\": true,\n",
      "    \"ascend_history\": false,\n",
      "    \"save_pos\": false,\n",
      "    \"sep_his\": false,\n",
      "    \"diversify\": false,\n",
      "    \"no_dedup\": false,\n",
      "    \"no_order_embed\": false,\n",
      "    \"no_rm_punc\": false,\n",
      "    \"scheduler\": \"linear\",\n",
      "    \"warmup\": 100,\n",
      "    \"shuffle\": false,\n",
      "    \"bert\": \"bert-base-uncased\",\n",
      "    \"tb\": false\n",
      "}\n",
      "[2021-09-27 13:52:48,037] INFO (utils.Manager) preparing dataset...\n",
      "[2021-09-27 13:52:48,039] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-09-27 13:52:48,053] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-09-27 13:52:48,908] INFO (utils.utils) deduplicating...\n",
      "[2021-09-27 13:52:50,518] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-09-27 13:52:50,521] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_dev/news.pkl\n",
      "[2021-09-27 13:52:51,261] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "# model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "bert_params = []\n",
    "base_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if re.search(\"bert\", name):\n",
    "        print(name)\n",
    "        bert_params.append(param)\n",
    "    else:\n",
    "        base_params.append(param)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "embedding.bert_word_embedding.weight\n",
      "ranker.bert.layer.0.attention.self.query.weight\n",
      "ranker.bert.layer.0.attention.self.query.bias\n",
      "ranker.bert.layer.0.attention.self.key.weight\n",
      "ranker.bert.layer.0.attention.self.key.bias\n",
      "ranker.bert.layer.0.attention.self.value.weight\n",
      "ranker.bert.layer.0.attention.self.value.bias\n",
      "ranker.bert.layer.0.attention.output.dense.weight\n",
      "ranker.bert.layer.0.attention.output.dense.bias\n",
      "ranker.bert.layer.0.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.0.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.0.intermediate.dense.weight\n",
      "ranker.bert.layer.0.intermediate.dense.bias\n",
      "ranker.bert.layer.0.output.dense.weight\n",
      "ranker.bert.layer.0.output.dense.bias\n",
      "ranker.bert.layer.0.output.LayerNorm.weight\n",
      "ranker.bert.layer.0.output.LayerNorm.bias\n",
      "ranker.bert.layer.1.attention.self.query.weight\n",
      "ranker.bert.layer.1.attention.self.query.bias\n",
      "ranker.bert.layer.1.attention.self.key.weight\n",
      "ranker.bert.layer.1.attention.self.key.bias\n",
      "ranker.bert.layer.1.attention.self.value.weight\n",
      "ranker.bert.layer.1.attention.self.value.bias\n",
      "ranker.bert.layer.1.attention.output.dense.weight\n",
      "ranker.bert.layer.1.attention.output.dense.bias\n",
      "ranker.bert.layer.1.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.1.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.1.intermediate.dense.weight\n",
      "ranker.bert.layer.1.intermediate.dense.bias\n",
      "ranker.bert.layer.1.output.dense.weight\n",
      "ranker.bert.layer.1.output.dense.bias\n",
      "ranker.bert.layer.1.output.LayerNorm.weight\n",
      "ranker.bert.layer.1.output.LayerNorm.bias\n",
      "ranker.bert.layer.2.attention.self.query.weight\n",
      "ranker.bert.layer.2.attention.self.query.bias\n",
      "ranker.bert.layer.2.attention.self.key.weight\n",
      "ranker.bert.layer.2.attention.self.key.bias\n",
      "ranker.bert.layer.2.attention.self.value.weight\n",
      "ranker.bert.layer.2.attention.self.value.bias\n",
      "ranker.bert.layer.2.attention.output.dense.weight\n",
      "ranker.bert.layer.2.attention.output.dense.bias\n",
      "ranker.bert.layer.2.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.2.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.2.intermediate.dense.weight\n",
      "ranker.bert.layer.2.intermediate.dense.bias\n",
      "ranker.bert.layer.2.output.dense.weight\n",
      "ranker.bert.layer.2.output.dense.bias\n",
      "ranker.bert.layer.2.output.LayerNorm.weight\n",
      "ranker.bert.layer.2.output.LayerNorm.bias\n",
      "ranker.bert.layer.3.attention.self.query.weight\n",
      "ranker.bert.layer.3.attention.self.query.bias\n",
      "ranker.bert.layer.3.attention.self.key.weight\n",
      "ranker.bert.layer.3.attention.self.key.bias\n",
      "ranker.bert.layer.3.attention.self.value.weight\n",
      "ranker.bert.layer.3.attention.self.value.bias\n",
      "ranker.bert.layer.3.attention.output.dense.weight\n",
      "ranker.bert.layer.3.attention.output.dense.bias\n",
      "ranker.bert.layer.3.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.3.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.3.intermediate.dense.weight\n",
      "ranker.bert.layer.3.intermediate.dense.bias\n",
      "ranker.bert.layer.3.output.dense.weight\n",
      "ranker.bert.layer.3.output.dense.bias\n",
      "ranker.bert.layer.3.output.LayerNorm.weight\n",
      "ranker.bert.layer.3.output.LayerNorm.bias\n",
      "ranker.bert.layer.4.attention.self.query.weight\n",
      "ranker.bert.layer.4.attention.self.query.bias\n",
      "ranker.bert.layer.4.attention.self.key.weight\n",
      "ranker.bert.layer.4.attention.self.key.bias\n",
      "ranker.bert.layer.4.attention.self.value.weight\n",
      "ranker.bert.layer.4.attention.self.value.bias\n",
      "ranker.bert.layer.4.attention.output.dense.weight\n",
      "ranker.bert.layer.4.attention.output.dense.bias\n",
      "ranker.bert.layer.4.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.4.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.4.intermediate.dense.weight\n",
      "ranker.bert.layer.4.intermediate.dense.bias\n",
      "ranker.bert.layer.4.output.dense.weight\n",
      "ranker.bert.layer.4.output.dense.bias\n",
      "ranker.bert.layer.4.output.LayerNorm.weight\n",
      "ranker.bert.layer.4.output.LayerNorm.bias\n",
      "ranker.bert.layer.5.attention.self.query.weight\n",
      "ranker.bert.layer.5.attention.self.query.bias\n",
      "ranker.bert.layer.5.attention.self.key.weight\n",
      "ranker.bert.layer.5.attention.self.key.bias\n",
      "ranker.bert.layer.5.attention.self.value.weight\n",
      "ranker.bert.layer.5.attention.self.value.bias\n",
      "ranker.bert.layer.5.attention.output.dense.weight\n",
      "ranker.bert.layer.5.attention.output.dense.bias\n",
      "ranker.bert.layer.5.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.5.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.5.intermediate.dense.weight\n",
      "ranker.bert.layer.5.intermediate.dense.bias\n",
      "ranker.bert.layer.5.output.dense.weight\n",
      "ranker.bert.layer.5.output.dense.bias\n",
      "ranker.bert.layer.5.output.LayerNorm.weight\n",
      "ranker.bert.layer.5.output.LayerNorm.bias\n",
      "ranker.bert.layer.6.attention.self.query.weight\n",
      "ranker.bert.layer.6.attention.self.query.bias\n",
      "ranker.bert.layer.6.attention.self.key.weight\n",
      "ranker.bert.layer.6.attention.self.key.bias\n",
      "ranker.bert.layer.6.attention.self.value.weight\n",
      "ranker.bert.layer.6.attention.self.value.bias\n",
      "ranker.bert.layer.6.attention.output.dense.weight\n",
      "ranker.bert.layer.6.attention.output.dense.bias\n",
      "ranker.bert.layer.6.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.6.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.6.intermediate.dense.weight\n",
      "ranker.bert.layer.6.intermediate.dense.bias\n",
      "ranker.bert.layer.6.output.dense.weight\n",
      "ranker.bert.layer.6.output.dense.bias\n",
      "ranker.bert.layer.6.output.LayerNorm.weight\n",
      "ranker.bert.layer.6.output.LayerNorm.bias\n",
      "ranker.bert.layer.7.attention.self.query.weight\n",
      "ranker.bert.layer.7.attention.self.query.bias\n",
      "ranker.bert.layer.7.attention.self.key.weight\n",
      "ranker.bert.layer.7.attention.self.key.bias\n",
      "ranker.bert.layer.7.attention.self.value.weight\n",
      "ranker.bert.layer.7.attention.self.value.bias\n",
      "ranker.bert.layer.7.attention.output.dense.weight\n",
      "ranker.bert.layer.7.attention.output.dense.bias\n",
      "ranker.bert.layer.7.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.7.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.7.intermediate.dense.weight\n",
      "ranker.bert.layer.7.intermediate.dense.bias\n",
      "ranker.bert.layer.7.output.dense.weight\n",
      "ranker.bert.layer.7.output.dense.bias\n",
      "ranker.bert.layer.7.output.LayerNorm.weight\n",
      "ranker.bert.layer.7.output.LayerNorm.bias\n",
      "ranker.bert.layer.8.attention.self.query.weight\n",
      "ranker.bert.layer.8.attention.self.query.bias\n",
      "ranker.bert.layer.8.attention.self.key.weight\n",
      "ranker.bert.layer.8.attention.self.key.bias\n",
      "ranker.bert.layer.8.attention.self.value.weight\n",
      "ranker.bert.layer.8.attention.self.value.bias\n",
      "ranker.bert.layer.8.attention.output.dense.weight\n",
      "ranker.bert.layer.8.attention.output.dense.bias\n",
      "ranker.bert.layer.8.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.8.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.8.intermediate.dense.weight\n",
      "ranker.bert.layer.8.intermediate.dense.bias\n",
      "ranker.bert.layer.8.output.dense.weight\n",
      "ranker.bert.layer.8.output.dense.bias\n",
      "ranker.bert.layer.8.output.LayerNorm.weight\n",
      "ranker.bert.layer.8.output.LayerNorm.bias\n",
      "ranker.bert.layer.9.attention.self.query.weight\n",
      "ranker.bert.layer.9.attention.self.query.bias\n",
      "ranker.bert.layer.9.attention.self.key.weight\n",
      "ranker.bert.layer.9.attention.self.key.bias\n",
      "ranker.bert.layer.9.attention.self.value.weight\n",
      "ranker.bert.layer.9.attention.self.value.bias\n",
      "ranker.bert.layer.9.attention.output.dense.weight\n",
      "ranker.bert.layer.9.attention.output.dense.bias\n",
      "ranker.bert.layer.9.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.9.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.9.intermediate.dense.weight\n",
      "ranker.bert.layer.9.intermediate.dense.bias\n",
      "ranker.bert.layer.9.output.dense.weight\n",
      "ranker.bert.layer.9.output.dense.bias\n",
      "ranker.bert.layer.9.output.LayerNorm.weight\n",
      "ranker.bert.layer.9.output.LayerNorm.bias\n",
      "ranker.bert.layer.10.attention.self.query.weight\n",
      "ranker.bert.layer.10.attention.self.query.bias\n",
      "ranker.bert.layer.10.attention.self.key.weight\n",
      "ranker.bert.layer.10.attention.self.key.bias\n",
      "ranker.bert.layer.10.attention.self.value.weight\n",
      "ranker.bert.layer.10.attention.self.value.bias\n",
      "ranker.bert.layer.10.attention.output.dense.weight\n",
      "ranker.bert.layer.10.attention.output.dense.bias\n",
      "ranker.bert.layer.10.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.10.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.10.intermediate.dense.weight\n",
      "ranker.bert.layer.10.intermediate.dense.bias\n",
      "ranker.bert.layer.10.output.dense.weight\n",
      "ranker.bert.layer.10.output.dense.bias\n",
      "ranker.bert.layer.10.output.LayerNorm.weight\n",
      "ranker.bert.layer.10.output.LayerNorm.bias\n",
      "ranker.bert.layer.11.attention.self.query.weight\n",
      "ranker.bert.layer.11.attention.self.query.bias\n",
      "ranker.bert.layer.11.attention.self.key.weight\n",
      "ranker.bert.layer.11.attention.self.key.bias\n",
      "ranker.bert.layer.11.attention.self.value.weight\n",
      "ranker.bert.layer.11.attention.self.value.bias\n",
      "ranker.bert.layer.11.attention.output.dense.weight\n",
      "ranker.bert.layer.11.attention.output.dense.bias\n",
      "ranker.bert.layer.11.attention.output.LayerNorm.weight\n",
      "ranker.bert.layer.11.attention.output.LayerNorm.bias\n",
      "ranker.bert.layer.11.intermediate.dense.weight\n",
      "ranker.bert.layer.11.intermediate.dense.bias\n",
      "ranker.bert.layer.11.output.dense.weight\n",
      "ranker.bert.layer.11.output.dense.bias\n",
      "ranker.bert.layer.11.output.LayerNorm.weight\n",
      "ranker.bert.layer.11.output.LayerNorm.bias\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# model.eval()\n",
    "# a,b = model(x2)\n",
    "\n",
    "a,b = model(x1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) tensor([[ 0.0355,  0.0448,  0.0208, -0.0203,  0.0166,  0.0120,  0.0257, -0.0202,\n",
      "         -0.0085, -0.0028, -0.0179,  0.0275,  0.0078,  0.0224,  0.0461, -0.0076,\n",
      "          0.0014,  0.0550, -0.0108,  0.0278,  0.0370,  0.0274,  0.0493,  0.0446,\n",
      "          0.0155, -0.0023,  0.0668,  0.0236,  0.0234,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,  0.0313,\n",
      "          0.0313,  0.0313,  0.0313,  0.0695]], grad_fn=<SelectBackward>) tensor([[0.0363, 0.0367, 0.0358, 0.0343, 0.0356, 0.0355, 0.0360, 0.0343, 0.0348,\n",
      "         0.0350, 0.0344, 0.0360, 0.0353, 0.0358, 0.0367, 0.0348, 0.0351, 0.0370,\n",
      "         0.0347, 0.0360, 0.0364, 0.0360, 0.0368, 0.0367, 0.0356, 0.0350, 0.0375,\n",
      "         0.0359, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}