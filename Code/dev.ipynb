{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare, convert_tokens_to_words\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Slicing_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TTMS import TTMS\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention\n",
    "\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "m = AutoModel.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')\n",
    "# t = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir='../../../Data/bert_cache')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "attn_mask = torch.tensor([[1.,1,1,1,0],[1,0,0,0,0]])\n",
    "hidden_states = torch.rand((2,3,4))\n",
    "extended_attn_mask = m.encoder.get_attention_mask(attn_mask)\n",
    "rel_pos = m.encoder.get_rel_pos(hidden_states)\n",
    "\n",
    "rel_pos, extended_attn_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "\n",
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "x1 = list(loaders[0])[0]\n",
    "x2 = list(loaders[1])[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-09-08 12:07:26,355] INFO (utils.utils) Hyper Parameters are \n",
      "scale:demo\n",
      "batch_size:5\n",
      "k:5\n",
      "threshold:-inf\n",
      "signal_length:100\n",
      "his_size:50\n",
      "impr_size:10\n",
      "lr:0.0001\n",
      "bert_lr:3e-05\n",
      "hidden_dim:384\n",
      "world_size:0\n",
      "step:0\n",
      "ascend_history:False\n",
      "no_dedup:False\n",
      "diversify:False\n",
      "granularity:avg\n",
      "no_sep_his:False\n",
      "no_order_embed:False\n",
      "bert:bert-base-uncased\n",
      "[2021-09-08 12:07:26,356] INFO (utils.utils) preparing dataset...\n",
      "[2021-09-08 12:07:26,360] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-09-08 12:07:26,373] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-09-08 12:07:27,189] INFO (utils.utils) deduplicating...\n",
      "[2021-09-08 12:07:28,670] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/bert/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-09-08 12:07:28,712] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/bert/MINDdemo_dev/news.pkl\n",
      "[2021-09-08 12:07:29,370] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "# encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "reducer = Matching_Reducer(manager)\n",
    "# reducer = Slicing_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "model = TTMS(manager, embedding, encoderN, encoderU, reducer).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "a,b = model(x1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.3368, 0.3549, 0.3033,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3194, 0.3795, 0.3507,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3151, 0.3332, 0.3297,  ...,   -inf,   -inf,   -inf],\n",
      "         ...,\n",
      "         [0.9985, 0.9985, 0.9985,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9985, 0.9985, 0.9985,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9985, 0.9985, 0.9985,  ...,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.4536, 0.4767, 0.4908,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.4972, 0.4764, 0.4597,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.4207, 0.4683, 0.4715,  ...,   -inf,   -inf,   -inf],\n",
      "         ...,\n",
      "         [0.4426, 0.4748, 0.4656,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.4551, 0.4502, 0.4448,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.4485, 0.4862, 0.4810,  ...,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.3354, 0.3199, 0.3452,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3481, 0.3353, 0.3381,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3290, 0.3642, 0.3817,  ...,   -inf,   -inf,   -inf],\n",
      "         ...,\n",
      "         [0.9959, 0.9959, 0.9959,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9959, 0.9959, 0.9959,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9959, 0.9959, 0.9959,  ...,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.3054, 0.3667, 0.3786,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3143, 0.3679, 0.3863,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3248, 0.3205, 0.3920,  ...,   -inf,   -inf,   -inf],\n",
      "         ...,\n",
      "         [0.9988, 0.9988, 0.9988,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9988, 0.9988, 0.9988,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9988, 0.9988, 0.9988,  ...,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.3246, 0.3116, 0.3366,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3068, 0.3154, 0.3597,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.3372, 0.3176, 0.3132,  ...,   -inf,   -inf,   -inf],\n",
      "         ...,\n",
      "         [0.9999, 0.9999, 0.9999,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9999, 0.9999, 0.9999,  ...,   -inf,   -inf,   -inf],\n",
      "         [0.9999, 0.9999, 0.9999,  ...,   -inf,   -inf,   -inf]]],\n",
      "       grad_fn=<MaskedFillBackward0>) torch.Size([5, 50, 99])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "ls = loss(a,target=x1['label'])\n",
    "ls.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d.grad"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}