{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.TwoTowerBaseModel import TwoTowerBaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    "from models.PLM import PLM\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "# torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "# m4 = AutoModel.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# m5 = AutoModel.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# m6 = AutoModel.from_pretrained('distilbert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m7 = AutoModel.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")\n",
    "# m8 = AutoModel.from_pretrained(\"allenai/longformer-base-4096\", cache_dir=config.path + \"bert_cache/\")\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")\n",
    "# t4 = AutoTokenizer.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# t5 = AutoTokenizer.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t6 = AutoTokenizer.from_pretrained('distilbert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t7 = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "config.bert = 'bert'\n",
    "config.mode = \"encode\"\n",
    "config.scale = \"large\"\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "# loaders = manager.prepare()\n",
    "# X1 = list(loaders[0])\n",
    "# # X2 = list(loaders[1])\n",
    "# x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "manager.gather_same_user_impr()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-11-22 15:06:12,105] INFO (root) gathering behavior log of ../../../Data/MIND/MINDlarge_dev/behaviors.tsv\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../Data/MIND/MINDlarge_dev/behaviors.tsv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20867/447037552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_same_user_impr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/v-pezhang/Code/Document-Reduction/Code/utils/Manager.py\u001b[0m in \u001b[0;36mgather_same_user_impr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0mbehaviors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehav_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mimpr_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../Data/MIND/MINDlarge_dev/behaviors.tsv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# embedding = BERT_Embedding(manager)\n",
    "manager.hidden_dim = 768\n",
    "manager.reducer = \"none\"\n",
    "# encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "# reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "# model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "model = PLM(manager, encoderU).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import math\n",
    "# GateFormer\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 8*150*150*50\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20479554310.33589"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# GateFormer TTW\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 8*150*150*50\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 50 * 3 * 2 * 768 * 2\n",
    "bert_project = 50*3*12*64*2\n",
    "bert_attn = 50*3*12*64*3*2 + 50*12*3*3*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "agg = 50*768*768*8\n",
    "\n",
    "sum = embedding + agg + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19902632710.33589"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# GateFormer global\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30)\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20469654310.33589"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer First\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer BERT\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*30*768*768*2*3 + 50*30*768*30*2*2 + 50*30*768*2*2\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer ATT\n",
    "import math\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 50*150*4\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 50*3*768*2*2\n",
    "bert_project = 50*12*3*64*2*3\n",
    "bert_attn = 50*12*3*64*3*2 + 50*12*3*3*64*2\n",
    "bert_intm = 50*3*768*768*2 + 50*3*768*2 + 50*3*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = ext_encoding + reduction + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer\n",
    "embedding = 50 * 2 * 100 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NRMS\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "project = 50 * 30 * 300 * 256 * 2 * 3\n",
    "attn = 50*30*256*30*4\n",
    "attnA = 50*30*256*2\n",
    "\n",
    "project2 = 50*256*256*2*3\n",
    "attn2 = 50*256*50*4\n",
    "attnA2 = 50*256*2\n",
    "\n",
    "sum = embedding + project + project2 + attn + attn2 + attnA + attnA2\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LSTUR\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "encoding = 3*300*150*30*50\n",
    "attn=50*30*150*2 + 50*30*150*2\n",
    "lstm = 50*150*150*8*2\n",
    "\n",
    "sum = embedding + encoding + lstm + attn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NAML\n",
    "embedding = 50 * 2 * 30 * 300 *2 *2\n",
    "encoding = 3*300*150*30*50*2\n",
    "attn=(50*30*150*2 + 50*30*150*2)*2\n",
    "attn2=50*150*2 + 50*150*2\n",
    "sum = embedding + encoding + attn + attn2\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BigBird\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*12*64*2 + 12*1024*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# longformer\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*512*12*64*2 + 12*1024*512*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EBNR\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Synthesizer\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "193334169600"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# funnel\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 10 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# newbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 4 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# distilbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 2\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 6 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "189 / 255560 * 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}