{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.BaseModel import BaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.ESM import ESM\n",
    "from models.TESRec import TESRec\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'config', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# config.reducer = 'entity'\n",
    "# config.embedding = 'deberta'\n",
    "# config.bert = 'microsoft/deberta-base'\n",
    "# config.device = 0\n",
    "config.seed = None\n",
    "manager = Manager(config)\n",
    "\n",
    "manager.hidden_dim = 768\n",
    "manager.bert = 'deberta'\n",
    "manager.granularity = 'token'\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "x2 = X2[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-10-21 11:00:10,823] INFO (utils.Manager) Hyper Parameters are \n",
      "{\n",
      "    \"scale\": \"demo\",\n",
      "    \"mode\": \"train\",\n",
      "    \"batch_size\": 5,\n",
      "    \"batch_size_news\": 100,\n",
      "    \"batch_size_history\": 100,\n",
      "    \"k\": 5,\n",
      "    \"threshold\": -Infinity,\n",
      "    \"abs_length\": 40,\n",
      "    \"signal_length\": 100,\n",
      "    \"his_size\": 50,\n",
      "    \"cdd_size\": 5,\n",
      "    \"impr_size\": 10,\n",
      "    \"dropout_p\": 0.2,\n",
      "    \"lr\": 0.0001,\n",
      "    \"bert_lr\": 3e-05,\n",
      "    \"embedding\": \"bert\",\n",
      "    \"encoderN\": \"cnn\",\n",
      "    \"encoderU\": \"rnn\",\n",
      "    \"selector\": \"sfi\",\n",
      "    \"reducer\": \"matching\",\n",
      "    \"ranker\": \"onepass\",\n",
      "    \"pooler\": \"attn\",\n",
      "    \"bert_dim\": 768,\n",
      "    \"hidden_dim\": 768,\n",
      "    \"base_rank\": 0,\n",
      "    \"world_size\": 0,\n",
      "    \"seed\": null,\n",
      "    \"granularity\": \"token\",\n",
      "    \"debias\": true,\n",
      "    \"full_attn\": true,\n",
      "    \"descend_history\": false,\n",
      "    \"shuffle_pos\": false,\n",
      "    \"save_pos\": false,\n",
      "    \"sep_his\": false,\n",
      "    \"diversify\": false,\n",
      "    \"no_dedup\": false,\n",
      "    \"no_order_embed\": false,\n",
      "    \"no_rm_punc\": false,\n",
      "    \"fast\": false,\n",
      "    \"scheduler\": \"linear\",\n",
      "    \"warmup\": 100,\n",
      "    \"shuffle\": false,\n",
      "    \"bert\": \"deberta\",\n",
      "    \"tb\": false\n",
      "}\n",
      "[2021-10-21 11:00:10,824] INFO (utils.Manager) preparing dataset...\n",
      "[2021-10-21 11:00:10,825] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/deberta/MINDdemo_train/behaviors.pkl\n",
      "[2021-10-21 11:00:10,874] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/deberta/MINDdemo_train/news.pkl\n",
      "[2021-10-21 11:00:11,697] INFO (utils.utils) deduplicating...\n",
      "[2021-10-21 11:00:13,513] INFO (utils.MIND) process NO.0 loading cached user behavior from data/cache/deberta/MINDdemo_dev/10/behaviors.pkl\n",
      "[2021-10-21 11:00:13,518] INFO (utils.MIND) process NO.0 loading cached news tokenization from data/cache/deberta/MINDdemo_dev/news.pkl\n",
      "[2021-10-21 11:00:14,145] INFO (utils.utils) deduplicating...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class PLM(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, encoderU):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.encoderU = encoderU\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,manager.bert_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        manager.name = '__'.join([\"plm\", manager.bert, manager.encoderU, manager.granularity])\n",
    "        self.name = manager.name\n",
    "\n",
    "        if manager.bert == 'unilm':\n",
    "            config = TuringNLRv3Config.from_pretrained(manager.unilm_config_path)\n",
    "            bert = TuringNLRv3ForSequenceClassification.from_pretrained(manager.unilm_path, config=config).bert\n",
    "\n",
    "        elif manager.bert == 'deberta':\n",
    "            # add a pooler\n",
    "            bert = AutoModel.from_pretrained(\n",
    "                manager.get_bert_for_load(),\n",
    "                cache_dir=manager.path + 'bert_cache/'\n",
    "            )\n",
    "            self.pooler = nn.Sequential(\n",
    "                nn.Linear(manager.bert_dim, manager.bert_dim),\n",
    "                nn.GELU()\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            bert = AutoModel.from_pretrained(\n",
    "                manager.get_bert_for_load(),\n",
    "                cache_dir=manager.path + 'bert_cache/'\n",
    "            )\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _forward(self,x):\n",
    "        # destroy encoding and embedding outside of the model\n",
    "\n",
    "        batch_size = x['cdd_encoded_index'].size(0)\n",
    "\n",
    "        if self.granularity != 'token':\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                # * cdd_mask to filter out padded cdd news\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device).view(-1, self.signal_length)\n",
    "        cdd_attn_mask = cdd_attn_mask.view(-1, self.signal_length)\n",
    "        bert_output = self.bert(cdd_news, cdd_attn_mask)\n",
    "        cdd_news_repr = bert_output[-1]\n",
    "        cdd_news_embedding = bert_output[0]\n",
    "        if hasattr(self, 'pooler'):\n",
    "            cdd_news_repr = self.pooler(cdd_news_repr[:, 0])\n",
    "        cdd_news_repr = cdd_news_repr.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device).view(-1, self.signal_length)\n",
    "        his_attn_mask = his_attn_mask.view(-1, self.signal_length)\n",
    "        his_news_repr = self.bert(his_news, his_attn_mask)[-1]\n",
    "        if hasattr(self, 'pooler'):\n",
    "            his_news_repr = self.pooler(his_news_repr[:, 0])\n",
    "        his_news_repr = his_news_repr.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        user_repr = self.encoderU(his_news_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), cdd_news_embedding + (cdd_news_repr, user_repr)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, cdd_news_embedding = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, cdd_news_embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class PLM2(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, embedding, encoderU):\n",
    "        super().__init__(manager)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderU = encoderU\n",
    "        self.bert = BERT_Encoder(manager)\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,manager.bert_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        manager.name = '__'.join([\"plm2\", manager.bert, manager.encoderU, manager.granularity])\n",
    "        self.name = manager.name\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _forward(self,x):\n",
    "        # destroy encoding and embedding outside of the model\n",
    "\n",
    "        batch_size = x['cdd_encoded_index'].size(0)\n",
    "\n",
    "        if self.granularity != 'token':\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                # * cdd_mask to filter out padded cdd news\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device)\n",
    "            his_attn_mask = x[\"his_attn_mask\"].to(self.device)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device)\n",
    "        cdd_news_embedding, cdd_news_repr = self.bert(\n",
    "            self.embedding(cdd_news, cdd_subword_prefix), cdd_attn_mask\n",
    "        )\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device)\n",
    "        _, his_news_repr = self.bert(\n",
    "            self.embedding(his_news, his_subword_prefix), his_attn_mask\n",
    "        )\n",
    "\n",
    "        user_repr = self.encoderU(his_news_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), cdd_news_embedding\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, cdd_news_embedding = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, cdd_news_embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "class PLM3(BaseModel):\n",
    "    \"\"\"\n",
    "    Tow tower model with selection\n",
    "\n",
    "    1. encode candidate news with bert\n",
    "    2. encode ps terms with the same bert, using [CLS] embedding as user representation\n",
    "    3. predict by scaled dot product\n",
    "    \"\"\"\n",
    "    def __init__(self, manager, encoderU):\n",
    "        from transformers import AutoModel\n",
    "        super().__init__(manager)\n",
    "\n",
    "        if manager.bert == 'unilm':\n",
    "            config = TuringNLRv3Config.from_pretrained(manager.unilm_config_path)\n",
    "            # config.pooler = None\n",
    "            bert = TuringNLRv3ForSequenceClassification.from_pretrained(manager.unilm_path, config=config).bert\n",
    "\n",
    "            self.rel_pos_bins = config.rel_pos_bins\n",
    "            self.max_rel_pos = config.max_rel_pos\n",
    "            # unique in UniLM\n",
    "            self.rel_pos_bias = bert.rel_pos_bias\n",
    "\n",
    "        else:\n",
    "            bert = AutoModel.from_pretrained(\n",
    "                manager.get_bert_for_load(),\n",
    "                cache_dir=manager.path + 'bert_cache/'\n",
    "            )\n",
    "\n",
    "        # bert = AutoModel.from_pretrained(manager.get_bert_for_load(), cache_dir=manager.path + 'bert_cache/')\n",
    "        self.bert_embeddings = bert.embeddings\n",
    "        self.bert = bert.encoder\n",
    "        self.bert_pooler = nn.Sequential(\n",
    "            nn.Linear(768,768),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.encoderU = encoderU\n",
    "\n",
    "        if manager.debias:\n",
    "            self.userBias = nn.Parameter(torch.randn(1,manager.bert_dim))\n",
    "            nn.init.xavier_normal_(self.userBias)\n",
    "\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "\n",
    "        self.granularity = manager.granularity\n",
    "        if self.granularity != 'token':\n",
    "            self.register_buffer('cdd_dest', torch.zeros((self.batch_size, self.impr_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "            if manager.reducer in [\"bm25\", \"entity\", \"first\"]:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, (manager.k + 2) * (manager.k + 2))), persistent=False)\n",
    "            else:\n",
    "                self.register_buffer('his_dest', torch.zeros((self.batch_size, self.his_size, self.signal_length * self.signal_length)), persistent=False)\n",
    "\n",
    "\n",
    "        manager.name = '__'.join([\"plm3\", manager.bert, manager.encoderU, manager.granularity])\n",
    "        self.name = manager.name\n",
    "\n",
    "    def clickPredictor(self, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = cdd_news_repr.matmul(user_repr.transpose(-2,-1)).squeeze(-1)/math.sqrt(cdd_news_repr.size(-1))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _forward(self,x):\n",
    "        # destroy encoding and embedding outside of the model\n",
    "\n",
    "        batch_size = x['cdd_encoded_index'].size(0)\n",
    "\n",
    "        if self.granularity != 'token':\n",
    "            cdd_size = x['cdd_subword_index'].size(1)\n",
    "\n",
    "            if self.training:\n",
    "                cdd_dest = self.cdd_dest[:batch_size, :cdd_size]\n",
    "                his_dest = self.his_dest[:batch_size]\n",
    "\n",
    "            # batch_size always equals 1 when evaluating\n",
    "            else:\n",
    "                cdd_dest = self.cdd_dest[[0], :cdd_size]\n",
    "                his_dest = self.his_dest[[0]]\n",
    "\n",
    "            cdd_subword_index = x['cdd_subword_index'].to(self.device)\n",
    "            his_subword_index = x['his_subword_index'].to(self.device)\n",
    "            his_signal_length = his_subword_index.size(-2)\n",
    "            cdd_subword_index = cdd_subword_index[:, :, :, 0] * self.signal_length + cdd_subword_index[:, :, :, 1]\n",
    "            his_subword_index = his_subword_index[:, :, :, 0] * his_signal_length + his_subword_index[:, :, :, 1]\n",
    "\n",
    "            if self.training:\n",
    "                # * cdd_mask to filter out padded cdd news\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1) * x[\"cdd_mask\"].to(self.device)\n",
    "            else:\n",
    "                cdd_subword_prefix = cdd_dest.scatter(dim=-1, index=cdd_subword_index, value=1)\n",
    "\n",
    "            cdd_subword_prefix = cdd_subword_prefix.view(batch_size, cdd_size, self.signal_length, self.signal_length)\n",
    "            his_subword_prefix = his_dest.scatter(dim=-1, index=his_subword_index, value=1) * x[\"his_mask\"].to(self.device)\n",
    "            his_subword_prefix = his_subword_prefix.view(batch_size, self.his_size, his_signal_length, his_signal_length)\n",
    "\n",
    "            if self.granularity == 'avg':\n",
    "                # average subword embeddings as the word embedding\n",
    "                cdd_subword_prefix = F.normalize(cdd_subword_prefix, p=1, dim=-1)\n",
    "                his_subword_prefix = F.normalize(his_subword_prefix, p=1, dim=-1)\n",
    "\n",
    "            cdd_attn_mask = cdd_subword_prefix.matmul(x['cdd_attn_mask'].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "            his_attn_mask = his_subword_prefix.matmul(x[\"his_attn_mask\"].to(self.device).float().unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            cdd_subword_prefix = None\n",
    "            his_subword_prefix = None\n",
    "            # cdd_attn_mask = ((1 - x['cdd_attn_mask'].to(self.device)) * -10000.).view(-1, 1, 1, self.signal_length)\n",
    "            # his_attn_mask = ((1 - x[\"his_attn_mask\"].to(self.device)) * -10000.).view(-1, 1, 1, self.signal_length)\n",
    "            cdd_attn_mask = x['cdd_attn_mask'].to(self.device).view(-1, self.signal_length)\n",
    "            his_attn_mask = x['his_attn_mask'].to(self.device).view(-1, self.signal_length)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].to(self.device).view(-1, self.signal_length)\n",
    "        cdd_news_embedding = self.bert_embeddings(cdd_news)\n",
    "        if type(cdd_news_embedding) is tuple:\n",
    "            position_ids = cdd_news_embedding[1]\n",
    "            cdd_news_embedding = cdd_news_embedding[0]\n",
    "        if hasattr(self, 'rel_pos_bias'):\n",
    "            rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "            rel_pos1 = relative_position_bucket(rel_pos_mat, num_buckets=self.rel_pos_bins, max_distance=self.max_rel_pos)\n",
    "            rel_pos1 = F.one_hot(rel_pos1, num_classes=self.rel_pos_bins).float()\n",
    "            rel_pos1 = self.rel_pos_bias(rel_pos1).permute(0, 3, 1, 2)\n",
    "            bert_output1 = self.bert(cdd_news_embedding, attention_mask=cdd_attn_mask, rel_pos=rel_pos1)[0]\n",
    "        else:\n",
    "            # [bs, sl/term_num+2, hd]\n",
    "            bert_output1 = self.bert(cdd_news_embedding, attention_mask=cdd_attn_mask)[0]\n",
    "        cdd_news_repr = self.bert_pooler(bert_output1).view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        his_news = x[\"his_encoded_index\"].to(self.device).view(-1, self.signal_length)\n",
    "        his_news_embedding = self.bert_embeddings(his_news)\n",
    "        if type(his_news_embedding) is tuple:\n",
    "            position_ids = his_news_embedding[1]\n",
    "            his_news_embedding = his_news_embedding[0]\n",
    "\n",
    "        if hasattr(self, 'rel_pos_bias'):\n",
    "            rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "            rel_pos = relative_position_bucket(rel_pos_mat, num_buckets=self.rel_pos_bins, max_distance=self.max_rel_pos)\n",
    "            rel_pos = F.one_hot(rel_pos, num_classes=self.rel_pos_bins).float()\n",
    "            rel_pos = self.rel_pos_bias(rel_pos).permute(0, 3, 1, 2)\n",
    "            bert_output = self.bert(his_news_embedding, attention_mask=his_attn_mask, rel_pos=rel_pos)[0]\n",
    "        else:\n",
    "            # [bs, sl/term_num+2, hd]\n",
    "            bert_output = self.bert(his_news_embedding, attention_mask=his_attn_mask)[0]\n",
    "        his_news_repr = self.bert_pooler(bert_output).view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        user_repr = self.encoderU(his_news_repr)\n",
    "\n",
    "        if hasattr(self, 'userBias'):\n",
    "            user_repr = user_repr + self.userBias\n",
    "\n",
    "        return self.clickPredictor(cdd_news_repr, user_repr), (cdd_news_embedding, cdd_attn_mask, bert_output1, cdd_news_repr)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, cdd_news_embedding = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, cdd_news_embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "\n",
    "# encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "# reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "# model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "\n",
    "# manager.load(model, 589, strict=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "class BERT_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        1. for news input, encode it with BERT and output news- and word-level representations\n",
    "        2. for ps_term input, insert [CLS] token at the head and insert [SEP] token at the end\n",
    "        3. add position embedding to the sequence, starting from 0 pos\n",
    "    \"\"\"\n",
    "    def __init__(self, manager):\n",
    "        super().__init__()\n",
    "\n",
    "        # dimension for the final output embedding/representation\n",
    "        self.hidden_dim = manager.bert_dim\n",
    "        self.signal_length = manager.signal_length\n",
    "\n",
    "        if manager.bert == 'unilm':\n",
    "            config = TuringNLRv3Config.from_pretrained(manager.unilm_config_path)\n",
    "            # config.pooler = None\n",
    "            bert = TuringNLRv3ForSequenceClassification.from_pretrained(manager.unilm_path, config=config).bert\n",
    "\n",
    "            self.rel_pos_bins = config.rel_pos_bins\n",
    "            self.max_rel_pos = config.max_rel_pos\n",
    "            # unique in UniLM\n",
    "            self.rel_pos_bias = bert.rel_pos_bias\n",
    "\n",
    "        else:\n",
    "            bert = AutoModel.from_pretrained(\n",
    "                manager.get_bert_for_load(),\n",
    "                cache_dir=manager.path + 'bert_cache/'\n",
    "            )\n",
    "\n",
    "        self.bert = bert.encoder\n",
    "        self.pooler = manager.pooler\n",
    "        # project news representations into the same semantic space\n",
    "        self.projector = nn.Linear(manager.bert_dim, manager.bert_dim)\n",
    "        self.activation = manager.get_activation_func()\n",
    "\n",
    "        # self.projector = bert.pooler\n",
    "\n",
    "        if manager.bert == 'deberta':\n",
    "            self.extend_attn_mask = True\n",
    "        else:\n",
    "            self.extend_attn_mask = False\n",
    "\n",
    "        word_embedding = bert.embeddings.word_embeddings\n",
    "        self.layerNorm = bert.embeddings.LayerNorm\n",
    "        self.dropOut = bert.embeddings.dropout\n",
    "\n",
    "        if manager.reducer != 'none':\n",
    "            self.bert_cls_embedding = nn.Parameter(word_embedding.weight[manager.get_special_token_id('[CLS]')].view(1,1,self.hidden_dim))\n",
    "            self.bert_sep_embedding = nn.Parameter(word_embedding.weight[manager.get_special_token_id('[SEP]')].view(1,1,self.hidden_dim))\n",
    "\n",
    "        if manager.pooler == 'attn':\n",
    "            self.query = nn.Parameter(torch.randn(1, self.hidden_dim))\n",
    "            nn.init.xavier_normal_(self.query)\n",
    "\n",
    "        try:\n",
    "            # self.bert_pos_embedding = nn.Parameter(bert.embeddings.position_embeddings.weight)\n",
    "            self.bert_pos_embedding = nn.Embedding.from_pretrained(bert.embeddings.position_embeddings.weight, freeze=False)\n",
    "        except:\n",
    "            self.bert_pos_embedding = None\n",
    "\n",
    "        try:\n",
    "            self.bert_token_type_embedding = nn.Parameter(bert.embeddings.token_type_embeddings.weight)\n",
    "        except:\n",
    "            self.bert_token_type_embedding = None\n",
    "\n",
    "        self.register_buffer('extra_attn_mask', torch.ones(1, 1), persistent=False)\n",
    "\n",
    "    def forward(self, news_embedding, attn_mask, ps_term_input=False):\n",
    "        \"\"\" encode news with bert\n",
    "\n",
    "        Args:\n",
    "            news_embedding: [batch_size, *, signal_length, embedding_dim]\n",
    "            attn_mask: [batch_size, *, signal_length]\n",
    "\n",
    "        Returns:\n",
    "            news_encoded_embedding: hidden vector of each token in news, of size [batch_size, *, signal_length, emedding_dim]\n",
    "            news_repr: news representation, of size [batch_size, *, embedding_dim]\n",
    "        \"\"\"\n",
    "        batch_size = news_embedding.size(0)\n",
    "        signal_length = news_embedding.size(-2)\n",
    "\n",
    "        # insert [CLS] and [SEP] token\n",
    "        bert_input = news_embedding.view(-1, signal_length, self.hidden_dim)\n",
    "        bs = bert_input.size(0)\n",
    "\n",
    "        attn_mask = attn_mask.view(-1, signal_length)\n",
    "\n",
    "        # concatenated ps_terms\n",
    "        if ps_term_input:\n",
    "            # add [CLS] and [SEP] to ps_terms\n",
    "            # bert_input = torch.cat([self.bert_cls_embedding.expand(bs, 1, self.hidden_dim), bert_input, self.bert_sep_embedding.expand(bs, 1, self.hidden_dim)], dim=-2)\n",
    "            # attn_mask = torch.cat([self.extra_attn_mask.expand(bs, 1), attn_mask, self.extra_attn_mask.expand(bs, 1)], dim=-1)\n",
    "            # signal_length += 2\n",
    "            bert_input = torch.cat([self.bert_cls_embedding.expand(bs, 1, self.hidden_dim), bert_input], dim=-2)\n",
    "            attn_mask = torch.cat([self.extra_attn_mask.expand(bs, 1), attn_mask], dim=-1)\n",
    "            signal_length += 1\n",
    "\n",
    "        if self.bert_token_type_embedding is not None:\n",
    "            bert_input = bert_input + self.bert_token_type_embedding[0]\n",
    "\n",
    "        if self.bert_pos_embedding is not None:\n",
    "            pos_ids = torch.arange(signal_length, device=news_embedding.device)\n",
    "            bert_input = bert_input + self.bert_pos_embedding(pos_ids)\n",
    "\n",
    "        bert_input = self.dropOut(self.layerNorm(bert_input))\n",
    "\n",
    "        if self.extend_attn_mask:\n",
    "            ext_attn_mask = attn_mask\n",
    "        else:\n",
    "            ext_attn_mask = (1.0 - attn_mask) * -10000.0\n",
    "            ext_attn_mask = ext_attn_mask.view(bs, 1, 1, -1)\n",
    "\n",
    "        if hasattr(self, 'rel_pos_bias'):\n",
    "            position_ids = torch.arange(signal_length, dtype=torch.long, device=bert_input.device).unsqueeze(0).expand(bs, signal_length)\n",
    "            rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "            rel_pos = relative_position_bucket(rel_pos_mat, num_buckets=self.rel_pos_bins, max_distance=self.max_rel_pos)\n",
    "            rel_pos = F.one_hot(rel_pos, num_classes=self.rel_pos_bins).float()\n",
    "            rel_pos = self.rel_pos_bias(rel_pos).permute(0, 3, 1, 2)\n",
    "            bert_output = self.bert(bert_input, attention_mask=ext_attn_mask, rel_pos=rel_pos)[0]\n",
    "\n",
    "        else:\n",
    "            # [bs, sl/term_num+2, hd]\n",
    "            bert_output = self.bert(bert_input, attention_mask=ext_attn_mask)[0]\n",
    "\n",
    "        if self.pooler == \"cls\":\n",
    "            news_repr = bert_output[:, 0].reshape(batch_size, -1, self.hidden_dim)\n",
    "        elif self.pooler == \"attn\":\n",
    "            news_repr = scaled_dp_attention(self.query, bert_output, bert_output, attn_mask=attn_mask.unsqueeze(1)).view(batch_size, -1, self.hidden_dim)\n",
    "        elif self.pooler == \"avg\":\n",
    "            news_repr = bert_output.mean(dim=-2).reshape(batch_size, -1, self.hidden_dim)\n",
    "        news_repr = self.activation(self.projector(news_repr))\n",
    "\n",
    "        # use the genuine bert pooler\n",
    "        # news_repr = self.projector(bert_output).reshape(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        news_encoded_embedding = bert_output.view(batch_size, -1, bert_input.size(-2), self.hidden_dim)\n",
    "\n",
    "        return (bert_input, ext_attn_mask, bert_output, news_repr), news_repr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# plm1 = PLM(manager, encoderU)\n",
    "plm2 = PLM2(manager, embedding, encoderU)\n",
    "plm3 = PLM3(manager, encoderU)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# plm1.eval()\n",
    "plm2.eval()\n",
    "plm3.eval()\n",
    "1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "a,emb1 = plm2(x2)\n",
    "b,emb2 = plm3(x2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "(emb1[0] == emb2[0]).all()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# emb1[0].shape, emb2[0].shape\n",
    "(emb1[2] == emb2[2]).all()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "emb2[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "(emb1[1] == emb2[1]).all()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}