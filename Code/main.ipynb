{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare\n",
    "from data.configs.demo import config\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder\n",
    "from models.Encoders.RNN import RNN_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer\n",
    "from models.Modules.DRM import BM25_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker\n",
    "from models.Rankers.BERT import BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.ESM import ESM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "record = list(loaders[0])[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-19 20:41:28,350] INFO (utils.utils) Hyper Parameters are \\mscale:demo\n",
      "mode:tune\n",
      "epochs:8\n",
      "batch_size:10\n",
      "k:3\n",
      "threshold:0\n",
      "title_length:20\n",
      "abs_length:40\n",
      "signal_length:80\n",
      "npratio:4\n",
      "his_size:50\n",
      "cdd_size:5\n",
      "impr_size:10\n",
      "dropout_p:0.2\n",
      "device:cpu\n",
      "lr:0.0001\n",
      "bert_lr:3e-05\n",
      "metrics:auc,mean_mrr,ndcg@5,ndcg@10\n",
      "embedding:bert\n",
      "selector:sfi\n",
      "reducer:matching\n",
      "interactor:onepass\n",
      "embedding_dim:300\n",
      "hidden_dim:150\n",
      "rank:0\n",
      "world_size:0\n",
      "step:0\n",
      "seeds:42\n",
      "interval:10\n",
      "val_freq:2\n",
      "schedule:linear\n",
      "order_history:False\n",
      "warmup:100\n",
      "pin_memory:False\n",
      "shuffle:False\n",
      "num_workers:0\n",
      "path:../../../Data/\n",
      "tb:False\n",
      "bert:bert-base-uncased\n",
      "[2021-08-19 20:41:28,351] INFO (utils.utils) preparing dataset...\n",
      "[2021-08-19 20:41:28,355] INFO (utils.MIND) using cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-08-19 20:41:28,369] INFO (utils.MIND) using cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-08-19 20:41:28,601] INFO (utils.MIND) using cached user behavior from data/cache/bert/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-08-19 20:41:28,603] INFO (utils.MIND) using cached news tokenization from data/cache/bert/MINDdemo_dev/news.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class ESM(nn.Module):\n",
    "    def __init__(self, config, embedding, encoderN, encoderU, docReducer, termFuser, ranker):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = config.scale\n",
    "        self.cdd_size = config.cdd_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.his_size = config.his_size\n",
    "        self.device = config.device\n",
    "\n",
    "        self.k = config.k\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderN = encoderN\n",
    "        self.encoderU = encoderU\n",
    "        self.docReducer = docReducer\n",
    "        self.termFuser = termFuser\n",
    "        self.ranker = ranker\n",
    "\n",
    "        self.hidden_dim = encoderN.hidden_dim\n",
    "        self.final_dim = ranker.final_dim\n",
    "\n",
    "        self.learningToRank = nn.Sequential(\n",
    "            nn.Linear(self.final_dim + 1, int(self.final_dim/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(self.final_dim/2),1)\n",
    "        )\n",
    "\n",
    "        self.name = '__'.join(['esm', self.encoderN.name, self.encoderU.name, self.docReducer.name, self.ranker.name])\n",
    "        config.name = self.name\n",
    "\n",
    "    def clickPredictor(self, reduced_tensor, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            reduced_tensor: [batch_size, cdd_size, final_dim]\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score_coarse = cdd_news_repr.matmul(user_repr.transpose(-2,-1))\n",
    "        score = torch.cat([reduced_tensor, score_coarse], dim=-1)\n",
    "\n",
    "        return self.learningToRank(score).squeeze(dim=-1)\n",
    "\n",
    "    def _forward(self,x):\n",
    "        if x[\"cdd_encoded_index\"].size(0) != self.batch_size:\n",
    "            self.batch_size = x[\"cdd_encoded_index\"].size(0)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].long().to(self.device)\n",
    "        cdd_news_embedding = self.embedding(cdd_news)\n",
    "        _, cdd_news_repr = self.encoderN(\n",
    "            cdd_news_embedding\n",
    "        )\n",
    "        his_news = x[\"his_encoded_index\"].long().to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news)\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding\n",
    "        )\n",
    "\n",
    "        user_repr = self.encoderU(his_news_repr)\n",
    "\n",
    "        ps_terms, ps_term_ids = self.docReducer(his_news_encoded_embedding, his_news_embedding, user_repr, x[\"his_attn_mask\"].to(self.device).bool())\n",
    "        # if self.termFuser:\n",
    "        #     ps_terms = self.termFuser(ps_terms, ps_term_ids, his_news)\n",
    "\n",
    "        # reduced_tensor = self.ranker(torch.cat([cdd_news_repr.unsqueeze(-2), cdd_news_embedding], dim=-2), torch.cat([user_repr, ps_terms], dim=-2))\n",
    "\n",
    "        reduced_tensor = self.ranker(cdd_news_embedding, ps_terms, x[\"cdd_attn_mask\"].to(self.device))\n",
    "\n",
    "        return self.clickPredictor(reduced_tensor, cdd_news_repr, user_repr), ps_term_ids\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, ps_term_ids = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, ps_term_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "encoderN = CNN_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "\n",
    "docReducer = Matching_Reducer(manager)\n",
    "# docReducer = BM25_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "esm = ESM(manager, embedding, encoderN, encoderU, docReducer, None, ranker).to(manager.device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "manager.load(esm, 1178)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-19 20:41:47,285] INFO (utils.Manager) loading model from data/model_params/esm__cnn__rnn-u__matching-reducer__onepass-bert-without-order/demo_step1178_[k=3].model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "a,b = esm(record)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "c = record['his_encoded_index'].gather(dim=-1, index=b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "t = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import pickle\n",
    "bm25 = pickle.load(open('/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news_bm25.pkl','rb'))\n",
    "\n",
    "his = bm25['encoded_news']\n",
    "his_sorted = bm25['encoded_news_sorted']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "k = 8\n",
    "\n",
    "t.decode(c[0][k]), t.decode(record['his_encoded_index'][0][k]), t.decode(his_sorted[record['his_id'][0][k]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('aboutjak,',\n",
       " '[CLS]\\'wheel of fortune\\'guest delivers hilarious, off the rails introduction we\\'d like to solve the puzzle, pat : blair davis\\'loveless marriage? on monday, \" wheel of fortune \" welcomed as a new contestant trucking business owner blair davis, who offered a biting introduction for himself. when host pat sajak asked the man from cardiff, california, about his family, davis plunged into',\n",
       " '[CLS] loveless introduction blair summaries fortune wheel davis cardiff marriage pat trucking darkest rails biting puzzle plunged sajak contestant welcomed delivers solve guest hilarious trapped offered personal heard host tvnews owner likely asked ever business 12 show california tv family like years monday last man one? new cls,. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "[101] + [12, 13] + [102]*-1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101, 12, 13]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class BM25(object):\n",
    "    \"\"\"\n",
    "    compute bm25 score\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, epsilon=0.5):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _build_tf_idf(self, documents):\n",
    "        \"\"\"\n",
    "        build term frequencies (how many times a term occurs in one news) and document frequencies (how many documents contains a term)\n",
    "        \"\"\"\n",
    "        word_count = 0\n",
    "        doc_count = len(documents)\n",
    "\n",
    "        tfs = []\n",
    "        df = defaultdict(int)\n",
    "        for document in documents:\n",
    "            tf = defaultdict(int)\n",
    "            # ignore [CLS]\n",
    "            for term in document[1:]:\n",
    "                tf[term] += 1\n",
    "                df[term] += 1\n",
    "                word_count + 1\n",
    "\n",
    "            tfs.append(tf)\n",
    "\n",
    "        self.tfs = tfs\n",
    "\n",
    "        idf = defaultdict(float)\n",
    "        for term,freq in df.items():\n",
    "            idf[term] = math.log((doc_count - freq + 0.5 ) / (freq + 0.5) + 1)\n",
    "\n",
    "        self.idf = idf\n",
    "\n",
    "\n",
    "    def __call__(self, documents):\n",
    "        \"\"\"\n",
    "        compute bm25 score of each term in each document and sort the terms by it\n",
    "        with b=0, totally ignoring the effect of document length\n",
    "\n",
    "        Args:\n",
    "            documents: list of strings\n",
    "        \"\"\"\n",
    "        self._build_tf_idf(documents)\n",
    "\n",
    "        document_length = len(documents[0])\n",
    "        bm25_scores = []\n",
    "        for tf in self.tfs:\n",
    "            score = defaultdict(float)\n",
    "            for term, freq in tf.items():\n",
    "                score[term] = (self.idf[term] * freq * (self.k + 1)) / (freq + self.k)\n",
    "\n",
    "            bm25_scores.append(dict(sorted(score.items(), key=lambda item: item[1], reverse=True)))\n",
    "\n",
    "        sorted_documents = []\n",
    "        sorted_attn_mask = []\n",
    "        for bm25 in bm25_scores:\n",
    "            bm25_length = len(bm25) + 1\n",
    "            pad_length = document_length - bm25_length\n",
    "\n",
    "            sorted_documents.append([101] + list(bm25.keys()) + [102]*pad_length)\n",
    "            sorted_attn_mask.append([1] * bm25_length + [0] * pad_length)\n",
    "\n",
    "        return sorted_documents, sorted_attn_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "b = BM25()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "documents = np.array([[101,102,102,102],[101,105,105,106]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "b(documents)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[101, 102, 102, 102], [101, 101, 105, 106]], [[1, 1, 0, 0], [1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "decb58d8582e8bebc2c9af4e5243ce054f2f8013c5b7e79ffbf7b8b9f3c0761b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}