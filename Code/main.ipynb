{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from utils.utils import prepare\n",
    "from data.configs.demo import config\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder\n",
    "from models.Encoders.RNN import RNN_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer\n",
    "from models.Modules.DRM import BM25_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker\n",
    "from models.Rankers.BERT import BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.ESM import ESM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "manager = Manager(config)\n",
    "loaders = prepare(manager)\n",
    "record = list(loaders[0])[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-20 04:51:41,773] INFO (utils.utils) Hyper Parameters are \\mscale:demo\n",
      "mode:tune\n",
      "epochs:8\n",
      "batch_size:10\n",
      "k:5\n",
      "threshold:-inf\n",
      "title_length:20\n",
      "abs_length:40\n",
      "signal_length:80\n",
      "npratio:4\n",
      "his_size:50\n",
      "cdd_size:5\n",
      "impr_size:10\n",
      "dropout_p:0.2\n",
      "device:cpu\n",
      "lr:0.0001\n",
      "bert_lr:3e-05\n",
      "metrics:auc,mean_mrr,ndcg@5,ndcg@10\n",
      "embedding:bert\n",
      "selector:sfi\n",
      "reducer:matching\n",
      "interactor:onepass\n",
      "embedding_dim:300\n",
      "hidden_dim:150\n",
      "rank:0\n",
      "world_size:0\n",
      "step:0\n",
      "seeds:42\n",
      "interval:10\n",
      "val_freq:2\n",
      "schedule:linear\n",
      "order_history:False\n",
      "warmup:100\n",
      "pin_memory:False\n",
      "shuffle:False\n",
      "num_workers:0\n",
      "path:../../../Data/\n",
      "tb:False\n",
      "bert:bert-base-uncased\n",
      "[2021-08-20 04:51:41,774] INFO (utils.utils) preparing dataset...\n",
      "[2021-08-20 04:51:41,861] INFO (utils.MIND) using cached user behavior from data/cache/bert/MINDdemo_train/10/behaviors..pkl\n",
      "[2021-08-20 04:51:41,871] INFO (utils.MIND) using cached news tokenization from data/cache/bert/MINDdemo_train/news.pkl\n",
      "[2021-08-20 04:51:42,151] INFO (utils.MIND) using cached user behavior from data/cache/bert/MINDdemo_dev/10/behaviors..pkl\n",
      "[2021-08-20 04:51:42,155] INFO (utils.MIND) using cached news tokenization from data/cache/bert/MINDdemo_dev/news.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class ESM(nn.Module):\n",
    "    def __init__(self, config, embedding, encoderN, encoderU, docReducer, termFuser, ranker):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale = config.scale\n",
    "        self.cdd_size = config.cdd_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.his_size = config.his_size\n",
    "        self.device = config.device\n",
    "\n",
    "        self.k = config.k\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.encoderN = encoderN\n",
    "        self.encoderU = encoderU\n",
    "        self.docReducer = docReducer\n",
    "        self.termFuser = termFuser\n",
    "        self.ranker = ranker\n",
    "\n",
    "        self.hidden_dim = encoderN.hidden_dim\n",
    "        self.final_dim = ranker.final_dim\n",
    "\n",
    "        self.learningToRank = nn.Sequential(\n",
    "            nn.Linear(self.final_dim + 1, int(self.final_dim/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(self.final_dim/2),1)\n",
    "        )\n",
    "\n",
    "        self.name = '__'.join(['esm', self.encoderN.name, self.encoderU.name, self.docReducer.name, self.ranker.name])\n",
    "        config.name = self.name\n",
    "\n",
    "    def clickPredictor(self, reduced_tensor, cdd_news_repr, user_repr):\n",
    "        \"\"\" calculate batch of click probabolity\n",
    "\n",
    "        Args:\n",
    "            reduced_tensor: [batch_size, cdd_size, final_dim]\n",
    "            cdd_news_repr: news-level representation, [batch_size, cdd_size, hidden_dim]\n",
    "            user_repr: user representation, [batch_size, 1, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            score of each candidate news, [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score_coarse = cdd_news_repr.matmul(user_repr.transpose(-2,-1))\n",
    "        score = torch.cat([reduced_tensor, score_coarse], dim=-1)\n",
    "\n",
    "        return self.learningToRank(score).squeeze(dim=-1)\n",
    "\n",
    "    def _forward(self,x):\n",
    "        if x[\"cdd_encoded_index\"].size(0) != self.batch_size:\n",
    "            self.batch_size = x[\"cdd_encoded_index\"].size(0)\n",
    "\n",
    "        cdd_news = x[\"cdd_encoded_index\"].long().to(self.device)\n",
    "        cdd_news_embedding = self.embedding(cdd_news)\n",
    "        _, cdd_news_repr = self.encoderN(\n",
    "            cdd_news_embedding\n",
    "        )\n",
    "        his_news = x[\"his_encoded_index\"].long().to(self.device)\n",
    "        his_news_embedding = self.embedding(his_news)\n",
    "        his_news_encoded_embedding, his_news_repr = self.encoderN(\n",
    "            his_news_embedding\n",
    "        )\n",
    "\n",
    "        user_repr = self.encoderU(his_news_repr)\n",
    "\n",
    "        ps_terms, ps_term_ids = self.docReducer(his_news_encoded_embedding, his_news_embedding, user_repr, x[\"his_attn_mask\"].to(self.device).bool())\n",
    "        # if self.termFuser:\n",
    "        #     ps_terms = self.termFuser(ps_terms, ps_term_ids, his_news)\n",
    "\n",
    "        reduced_tensor = self.ranker(cdd_news_embedding, ps_terms, x[\"cdd_attn_mask\"].to(self.device))\n",
    "\n",
    "        return self.clickPredictor(reduced_tensor, cdd_news_repr, user_repr), ps_term_ids\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Decoupled function, score is unormalized click score\n",
    "        \"\"\"\n",
    "        score, ps_term_ids = self._forward(x)\n",
    "\n",
    "        if self.training:\n",
    "            prob = nn.functional.log_softmax(score, dim=1)\n",
    "        else:\n",
    "            prob = torch.sigmoid(score)\n",
    "\n",
    "        return prob, ps_term_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "encoderN = CNN_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "\n",
    "docReducer = Matching_Reducer(manager)\n",
    "# docReducer = BM25_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "esm = ESM(manager, embedding, encoderN, encoderU, docReducer, None, ranker).to(manager.device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "manager.scale = 'large'\n",
    "\n",
    "manager.load(esm, 50000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-20 04:52:24,081] INFO (utils.Manager) loading model from data/model_params/esm__cnn__rnn-u__matching-reducer__onepass-bert/large_step50000_[k=5].model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "a,b = esm(record)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "c = record['his_encoded_index'].gather(dim=-1, index=b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "t = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pickle\n",
    "bm25 = pickle.load(open('/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news_bm25.pkl','rb'))\n",
    "\n",
    "his = bm25['encoded_news']\n",
    "his_sorted = bm25['encoded_news_sorted']"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news_bm25.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_72195/1486971841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news_bm25.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_news'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhis_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_news_sorted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/workspace/Peitian/Code/Document-Reduction/Code/data/cache/bert/MINDdemo_train/news_bm25.pkl'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "batch, k = 1,13\n",
    "\n",
    "t.decode(c[batch][k]), t.decode(record['his_encoded_index'][batch][k]), #t.decode(his_sorted[record['his_id'][0][k]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('dead found say officials,',\n",
       " '[CLS] missing west point cadet found dead, officials say a u. s. military academy cadet who vanished last week has been found dead, officials said wednesday. news newsus [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "[101] + [12, 13] + [102]*-1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101, 12, 13]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class BM25(object):\n",
    "    \"\"\"\n",
    "    compute bm25 score\n",
    "    \"\"\"\n",
    "    def __init__(self, k=2, epsilon=0.5):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _build_tf_idf(self, documents):\n",
    "        \"\"\"\n",
    "        build term frequencies (how many times a term occurs in one news) and document frequencies (how many documents contains a term)\n",
    "        \"\"\"\n",
    "        word_count = 0\n",
    "        doc_count = len(documents)\n",
    "\n",
    "        tfs = []\n",
    "        df = defaultdict(int)\n",
    "        for document in documents:\n",
    "            tf = defaultdict(int)\n",
    "            # ignore [CLS]\n",
    "            for term in document[1:]:\n",
    "                tf[term] += 1\n",
    "                df[term] += 1\n",
    "                word_count + 1\n",
    "\n",
    "            tfs.append(tf)\n",
    "\n",
    "        self.tfs = tfs\n",
    "\n",
    "        idf = defaultdict(float)\n",
    "        for term,freq in df.items():\n",
    "            idf[term] = math.log((doc_count - freq + 0.5 ) / (freq + 0.5) + 1)\n",
    "\n",
    "        self.idf = idf\n",
    "\n",
    "\n",
    "    def __call__(self, documents):\n",
    "        \"\"\"\n",
    "        compute bm25 score of each term in each document and sort the terms by it\n",
    "        with b=0, totally ignoring the effect of document length\n",
    "\n",
    "        Args:\n",
    "            documents: list of strings\n",
    "        \"\"\"\n",
    "        self._build_tf_idf(documents)\n",
    "\n",
    "        document_length = len(documents[0])\n",
    "        bm25_scores = []\n",
    "        for tf in self.tfs:\n",
    "            score = defaultdict(float)\n",
    "            for term, freq in tf.items():\n",
    "                score[term] = (self.idf[term] * freq * (self.k + 1)) / (freq + self.k)\n",
    "\n",
    "            bm25_scores.append(dict(sorted(score.items(), key=lambda item: item[1], reverse=True)))\n",
    "\n",
    "        sorted_documents = []\n",
    "        sorted_attn_mask = []\n",
    "        for bm25 in bm25_scores:\n",
    "            bm25_length = len(bm25) + 1\n",
    "            pad_length = document_length - bm25_length\n",
    "\n",
    "            sorted_documents.append([101] + list(bm25.keys()) + [102]*pad_length)\n",
    "            sorted_attn_mask.append([1] * bm25_length + [0] * pad_length)\n",
    "\n",
    "        return sorted_documents, sorted_attn_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "b = BM25()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "documents = np.array([[101,102,102,102],[101,105,105,106]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "b(documents)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[101, 102, 102, 102], [101, 101, 105, 106]], [[1, 1, 0, 0], [1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}