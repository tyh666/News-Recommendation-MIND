{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\n",
    "import math,random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.Attention import Attention\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Embeddings.GLOVE import GLOVE_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder\n",
    "from models.Encoders.RNN import RNN_User_Encoder\n",
    "from models.Interactors.CNN import CNN_Interactor\n",
    "from models.Interactors.FIM import FIM_Interactor\n",
    "from models.Interactors.KNRM import KNRM_Interactor\n",
    "from models.Interactors.BERT import BERT_Interactor\n",
    "\n",
    "from models.ESM import ESM\n",
    "\n",
    "from data.configs.demo import config\n",
    "from utils.utils import prepare\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Modules.DRM import DRM_Matching\n",
    "# from models.Modules.TFM import TFM"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c18fe9381656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBERT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERT_Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOVE\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGLOVE_Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN_Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/v-pezhang/Code/Document-Reduction/Code/models/Embeddings/BERT.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBERT_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "config.his_size = 10\n",
    "config.embedding = 'bert'\n",
    "config.hidden_dim = 768\n",
    "config.device = 0\n",
    "config.learning_rate = 1e-6\n",
    "\n",
    "config.path = \"C:/\"\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "vocab, loaders = prepare(manager)\n",
    "record = next(iter(loaders[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-10 19:44:44,709] INFO (root) Hyper Parameters are scale:demo, mode:tune, epochs:8, batch_size:10, k:3, threshold:0, title_length:20, abs_length:40, signal_length:50, npratio:4, his_size:10, dropout_p:0.2, device:cpu, learning_rate:1e-06, metrics:auc,mean_mrr,ndcg@5,ndcg@10, embedding:bert, embedding_dim:300, hidden_dim:768, rank:0, world_size:0, step:[0], seeds:42, interval:10, val_freq:2, schedule:None, path:C:/, tb:False, bert:bert-base-uncased, cdd_size:5\n",
      "[2021-08-10 19:44:44,710] INFO (root) preparing dataset...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "embedding = BERT_Embedding(manager)\n",
    "encoderN = CNN_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "docReducer = DRM_Matching(manager)\n",
    "# termFuser = TFM(manager.his_size, manager.k)\n",
    "# interactor = CNN_Interactor(manager)\n",
    "interactor = BERT_Interactor(manager)\n",
    "\n",
    "esm = ESM(manager, embedding, encoderN, encoderU, docReducer, None, interactor).to(manager.device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from itertools import chain"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "for k in esm.parameters():\r\n",
    "    print(k.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 50, 768])\n",
      "torch.Size([30522, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([1, 1, 1, 768])\n",
      "torch.Size([1, 10, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([384, 768])\n",
      "torch.Size([384])\n",
      "torch.Size([1, 384])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "x = chain(*[v.parameters() for k,v in esm.named_children() if k not in ['embedding','interactor']])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "x = chain(x, esm.embedding.parameters())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "x = chain(x, esm.interactor.parameters())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "for i in x:\r\n",
    "    print(i.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([384, 768])\n",
      "torch.Size([384])\n",
      "torch.Size([1, 384])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 50, 768])\n",
      "torch.Size([30522, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "[v for k,v in esm.named_children() if k not in ['embedding','interactor']]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[CNN_Encoder(\n",
       "   (wordQueryProject): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (CNN): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "   (layerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (RELU): ReLU()\n",
       "   (Tanh): Tanh()\n",
       " ),\n",
       " RNN_User_Encoder(\n",
       "   (lstm): LSTM(768, 768, batch_first=True)\n",
       " ),\n",
       " DRM_Matching(),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=384, out_features=1, bias=True)\n",
       " )]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "esm.named_modules()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x0000024A0056CCF0>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "manager.tune(esm, loaders)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-08-08 02:23:37,875] INFO (utils.Manager) training...\n",
      "epoch 1 , step 140 , loss: 1.5820:  49%|████▉     | 146/295 [00:46<00:47,  3.12it/s][2021-08-08 02:24:24,940] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [00:55<00:00, 32.52it/s]\n",
      "[2021-08-08 02:25:21,153] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.4966, 'mean_mrr': 0.2273, 'ndcg@5': 0.2398, 'ndcg@10': 0.3023, 'epoch': 1, 'step': 146}\n",
      "epoch 1 , step 290 , loss: 1.5815:  99%|█████████▉| 292/295 [02:30<00:01,  1.94it/s][2021-08-08 02:26:08,356] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [01:00<00:00, 29.90it/s]\n",
      "[2021-08-08 02:27:09,454] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.4976, 'mean_mrr': 0.2288, 'ndcg@5': 0.2282, 'ndcg@10': 0.2929, 'epoch': 1, 'step': 292}\n",
      "epoch 1 , step 290 , loss: 1.5815: 100%|██████████| 295/295 [03:32<00:00,  1.39it/s]\n",
      "epoch 2 , step 140 , loss: 1.6237:  49%|████▉     | 146/295 [00:46<00:47,  3.11it/s][2021-08-08 02:27:57,494] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [01:00<00:00, 30.08it/s]\n",
      "[2021-08-08 02:28:58,234] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.5059, 'mean_mrr': 0.2279, 'ndcg@5': 0.2332, 'ndcg@10': 0.2922, 'epoch': 2, 'step': 146}\n",
      "epoch 2 , step 290 , loss: 1.6153:  99%|█████████▉| 292/295 [02:34<00:01,  1.89it/s][2021-08-08 02:29:45,378] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [00:59<00:00, 30.23it/s]\n",
      "[2021-08-08 02:30:45,788] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.5086, 'mean_mrr': 0.2311, 'ndcg@5': 0.233, 'ndcg@10': 0.2967, 'epoch': 2, 'step': 292}\n",
      "epoch 2 , step 290 , loss: 1.6153: 100%|██████████| 295/295 [03:36<00:00,  1.36it/s]\n",
      "epoch 3 , step 140 , loss: 1.6180:  49%|████▉     | 146/295 [00:47<00:48,  3.04it/s][2021-08-08 02:31:34,898] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [00:58<00:00, 30.77it/s]\n",
      "[2021-08-08 02:32:34,268] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.4918, 'mean_mrr': 0.2242, 'ndcg@5': 0.2221, 'ndcg@10': 0.2813, 'epoch': 3, 'step': 146}\n",
      "epoch 3 , step 290 , loss: 1.6101:  99%|█████████▉| 292/295 [02:34<00:01,  1.90it/s][2021-08-08 02:33:20,925] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [00:59<00:00, 30.28it/s]\n",
      "[2021-08-08 02:34:21,232] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.5133, 'mean_mrr': 0.2376, 'ndcg@5': 0.2347, 'ndcg@10': 0.2989, 'epoch': 3, 'step': 292}\n",
      "epoch 3 , step 290 , loss: 1.6101: 100%|██████████| 295/295 [03:35<00:00,  1.37it/s]\n",
      "epoch 4 , step 140 , loss: 1.6173:  49%|████▉     | 146/295 [00:46<00:47,  3.12it/s][2021-08-08 02:35:09,128] INFO (utils.Manager) evaluating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1812/1812 [00:59<00:00, 30.50it/s]\n",
      "[2021-08-08 02:36:09,024] INFO (utils.Manager) current result of esm_cnn_rnn-user-encoder_matching-based_bert is {'auc': 0.4922, 'mean_mrr': 0.2182, 'ndcg@5': 0.2153, 'ndcg@10': 0.2786, 'epoch': 4, 'step': 146}\n",
      "epoch 4 , step 270 , loss: 1.6103:  92%|█████████▏| 271/295 [02:26<00:12,  1.85it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}