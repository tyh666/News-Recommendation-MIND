{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "embedding = torch.rand((3,2),requires_grad=True)\r\n",
    "embedding.retain_grad()\r\n",
    "\r\n",
    "a = torch.rand((2,3),requires_grad=True)\r\n",
    "a.retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient formulation of the index in topk based selection, proposed in SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.4690, 1.0891, 0.0000],\n        [0.0000, 0.9664, 1.1481]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight,b = torch.topk(a,k=2,dim=-1)\r\n",
    "# b.retain_grad()\r\n",
    "\r\n",
    "c = a.gather(dim=-1,index=b)\r\n",
    "c.retain_grad()\r\n",
    "\r\n",
    "# d = torch.zeros(a.size()).scatter(dim=-1,index=b,src=torch.ones(a.size()))\r\n",
    "d = embedding[b]*(weight.unsqueeze(dim=-1))\r\n",
    "\r\n",
    "loss = (d**2).sum()\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient formulation of the index using REINFORCE policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\r\n",
    "\r\n",
    "rewards = []\r\n",
    "policies = []\r\n",
    "losses = []\r\n",
    "\r\n",
    "net = torch.nn.Linear(3,3)\r\n",
    "distribution = Categorical(net(a).abs())\r\n",
    "action = distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([2, 2]),\n None,\n tensor([[ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 3.7785, 10.2651]]),\n None)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = \r\n",
    "loss = (res**2).sum()\r\n",
    "\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "action, a.grad, embedding.grad, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4937,  1.5606,  2.4589],\n        [ 0.0298, -0.3498,  3.3279]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\r\n",
    "from torch.distributions import Normal\r\n",
    "\r\n",
    "a = torch.rand((2,3),requires_grad=True)\r\n",
    "\r\n",
    "dis = Normal(a[0],a[1])\r\n",
    "m = dis.rsample()\r\n",
    "loss = (m**2).sum()\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.distributions as distributions\r\n",
    "\r\n",
    "\r\n",
    "embedding = torch.rand((3,3),requires_grad=True)\r\n",
    "embedding.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = torch.tensor([[0.2,0.5,0.3]],requires_grad=True)\r\n",
    "distribution.retain_grad()\r\n",
    "\r\n",
    "distribution_hard = nn.functional.softmax(distribution.masked_fill(distribution < distribution.max(), -float('inf')),dim=-1)\r\n",
    "ret = distribution - distribution.detach() + distribution_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = torch.tensor([[0.2,0.5,0.3]],requires_grad=True)\r\n",
    "distribution.retain_grad()\r\n",
    "\r\n",
    "index = distribution.max(-1,keepdim=True)[1]\r\n",
    "distribution_hard = torch.zeros_like(distribution, memory_format=torch.legacy_contiguous_format).scatter_(-1, index, 1.0)\r\n",
    "\r\n",
    "ret = distribution - distribution.detach() + distribution_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.9843, 2.6652, 0.7562]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = ret.matmul(embedding)\r\n",
    "loss = (selected ** 2).sum()\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "distribution.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('nn': conda)",
   "name": "python388jvsc74a57bd09616ec0cf0e0dd041cba3c8886d471a5cc72bbf20e2c795f4079199200777fdd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}