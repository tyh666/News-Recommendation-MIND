{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"../\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.TwoTowerBaseModel import TwoTowerBaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    "from models.PLM import PLM\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "# torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import LongformerConfig, LongformerModel, BigBirdModel, BigBirdConfig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "m.config.is_decoder"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "a = BigBirdConfig()\n",
    "a.block_size = 2\n",
    "a.num_random_blocks = 2\n",
    "a.max_position_embeddings = 64\n",
    "m = BigBirdModel(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "inputs = t3(\"i\",padding=\"max_length\",max_length=32,return_tensors='pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "res = m(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "m7.config, m.config"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(BigBirdConfig {\n",
       "   \"_name_or_path\": \"google/bigbird-roberta-base\",\n",
       "   \"architectures\": [\n",
       "     \"BigBirdForPreTraining\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"attention_type\": \"block_sparse\",\n",
       "   \"block_size\": 64,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu_new\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 4096,\n",
       "   \"model_type\": \"big_bird\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"num_random_blocks\": 3,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"rescale_embeddings\": false,\n",
       "   \"sep_token_id\": 66,\n",
       "   \"transformers_version\": \"4.10.0\",\n",
       "   \"type_vocab_size\": 2,\n",
       "   \"use_bias\": true,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50358\n",
       " },\n",
       " BigBirdConfig {\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"attention_type\": \"block_sparse\",\n",
       "   \"block_size\": 32,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu_new\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 64,\n",
       "   \"model_type\": \"big_bird\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"num_random_blocks\": 3,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"rescale_embeddings\": false,\n",
       "   \"sep_token_id\": 66,\n",
       "   \"transformers_version\": \"4.10.0\",\n",
       "   \"type_vocab_size\": 2,\n",
       "   \"use_bias\": true,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50358\n",
       " })"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "# m4 = AutoModel.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# m5 = AutoModel.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# m6 = AutoModel.from_pretrained('distilbert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "m7 = AutoModel.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")\n",
    "# m8 = AutoModel.from_pretrained(\"allenai/longformer-base-4096\", cache_dir=config.path + \"bert_cache/\")\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")\n",
    "# t4 = AutoTokenizer.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# t5 = AutoTokenizer.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t6 = AutoTokenizer.from_pretrained('distilbert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t7 = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config.bert = 'bert'\n",
    "config.mode = \"encode\"\n",
    "config.scale = \"large\"\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "# X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# embedding = BERT_Embedding(manager)\n",
    "manager.hidden_dim = 768\n",
    "manager.reducer = \"none\"\n",
    "# encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "# reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "# model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "model = PLM(manager, encoderU).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 8*150*150*50\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer First\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer BERT\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*30*768*768*2*3 + 50*30*768*30*2*2 + 50*30*768*2*2\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer ATT\n",
    "import math\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 50*150*4\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 50*3*768*2*2\n",
    "bert_project = 50*12*3*64*2*3\n",
    "bert_attn = 50*12*3*64*3*2 + 50*12*3*3*64*2\n",
    "bert_intm = 50*3*768*768*2 + 50*3*768*2 + 50*3*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = ext_encoding + reduction + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GateFormer\n",
    "embedding = 50 * 2 * 100 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NRMS\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "project = 50 * 30 * 300 * 256 * 2 * 3\n",
    "attn = 50*30*256*30*4\n",
    "attnA = 50*30*256*2\n",
    "\n",
    "project2 = 50*256*256*2*3\n",
    "attn2 = 50*256*50*4\n",
    "attnA2 = 50*256*2\n",
    "\n",
    "sum = embedding + project + project2 + attn + attn2 + attnA + attnA2\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LSTUR\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "encoding = 3*300*150*30*50\n",
    "attn=50*30*150*2 + 50*30*150*2\n",
    "lstm = 50*150*150*8*2\n",
    "\n",
    "sum = embedding + encoding + lstm + attn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NAML\n",
    "embedding = 50 * 2 * 30 * 300 *2 *2\n",
    "encoding = 3*300*150*30*50*2\n",
    "attn=(50*30*150*2 + 50*30*150*2)*2\n",
    "attn2=50*150*2 + 50*150*2\n",
    "sum = embedding + encoding + attn + attn2\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# BigBird\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*12*64*2 + 12*1024*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# longformer\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*512*12*64*2 + 12*1024*512*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EBNR\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Synthesizer\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# funnel\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 10 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# newbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 4 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# distilbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 2\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 6 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "189 / 255560 * 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}