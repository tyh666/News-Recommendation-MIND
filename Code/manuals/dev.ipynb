{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"../\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from thop import profile\n",
    "from collections import defaultdict\n",
    "from data.configs.demo import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertConfig, AutoModelForSequenceClassification\n",
    "from utils.Manager import Manager\n",
    "\n",
    "from models.Embeddings.BERT import BERT_Embedding\n",
    "from models.Encoders.CNN import CNN_Encoder,CNN_User_Encoder\n",
    "from models.Encoders.RNN import RNN_Encoder,RNN_User_Encoder\n",
    "from models.Encoders.MHA import MHA_Encoder, MHA_User_Encoder\n",
    "from models.Modules.DRM import Matching_Reducer, Identical_Reducer\n",
    "from models.Rankers.BERT import BERT_Onepass_Ranker, BERT_Original_Ranker\n",
    "from models.Rankers.CNN import CNN_Ranker\n",
    "from models.Encoders.Pooling import Attention_Pooling, Average_Pooling\n",
    "from models.UniLM.modeling import TuringNLRv3Model, TuringNLRv3ForSequenceClassification, relative_position_bucket\n",
    "from models.UniLM.configuration_tnlrv3 import TuringNLRv3Config\n",
    "\n",
    "from models.TwoTowerBaseModel import TwoTowerBaseModel\n",
    "\n",
    "from models.Encoders.BERT import BERT_Encoder\n",
    "from models.Encoders.Pooling import *\n",
    "\n",
    "from models.TESRec import TESRec\n",
    "from models.XFormer import XFormer\n",
    "from models.PLM import PLM\n",
    " \n",
    "from models.Modules.Attention import MultiheadAttention, get_attn_mask, XSoftmax\n",
    "# torch.set_printoptions(threshold=100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# m = AutoModel.from_pretrained('bert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m2 = AutoModel.from_pretrained('microsoft/deberta-base',cache_dir=config.path + 'bert_cache/')\n",
    "# m3 = TuringNLRv3ForSequenceClassification.from_pretrained(config.unilm_path, config=TuringNLRv3Config.from_pretrained(config.unilm_config_path))\n",
    "# m4 = AutoModel.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# m5 = AutoModel.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "m6 = AutoModel.from_pretrained('distilbert-base-uncased',cache_dir=config.path + 'bert_cache/')\n",
    "# m7 = AutoModel.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")\n",
    "# m8 = AutoModel.from_pretrained(\"allenai/longformer-base-4096\", cache_dir=config.path + \"bert_cache/\")\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t2 = AutoTokenizer.from_pretrained('microsoft/deberta-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t3 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', cache_dir=config.path + \"bert_cache/\")\n",
    "# t4 = AutoTokenizer.from_pretrained('google/reformer-crime-and-punishment', cache_dir=config.path + \"bert_cache/\")\n",
    "# t5 = AutoTokenizer.from_pretrained('funnel-transformer/small-base', cache_dir=config.path + \"bert_cache/\")\n",
    "# t6 = AutoTokenizer.from_pretrained('distilbert-base-uncased', cache_dir=config.path + \"bert_cache/\")\n",
    "# t7 = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\", cache_dir=config.path + \"bert_cache/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "m6"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config.bert = 'bert'\n",
    "config.mode = \"encode\"\n",
    "config.scale = \"large\"\n",
    "\n",
    "manager = Manager(config)\n",
    "\n",
    "loaders = manager.prepare()\n",
    "X1 = list(loaders[0])\n",
    "# X2 = list(loaders[1])\n",
    "x1 = X1[0]\n",
    "# x2 = X2[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# embedding = BERT_Embedding(manager)\n",
    "manager.hidden_dim = 768\n",
    "manager.reducer = \"none\"\n",
    "# encoderN = CNN_Encoder(manager)\n",
    "# encoderN = RNN_Encoder(manager)\n",
    "# encoderN = MHA_Encoder(manager)\n",
    "\n",
    "# encoderU = CNN_User_Encoder(manager)\n",
    "encoderU = RNN_User_Encoder(manager)\n",
    "# encoderU = MHA_User_Encoder(manager)\n",
    "# encoderU = Attention_Pooling(manager)\n",
    "# encoderU = Average_Pooling(manager)\n",
    "\n",
    "# reducer = Matching_Reducer(manager)\n",
    "# reducer = Identical_Reducer(manager)\n",
    "\n",
    "# ranker = CNN_Ranker(manager)\n",
    "# ranker = BERT_Onepass_Ranker(manager)\n",
    "# ranker = BERT_Original_Ranker(manager)\n",
    "\n",
    "# model = TESRec(manager, embedding, encoderN, encoderU, reducer).to(manager.device)\n",
    "# model = ESM(manager, embedding, encoderN, encoderU, reducer, ranker).to(manager.device)\n",
    "model = PLM(manager, encoderU).to(manager.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# GateFormer\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 8*150*150*50\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20479554310.33589"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# GateFormer First\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19950796800"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# GateFormer BERT\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*30*768*768*2*3 + 50*30*768*30*2*2 + 50*30*768*2*2\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + ext_encoding + reduction + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25402518310.33589"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# GateFormer ATT\n",
    "import math\n",
    "embedding = 50 * 2 * 30 * 768 * 2\n",
    "\n",
    "ext_encoding = 50*(3*150*768*30 + 30*150*4) + 50*150*4\n",
    "reduction = (2*30*150 + 30*math.log2(30))*50 + 50*3\n",
    "\n",
    "bert_embed = 50*3*768*2*2\n",
    "bert_project = 50*12*3*64*2*3\n",
    "bert_attn = 50*12*3*64*3*2 + 50*12*3*3*64*2\n",
    "bert_intm = 50*3*768*768*2 + 50*3*768*2 + 50*3*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = ext_encoding + reduction + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20193182710.33589"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# GateFormer\n",
    "embedding = 50 * 2 * 100 * 768 * 2\n",
    "\n",
    "bert_embed = 150 * 2 * 768 * 2\n",
    "bert_project = 150*12*64*2\n",
    "bert_attn = 150*12*64*150*2 + 12*150*150*64*2\n",
    "bert_intm = 150*768*768*2 + 150*768*2 + 150*768*3072*4\n",
    "bert_pool = 150*768*4\n",
    "\n",
    "sum = embedding + bert_embed + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19961548800"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# NRMS\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "project = 50 * 30 * 300 * 256 * 2 * 3\n",
    "attn = 50*30*256*30*4\n",
    "attnA = 50*30*256*2\n",
    "\n",
    "project2 = 50*256*256*2*3\n",
    "attn2 = 50*256*50*4\n",
    "attnA2 = 50*256*2\n",
    "\n",
    "sum = embedding + project + project2 + attn + attn2 + attnA + attnA2\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "762094400"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# LSTUR\n",
    "embedding = 50 * 2 * 30 * 300 * 2\n",
    "\n",
    "encoding = 3*300*150*30*50\n",
    "attn=50*30*150*2 + 50*30*150*2\n",
    "lstm = 50*150*150*8*2\n",
    "\n",
    "sum = embedding + encoding + lstm + attn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "223200000"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# NAML\n",
    "embedding = 50 * 2 * 30 * 300 *2 *2\n",
    "encoding = 3*300*150*30*50*2\n",
    "attn=(50*30*150*2 + 50*30*150*2)*2\n",
    "attn2=50*150*2 + 50*150*2\n",
    "sum = embedding + encoding + attn + attn2\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "410430000"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# BigBird\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*12*64*2 + 12*1024*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "130547712000"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# longformer\n",
    "embedding = 2 * 1024 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 1024*12*64*2\n",
    "bert_attn = 1024*512*12*64*2 + 12*1024*512*64*2\n",
    "bert_intm = 1024*768*768*2 + 1024*768*2 + 1024*768*3072*4\n",
    "bert_pool = 1024*768*4\n",
    "\n",
    "sum = embedding + (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "149837316096"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# EBNR\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "193417113600"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Synthesizer\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "193417113600"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# funnel\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 10 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "161271705600"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# newbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 3\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 4 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64835481600"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# distilbert\n",
    "embedding = 50 * 2 * 30 * 768 * 2 * 2\n",
    "\n",
    "bert_project = 50*12*30*64*2*3\n",
    "bert_attn = 50*12*30*64*30*2 + 50*12*30*30*64*2\n",
    "bert_intm = 50*30*768*768*2 + 50*30*768*2 + 50*30*768*3072*4\n",
    "bert_pool = 50*1*768*768*2\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 6 + bert_pool\n",
    "\n",
    "rnn = 8*2*768*768*50\n",
    "\n",
    "sum = embedding + bert + rnn\n",
    "sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96976281600"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "189 / 255560 * 1000"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7395523556112068"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "0f43efc2da5f33d61ae1bd929e6c7df9dd2aa7f250aadb5586d31de3e27bb6a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}