{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class BertSelfAttention(nn.Module):\r\n",
    "    def __init__(self, config):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.num_attention_heads = config.num_attention_heads\r\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n",
    "\r\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\r\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\r\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\r\n",
    "\r\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\r\n",
    "\r\n",
    "        self.signal_length = config.signal_length\r\n",
    "        \r\n",
    "        # self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\r\n",
    "        # if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\r\n",
    "        #     self.max_position_embeddings = config.max_position_embeddings\r\n",
    "        #     self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\r\n",
    "\r\n",
    "\r\n",
    "    def transpose_for_scores(self, x):\r\n",
    "        \"\"\"\r\n",
    "        transpose the head_num dimension, to make every head operates in parallel\r\n",
    "        \"\"\"\r\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\r\n",
    "        x = x.view(*new_x_shape)\r\n",
    "        return x.permute(0, 2, 1, 3)\r\n",
    "\r\n",
    "    def forward(\r\n",
    "        self,\r\n",
    "        hidden_states,\r\n",
    "        references,\r\n",
    "        attention_mask=None,\r\n",
    "        head_mask=None,\r\n",
    "        output_attentions=False,\r\n",
    "    ):\r\n",
    "        \"\"\" customized bert self attention, attending to the references\r\n",
    "\r\n",
    "        Args:\r\n",
    "            hidden_states: normally encoded candidate news, [batch_size, signal_length, hidden_dim]\r\n",
    "            references: normally personalized terms, [batch_size, term_num, hidden_dim]\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        inputs = torch.cat([hidden_states, references], dim=-2)\r\n",
    "\r\n",
    "        # [batch_size, head_num, signal_length, hidden_dim]\r\n",
    "        key_layer = self.transpose_for_scores(self.key(inputs))\r\n",
    "        value_layer = self.transpose_for_scores(self.value(inputs))\r\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\r\n",
    "\r\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\r\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\r\n",
    "\r\n",
    "        # if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\r\n",
    "        #     seq_length = hidden_states.size()[1]\r\n",
    "        #     position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\r\n",
    "        #     position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\r\n",
    "        #     distance = position_ids_l - position_ids_r\r\n",
    "        #     positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\r\n",
    "        #     positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\r\n",
    "\r\n",
    "        #     if self.position_embedding_type == \"relative_key\":\r\n",
    "        #         relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\r\n",
    "        #         attention_scores = attention_scores + relative_position_scores\r\n",
    "        #     elif self.position_embedding_type == \"relative_key_query\":\r\n",
    "        #         relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\r\n",
    "        #         relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\r\n",
    "        #         attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\r\n",
    "\r\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\r\n",
    "        if attention_mask is not None:\r\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\r\n",
    "            attention_scores = attention_scores + attention_mask\r\n",
    "\r\n",
    "        # Normalize the attention scores to probabilities.\r\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\r\n",
    "\r\n",
    "        # This is actually dropping out entire tokens to attend to, which might\r\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\r\n",
    "        attention_probs = self.dropout(attention_probs)\r\n",
    "\r\n",
    "        # Mask heads if we want to\r\n",
    "        if head_mask is not None:\r\n",
    "            attention_probs = attention_probs * head_mask\r\n",
    "\r\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\r\n",
    "\r\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\r\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\r\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\r\n",
    "\r\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\r\n",
    "\r\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "01f651214b0df947376b22bdce96981aa6546e177d04daceb10ed93c0185908c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}