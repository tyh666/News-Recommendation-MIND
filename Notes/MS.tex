\documentclass{beamer}
\title{Paper Review}
\author{Peitian Zhang}
\institute[RUC]{Renmin University of China}
\institute[MSRA]{Microsoft Research Asia}
\usetheme[]{Warsaw}

\newenvironment{insight}{\begin{block}{Insight}}{\end{block}}
\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \tableofcontents
\end{frame}

\section{Basics}
\subsection{Workflow}
\begin{frame}
    % \frametitle{Retrieval Procedure}
    \begin{itemize}
        \item Create inverted-index
        \item Use retrieval model such as BM25 and query likelihood to recall several passages according to the query
    \end{itemize}
    \begin{block}{Hint}
        It's not so clear to me that how weighted inverted index works.
    \end{block}
\end{frame}
\subsection{Datasets}
\begin{frame}
    \begin{itemize}
        \item 1
    \end{itemize}
\end{frame}
\subsection{Metrics}
\begin{frame}
    \begin{itemize}
        \item Recall
    \end{itemize}
\end{frame}

\section{Papers}
\subsection{DeepCT}
\begin{frame}{Overview}
    \begin{itemize}
        \item Generate the contextutalized word weight according to its embeddings (produced by BERT).
        \begin{equation}
            y = Wx + b,
        \end{equation}
        where $x$ is the BERT embedding of the word, $W$ and $b$ are parameters.

        \item The weights is used to construct inverted index and applied in the first-stage retrieval.
    \end{itemize}
    \begin{insight}
        Select top $K$ terms by DeepCT.
    \end{insight}
\end{frame}
\begin{frame}{Training}
    \begin{itemize}
        \item Fine-tune BERT with ground-truth weight for each term in the document, which is estimated by the coocurrance of the term in the doc and the query.
        \item Mean square loss.
    \end{itemize}
\end{frame}


\end{document}